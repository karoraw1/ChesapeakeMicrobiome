{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the environmental conditions data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Volumes/KeithSSD/ChesapeakeMicrobiome/data\"\n",
    "data_dir = \"/Volumes/KeithSSD/ChesapeakeMicrobiome/data\"\n",
    "\n",
    "env_data <- read.delim(env_data_file, row.names=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(library(ggplot2))\n",
    "suppressPackageStartupMessages(library(reshape2))\n",
    "suppressPackageStartupMessages(library(randomForest))\n",
    "suppressPackageStartupMessages(library(caret))\n",
    "suppressPackageStartupMessages(library(repr))\n",
    "suppressPackageStartupMessages(library(foreach))\n",
    "suppressPackageStartupMessages(library(doParallel))\n",
    "suppressPackageStartupMessages(library(ggrepel))\n",
    "suppressPackageStartupMessages(library(ComplexHeatmap))\n",
    "suppressPackageStartupMessages(library(circlize))\n",
    "suppressPackageStartupMessages(library(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"StationName\"              \"depth_float\"             \n",
      " [3] \"Latitude\"                 \"Longitude\"               \n",
      " [5] \"RawCount\"                 \"TrimCount\"               \n",
      " [7] \"CollectionAgency\"         \"sequencing_ID\"           \n",
      " [9] \"Month\"                    \"Year\"                    \n",
      "[11] \"Month_Year\"               \"julian_day\"              \n",
      "[13] \"day_length\"               \"anti_day_length\"         \n",
      "[15] \"julian_seconds\"           \"Depth_Percentage\"        \n",
      "[17] \"Discharge_James_14\"       \"Discharge_Susquehanna_14\"\n",
      "[19] \"enspie\"                   \"faith_pd\"                \n",
      "[21] \"WTEMP\"                    \"SALINITY\"                \n",
      "[23] \"DO\"                       \"PH\"                      \n",
      "[25] \"habitat\"                  \"AC_PrinComp2\"            \n",
      "[27] \"TON\"                      \"TP\"                      \n",
      "[29] \"TN\"                       \"PN\"                      \n",
      "[31] \"PP\"                       \"PC\"                      \n",
      "[33] \"TSS\"                      \"NO2F\"                    \n",
      "[35] \"DON\"                      \"DIN\"                     \n",
      "[37] \"NH4F\"                     \"NO23F\"                   \n",
      "[39] \"DOP\"                      \"CHLA\"                    \n",
      "[41] \"NO3F\"                     \"PHEO\"                    \n",
      "[43] \"PO4F\"                     \"TDN\"                     \n",
      "[45] \"TDP\"                      \"SIGMA_T\"                 \n",
      "[47] \"SPCOND\"                  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env_data$Year <- factor(env_data$Year)\n",
    "env_data$Month <- factor(env_data$Month)\n",
    "env_data$habitat <- factor(env_data$habitat)\n",
    "\n",
    "transect_data_f = \"/Volumes/KeithSSD/CB_V4/otu_data/WaterQualityData/matched_cleaned_data/transect_mdata_colset_1.tsv\"\n",
    "tran_df <- read.delim(transect_data_f, row.names=1)\n",
    "missing_columns = setdiff(colnames(tran_df), colnames(env_data))\n",
    "\n",
    "for (i in missing_columns){\n",
    "    env_data[,i] <- NA\n",
    "    env_data[rownames(tran_df),i] = tran_df[rownames(tran_df), i]\n",
    "}\n",
    "\n",
    "print(colnames(env_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "  1   2   3   4   5   6 \n",
       " 25 103  43  47  45  27 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "   Top    Mid Bottom \n",
       "    67     46    177 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "oligo  meso  poly \n",
       "   18   185    87 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_data[,'StatName'] <- NA\n",
    "env_data[which(env_data[,'StationName'] %in% c('CB22', 'CB31', 'CB32')),'StatName'] <- '1'\n",
    "env_data[which(env_data[,'StationName'] %in% c('CB33C', 'CB41C')),'StatName'] <- '2'\n",
    "env_data[which(env_data[,'StationName'] %in% c('CB42C', 'CB43C', 'CB44', 'CB51')),'StatName'] <- '3'\n",
    "env_data[which(env_data[,'StationName'] %in% c('CB52', 'CB53', 'CB54', 'CB71')),'StatName'] <- '4'\n",
    "env_data[which(env_data[,'StationName'] %in% c('CB61', 'CB62', 'CB63', 'CB64')),'StatName'] <- '5'\n",
    "env_data[which(env_data[,'StationName'] %in% c('CB72', 'CB73', 'CB74')),'StatName'] <- '6'\n",
    "env_data$StatName <- factor(env_data$StatName)\n",
    "print(sum(is.na(env_data[,'StatName'])))\n",
    "table(env_data$StatName)\n",
    "\n",
    "our_station = env_data[,'StationName'] == 'CB33C' \n",
    "env_data[,'depth_segment'] <- NA\n",
    "env_data[which((!our_station) & env_data$depth_float > 2), 'depth_segment'] = 'Bottom'\n",
    "env_data[which(env_data$depth_float < 2), 'depth_segment'] = 'Top'\n",
    "env_data[which(our_station & env_data$depth_float > 11), 'depth_segment'] = 'Bottom'\n",
    "env_data[is.na(env_data[,'depth_segment']),'depth_segment'] = 'Mid'\n",
    "env_data[,'depth_segment'] = factor(env_data[,'depth_segment'], levels=c('Top', 'Mid', 'Bottom'))\n",
    "table(env_data$depth_segment)\n",
    "\n",
    "\n",
    "env_data[,'Sal.Zone'] <- NA\n",
    "oligohaline <- c('CB22', 'CB31')\n",
    "mesohaline <- c('CB32', 'CB33C', 'CB41C', 'CB42C', 'CB43C', 'CB44', 'CB51', 'CB52', 'CB53', 'CB54')\n",
    "polyhaline <- c('CB61', 'CB62', 'CB63', 'CB64', 'CB71', 'CB72', 'CB73', 'CB74')\n",
    "env_data[which(env_data[,'StationName'] %in% oligohaline),'Sal.Zone'] <- 'oligo'\n",
    "env_data[which(env_data[,'StationName'] %in% mesohaline),'Sal.Zone'] <- 'meso'\n",
    "env_data[which(env_data[,'StationName'] %in% polyhaline),'Sal.Zone'] <- 'poly'\n",
    "env_data$Sal.Zone <- factor(env_data$Sal.Zone, levels=c('oligo', 'meso', 'poly'))\n",
    "print(sum(is.na(env_data[,'Sal.Zone'])))\n",
    "table(env_data$Sal.Zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'StationName'</li>\n",
       "\t<li>'depth_float'</li>\n",
       "\t<li>'Latitude'</li>\n",
       "\t<li>'Longitude'</li>\n",
       "\t<li>'RawCount'</li>\n",
       "\t<li>'TrimCount'</li>\n",
       "\t<li>'CollectionAgency'</li>\n",
       "\t<li>'sequencing_ID'</li>\n",
       "\t<li>'Month'</li>\n",
       "\t<li>'Year'</li>\n",
       "\t<li>'Month_Year'</li>\n",
       "\t<li>'julian_day'</li>\n",
       "\t<li>'day_length'</li>\n",
       "\t<li>'anti_day_length'</li>\n",
       "\t<li>'julian_seconds'</li>\n",
       "\t<li>'Depth_Percentage'</li>\n",
       "\t<li>'Discharge_James_14'</li>\n",
       "\t<li>'Discharge_Susquehanna_14'</li>\n",
       "\t<li>'enspie'</li>\n",
       "\t<li>'faith_pd'</li>\n",
       "\t<li>'WTEMP'</li>\n",
       "\t<li>'SALINITY'</li>\n",
       "\t<li>'DO'</li>\n",
       "\t<li>'PH'</li>\n",
       "\t<li>'habitat'</li>\n",
       "\t<li>'AC_PrinComp2'</li>\n",
       "\t<li>'TON'</li>\n",
       "\t<li>'TP'</li>\n",
       "\t<li>'TN'</li>\n",
       "\t<li>'PN'</li>\n",
       "\t<li>'PP'</li>\n",
       "\t<li>'PC'</li>\n",
       "\t<li>'TSS'</li>\n",
       "\t<li>'NO2F'</li>\n",
       "\t<li>'DON'</li>\n",
       "\t<li>'DIN'</li>\n",
       "\t<li>'NH4F'</li>\n",
       "\t<li>'NO23F'</li>\n",
       "\t<li>'DOP'</li>\n",
       "\t<li>'CHLA'</li>\n",
       "\t<li>'NO3F'</li>\n",
       "\t<li>'PHEO'</li>\n",
       "\t<li>'PO4F'</li>\n",
       "\t<li>'TDN'</li>\n",
       "\t<li>'TDP'</li>\n",
       "\t<li>'SIGMA_T'</li>\n",
       "\t<li>'SPCOND'</li>\n",
       "\t<li>'StatName'</li>\n",
       "\t<li>'depth_segment'</li>\n",
       "\t<li>'Sal.Zone'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'StationName'\n",
       "\\item 'depth\\_float'\n",
       "\\item 'Latitude'\n",
       "\\item 'Longitude'\n",
       "\\item 'RawCount'\n",
       "\\item 'TrimCount'\n",
       "\\item 'CollectionAgency'\n",
       "\\item 'sequencing\\_ID'\n",
       "\\item 'Month'\n",
       "\\item 'Year'\n",
       "\\item 'Month\\_Year'\n",
       "\\item 'julian\\_day'\n",
       "\\item 'day\\_length'\n",
       "\\item 'anti\\_day\\_length'\n",
       "\\item 'julian\\_seconds'\n",
       "\\item 'Depth\\_Percentage'\n",
       "\\item 'Discharge\\_James\\_14'\n",
       "\\item 'Discharge\\_Susquehanna\\_14'\n",
       "\\item 'enspie'\n",
       "\\item 'faith\\_pd'\n",
       "\\item 'WTEMP'\n",
       "\\item 'SALINITY'\n",
       "\\item 'DO'\n",
       "\\item 'PH'\n",
       "\\item 'habitat'\n",
       "\\item 'AC\\_PrinComp2'\n",
       "\\item 'TON'\n",
       "\\item 'TP'\n",
       "\\item 'TN'\n",
       "\\item 'PN'\n",
       "\\item 'PP'\n",
       "\\item 'PC'\n",
       "\\item 'TSS'\n",
       "\\item 'NO2F'\n",
       "\\item 'DON'\n",
       "\\item 'DIN'\n",
       "\\item 'NH4F'\n",
       "\\item 'NO23F'\n",
       "\\item 'DOP'\n",
       "\\item 'CHLA'\n",
       "\\item 'NO3F'\n",
       "\\item 'PHEO'\n",
       "\\item 'PO4F'\n",
       "\\item 'TDN'\n",
       "\\item 'TDP'\n",
       "\\item 'SIGMA\\_T'\n",
       "\\item 'SPCOND'\n",
       "\\item 'StatName'\n",
       "\\item 'depth\\_segment'\n",
       "\\item 'Sal.Zone'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'StationName'\n",
       "2. 'depth_float'\n",
       "3. 'Latitude'\n",
       "4. 'Longitude'\n",
       "5. 'RawCount'\n",
       "6. 'TrimCount'\n",
       "7. 'CollectionAgency'\n",
       "8. 'sequencing_ID'\n",
       "9. 'Month'\n",
       "10. 'Year'\n",
       "11. 'Month_Year'\n",
       "12. 'julian_day'\n",
       "13. 'day_length'\n",
       "14. 'anti_day_length'\n",
       "15. 'julian_seconds'\n",
       "16. 'Depth_Percentage'\n",
       "17. 'Discharge_James_14'\n",
       "18. 'Discharge_Susquehanna_14'\n",
       "19. 'enspie'\n",
       "20. 'faith_pd'\n",
       "21. 'WTEMP'\n",
       "22. 'SALINITY'\n",
       "23. 'DO'\n",
       "24. 'PH'\n",
       "25. 'habitat'\n",
       "26. 'AC_PrinComp2'\n",
       "27. 'TON'\n",
       "28. 'TP'\n",
       "29. 'TN'\n",
       "30. 'PN'\n",
       "31. 'PP'\n",
       "32. 'PC'\n",
       "33. 'TSS'\n",
       "34. 'NO2F'\n",
       "35. 'DON'\n",
       "36. 'DIN'\n",
       "37. 'NH4F'\n",
       "38. 'NO23F'\n",
       "39. 'DOP'\n",
       "40. 'CHLA'\n",
       "41. 'NO3F'\n",
       "42. 'PHEO'\n",
       "43. 'PO4F'\n",
       "44. 'TDN'\n",
       "45. 'TDP'\n",
       "46. 'SIGMA_T'\n",
       "47. 'SPCOND'\n",
       "48. 'StatName'\n",
       "49. 'depth_segment'\n",
       "50. 'Sal.Zone'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"StationName\"              \"depth_float\"             \n",
       " [3] \"Latitude\"                 \"Longitude\"               \n",
       " [5] \"RawCount\"                 \"TrimCount\"               \n",
       " [7] \"CollectionAgency\"         \"sequencing_ID\"           \n",
       " [9] \"Month\"                    \"Year\"                    \n",
       "[11] \"Month_Year\"               \"julian_day\"              \n",
       "[13] \"day_length\"               \"anti_day_length\"         \n",
       "[15] \"julian_seconds\"           \"Depth_Percentage\"        \n",
       "[17] \"Discharge_James_14\"       \"Discharge_Susquehanna_14\"\n",
       "[19] \"enspie\"                   \"faith_pd\"                \n",
       "[21] \"WTEMP\"                    \"SALINITY\"                \n",
       "[23] \"DO\"                       \"PH\"                      \n",
       "[25] \"habitat\"                  \"AC_PrinComp2\"            \n",
       "[27] \"TON\"                      \"TP\"                      \n",
       "[29] \"TN\"                       \"PN\"                      \n",
       "[31] \"PP\"                       \"PC\"                      \n",
       "[33] \"TSS\"                      \"NO2F\"                    \n",
       "[35] \"DON\"                      \"DIN\"                     \n",
       "[37] \"NH4F\"                     \"NO23F\"                   \n",
       "[39] \"DOP\"                      \"CHLA\"                    \n",
       "[41] \"NO3F\"                     \"PHEO\"                    \n",
       "[43] \"PO4F\"                     \"TDN\"                     \n",
       "[45] \"TDP\"                      \"SIGMA_T\"                 \n",
       "[47] \"SPCOND\"                   \"StatName\"                \n",
       "[49] \"depth_segment\"            \"Sal.Zone\"                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tenv_data = env_data[!(is.na(env_data$TON)),]\n",
    "colnames(tenv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenv_data$Years <- NA\n",
    "tenv_data$Years <- (tenv_data$julian_day - 365)/365\n",
    "tenv_data$Season <- tenv_data$anti_day_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years   -8.299809e-17  --  1 \n",
      "Salinity   1.56607e-17  --  1 \n",
      "Season   -1.203705e-17  --  1 \n",
      "Density   1.849745e-16  --  1 \n",
      "NO3   2.986736e-17  --  1 \n",
      "NO2   -3.432915e-17  --  1 \n",
      "CHLA   -7.401246e-17  --  1 \n",
      "TP   1.785079e-17  --  1 \n",
      "PHEO   -2.76857e-17  --  1 \n",
      "NH4   -6.346438e-17  --  1 \n",
      "WTEMP   -2.62254e-16  --  1 \n",
      "Discharge   -9.277578e-17  --  1 \n",
      "DO   8.043184e-17  --  1 \n"
     ]
    }
   ],
   "source": [
    "# Select variables and clean up response set\n",
    "\n",
    "select_env = c('Years', 'SALINITY', 'Season', 'SIGMA_T', 'StatName', 'habitat', \n",
    "               'NO3F', 'NO2F', 'CHLA', 'TP', 'PHEO', 'NH4F', 'WTEMP', \n",
    "               'Discharge_Susquehanna_14', 'DO')\n",
    "\n",
    "response_set = tenv_data[,select_env]\n",
    "colnames(response_set)[colnames(response_set) == 'habitat'] = 'Habitat'\n",
    "colnames(response_set)[colnames(response_set) == 'SALINITY'] = 'Salinity'\n",
    "colnames(response_set)[colnames(response_set) == 'NO3F'] = 'NO3'\n",
    "colnames(response_set)[colnames(response_set) == 'NO2F'] = 'NO2'\n",
    "colnames(response_set)[colnames(response_set) == 'NH4F'] = 'NH4'\n",
    "colnames(response_set)[colnames(response_set) == 'Discharge_Susquehanna_14'] = 'Discharge'\n",
    "colnames(response_set)[colnames(response_set) == 'StatName'] = 'Station.Group'\n",
    "colnames(response_set)[colnames(response_set) == 'SIGMA_T'] = 'Density'\n",
    "\n",
    "ind <- sapply(response_set, is.numeric)\n",
    "for (isnum in names(ind)[ind]){\n",
    "    response_set[,isnum] <- as.numeric(scale(response_set[,isnum]))\n",
    "    cat(isnum, \" \", mean(response_set[,isnum] ), \" -- \", sd(response_set[,isnum]), '\\n')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and fix OEUs\n",
    "fxn_file = \"/Users/login/Google Drive/SiYi_Xiaotong_Materials/FAPROTAX_counts_silva.txt\"\n",
    "taxa_file = '/Users/login/Google Drive/SiYi_Xiaotong_Materials/taxa_family_counts_relabund.txt'\n",
    "otu_table_file = \"/Volumes/KeithSSD/CB_V4/otu_data/final_rarefied_table.tsv\"\n",
    "\n",
    "data_files = list('Taxa.Family'=taxa_file, 'Fxn.SILVA'=fxn_file, 'ASV.OEU'=otu_table_file)\n",
    "data_objects = list()\n",
    "for (i in names(data_files))\n",
    "    data_objects[[i]] = read.delim(data_files[[i]], row.names=1)[rownames(response_set),]\n",
    "\n",
    "# relabund \n",
    "data_objects$ASV.OEU = ((data_objects$ASV.OEU + 0.1)/rowSums(data_objects$ASV.OEU + 0.1))*1e6\n",
    "\n",
    "# remove unclassifieds\n",
    "data_objects$Taxa.Family$nan <- NULL\n",
    "\n",
    "# remove garbage functions (>.95 correlation with other similarly names functions and possibly also irrelevant)\n",
    "to_drop = c('methylotrophy', 'aerobic_ammonia_oxidation', 'sulfate_respiration',\n",
    "            'dark_sulfite_oxidation', 'arsenate_respiration', 'nitrite_ammonification',\n",
    "            'dissimilatory_arsenate_reduction', 'nitrite_denitrification', \n",
    "            'nitrous_oxide_denitrification', 'nitrate_denitrification', 'fumarate_respiration',\n",
    "            'mammal_gut', 'plant_pathogen',  'cyanobacteria', 'phototrophy', \n",
    "            'anoxygenic_photoautotrophy_S_oxidizing', 'anoxygenic_photoautotrophy_Fe_oxidizing',\n",
    "            'nitrate_respiration', 'aliphatic_non_methane_hydrocarbon_degradation', \n",
    "            'aerobic_chemoheterotrophy', 'nitrite_respiration', 'dark_sulfide_oxidation')\n",
    "\n",
    "to_drop = unique(c(to_drop, colnames(data_objects$Fxn.SILVA)[colSums(data_objects$Fxn.SILVA) == 0]))\n",
    "data_objects$Fxn.SILVA[,to_drop] <- NULL\n",
    "\n",
    "drop_tax = colnames(data_objects$Taxa.Family)[colSums(data_objects$Taxa.Family) == 0]\n",
    "data_objects$Taxa.Family[,drop_tax] <- NULL\n",
    "\n",
    "cluster_file = '/Volumes/KeithSSD/CB_V4/otu_data/otu_cluster_vector.txt'\n",
    "cluster_vect = read.delim(cluster_file)\n",
    "cluster_vect$asv.name <- rownames(cluster_vect)\n",
    "cat(nrow(cluster_vect), \" \", 'OTUs\\n')\n",
    "cluster_vect = cluster_vect[!is.na(cluster_vect$cluster_number),]\n",
    "oeu_df = data.frame(matrix(NA, nrow=dim(response_set)[1], ncol=max(cluster_vect$cluster_number)))\n",
    "rownames(oeu_df) <- rownames(response_set)\n",
    "colnames(oeu_df) <- unique(cluster_vect$cluster_number)\n",
    "cluster_summary = data.frame('n'=c(), 'correl'=c(), 'total'=c())\n",
    "for (clust in colnames(oeu_df)){\n",
    "    cl_mems = cluster_vect[which(cluster_vect$cluster_number == clust), 'asv.name']\n",
    "    average_cor = mean(as.dist(cor(data_objects$ASV.OEU[, cl_mems], method=\"pearson\")))\n",
    "    while ((average_cor < 0.75) & (length(cl_mems) > 1)) {\n",
    "        mini_cors = data.frame('cor_i' = c())\n",
    "        for (cl_mem in cl_mems){\n",
    "            other_mems = setdiff(cl_mems, c(cl_mem))\n",
    "            mini_cors[cl_mem, 'cor_i'] = mean(cor(x=data_objects$ASV.OEU[, cl_mem], y=data_objects$ASV.OEU[, other_mems], \n",
    "                                       method=\"pearson\"))\n",
    "        }\n",
    "        loser = rownames(mini_cors)[which(mini_cors$cor_i == min(mini_cors$cor_i))]\n",
    "        cl_mems = cl_mems[!(cl_mems %in% loser)]\n",
    "        if (length(cl_mems) > 1){\n",
    "            average_cor = mean(as.dist(cor(data_objects$ASV.OEU[, cl_mems], \n",
    "                                           data_objects$ASV.OEU[, cl_mems], \n",
    "                                           method=\"pearson\")))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if ((length(cl_mems) > 1) & (average_cor >= 0.75)) {\n",
    "        cluster_summary[clust, 'n'] = length(cl_mems)\n",
    "        cluster_summary[clust, 'correl'] = mean(as.dist(cor(data_objects$ASV.OEU[, cl_mems], method=\"pearson\")))\n",
    "        oeu_df[,clust] = rowSums(data_objects$ASV.OEU[, cl_mems])\n",
    "        cluster_summary[clust, 'mean.abund'] = mean(log(oeu_df[,clust] + 1))\n",
    "        }\n",
    "}\n",
    "oeu_df = oeu_df[,which(colSums(is.na(oeu_df)) == 0)]\n",
    "cat(sum(cluster_summary$n), \" \", 'clustered\\n')\n",
    "cat(length(cluster_summary$n), \" \", 'clusters\\n')\n",
    "colnames(oeu_df) <- paste(\"OEU\", colnames(oeu_df), sep='.')\n",
    "data_objects[['ASV.OEU']] <- log(oeu_df + 1)\n",
    "data_objects[['Fxn.SILVA']] <- log(data_objects$Fxn.SILVA + 1)\n",
    "data_objects[['Taxa.Family']] <- log(data_objects$Taxa.Family + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(oeu_df)\n",
    "sum(cluster_summary$n)/1561\n",
    "y = table(cluster_summary$n)\n",
    "sum(y[names(y)[1:4]]) / sum(y)\n",
    "mean(cluster_summary$correl)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsq <- function (x, y) cor(x, y) ^ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCores <- 3\n",
    "counter = 0\n",
    "folds = 10\n",
    "\n",
    "score_summaries = expand.grid(names(data_objects), c('Full', 'Slim'))\n",
    "score_summaries = paste(score_summaries$Var1, score_summaries$Var2, sep=\".\")\n",
    "predictor_list = unlist(lapply(data_objects, colnames))\n",
    "names(predictor_list) <-NULL\n",
    "all_summaries = c(score_summaries, predictor_list)\n",
    "vs_trial_df = data.frame(matrix(nrow=dim(response_set)[2], ncol=length(all_summaries)))\n",
    "rownames(vs_trial_df) = colnames(response_set)\n",
    "colnames(vs_trial_df) = all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the timer\n",
    "start_time = Sys.time()\n",
    "# iterate over predictor sets\n",
    "for (trial in names(data_objects)){\n",
    "    # extract predictor matrix\n",
    "    raw_pred_df = data_objects[[trial]]\n",
    "    pred_cols = colnames(raw_pred_df)\n",
    "    print(c(\"Trial\", trial))\n",
    "    \n",
    "    for (resp in names(response_set)){\n",
    "        # set seed \n",
    "        counter = counter + 1\n",
    "        set.seed(123*counter)\n",
    "        # combine into 1 object\n",
    "\n",
    "        pred_resp = cbind.data.frame(raw_pred_df, response_set[,resp])\n",
    "        colnames(pred_resp) <- c(colnames(raw_pred_df), resp)\n",
    "        \n",
    "        # make a tune grid\n",
    "        if (class(response_set[,resp]) == 'factor'){\n",
    "            default.var = floor(sqrt(ncol(raw_pred_df)))\n",
    "        } else {\n",
    "            default.var = max(floor(ncol(raw_pred_df)/3), 1)\n",
    "        }\n",
    "        \n",
    "        # tune the RF model (1)\n",
    "        control <- trainControl(method=\"cv\", number=5)\n",
    "        tune.grid.full <- expand.grid(mtry=unique(round(default.var*(seq(0.5, 1.5, 0.1)))))\n",
    "        rf_default <- train(x=pred_resp[,pred_cols], y=pred_resp[,resp], \n",
    "                            method=\"rf\", tuneGrid=tune.grid.full, trControl=control)\n",
    "        opt_mtry_full = rf_default$bestTune[['mtry']]\n",
    "\n",
    "        # prefilter\n",
    "        prf_model = randomForest(x=pred_resp[,pred_cols], y=pred_resp[,resp], \n",
    "                                 ntree = 250, random_state=counter, mtry=opt_mtry_full,\n",
    "                                 importance=T)\n",
    "\n",
    "        # get full importances and prefilter\n",
    "        pvar_import = importance(prf_model, type=1, scale=F)\n",
    "        sorted_VIs = base::sort(pvar_import, index.return=T)\n",
    "        sel_vars = rownames(pvar_import)[sorted_VIs$ix[1:40]]\n",
    "\n",
    "        # retune 3\n",
    "        if (class(response_set[,resp]) == 'factor'){\n",
    "            default.var.slim = floor(sqrt(length(sel_vars)))\n",
    "        } else {\n",
    "            default.var.slim = max(floor(length(sel_vars)/3), 1)\n",
    "        }\n",
    "        \n",
    "        tune.grid.slim <- expand.grid(mtry=unique(round(default.var.slim*(seq(0.5, 1.5, 0.1)))))\n",
    "        rf_slim <- train(x=pred_resp[,sel_vars], y=pred_resp[,resp], \n",
    "                         method=\"rf\", tuneGrid=tune.grid.slim, trControl=control)\n",
    "        opt_mtry_slim = rf_slim$bestTune[['mtry']]\n",
    "\n",
    "        cat(trial, \"--\", resp, \"\\n\")\n",
    "        end_time = Sys.time(); print(end_time - start_time);\n",
    "        \n",
    "        # fit full and slim models 10fold\n",
    "        registerDoParallel(numCores)\n",
    "        results = foreach(i=1:folds, .combine=data.frame) %dopar% {\n",
    "\n",
    "            # split into test/train\n",
    "            this_counter = counter + i\n",
    "            set.seed(123*this_counter)\n",
    "            train_ind = createDataPartition(response_set[,resp], p=.75, list = FALSE)\n",
    "            train_df <- pred_resp[train_ind, ]\n",
    "            test_df <- pred_resp[-train_ind, ]\n",
    "            \n",
    "            # if factors were dropped from training or test df, drop from both \n",
    "            if (class(response_set[,resp]) == 'factor') {\n",
    "                avail_levels = intersect(unique(test_df[,resp]), unique(test_df[,resp]))\n",
    "                missing_levels = setdiff(unique(pred_resp[,resp]), avail_levels)\n",
    "                if (length(missing_levels) > 0){\n",
    "                    train_df = train_df[!(train_df[,resp] %in% missing_levels),]\n",
    "                    test_df = test_df[!(test_df[,resp] %in% missing_levels),]\n",
    "                    test_df[,resp] <- droplevels(test_df[,resp])\n",
    "                    train_df[,resp] <- droplevels(train_df[,resp])\n",
    "                }\n",
    "            }\n",
    "\n",
    "            rf_model = randomForest(x = train_df[,sel_vars], y = train_df[,resp], \n",
    "                                    ntree = 250, random_state=this_counter, mtry=opt_mtry_slim)\n",
    "            \n",
    "            rf_model2 = randomForest(x = train_df[,pred_cols], y = train_df[,resp], \n",
    "                                     ntree = 250, random_state=this_counter, mtry=opt_mtry_full,\n",
    "                                    importance=T)\n",
    "\n",
    "            # preditct for both\n",
    "            rf_pred_test = predict(rf_model, newdata = test_df[,sel_vars])\n",
    "            rf_pred_test2 = predict(rf_model2, newdata = test_df[,pred_cols])\n",
    "\n",
    "            # score \n",
    "            if (class(response_set[,resp]) == 'factor') {\n",
    "                rf_cm = confusionMatrix(rf_pred_test, test_df[,resp])\n",
    "                rf_score = 1 - rf_cm$overall['Accuracy']\n",
    "\n",
    "                rf_cm2 = confusionMatrix(rf_pred_test2, test_df[,resp])\n",
    "                rf_score2 = 1 - rf_cm2$overall['Accuracy']\n",
    "            } else {\n",
    "                rf_score = postResample(pred = rf_pred_test, obs = test_df[,resp])[['MAE']]\n",
    "                rf_score2 = postResample(pred = rf_pred_test2, obs = test_df[,resp])[['MAE']]\n",
    "            }    \n",
    "\n",
    "            # report\n",
    "            var_import = as.data.frame(importance(rf_model2, type=1, scale=T))\n",
    "            colnames(var_import) <- paste('MetricOrDeltaMetric', i, sep=\".\")\n",
    "            var_import[paste(trial, 'Full', sep=\".\"), c(1)] = rf_score2\n",
    "            var_import[paste(trial, 'Slim', sep=\".\"), c(1)] = rf_score\n",
    "            var_import\n",
    "        }\n",
    "\n",
    "        stopImplicitCluster()\n",
    "        flush.console()\n",
    "\n",
    "        # move counter forward to initiate new seeds \n",
    "        counter = counter + folds\n",
    "\n",
    "        # take the mean of each fold \n",
    "        scores = apply(results, MARGIN=1, FUN=mean)\n",
    "        for (score_col in names(scores))\n",
    "            vs_trial_df[resp, score_col] = scores[[score_col]]\n",
    "\n",
    "        print(paste('Complete: ', round((counter/(dim(response_set)[2]*3*11))*100, 1), \"%\", sep=\"\" ))\n",
    "        end_time = Sys.time(); print(end_time - start_time);\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "write.table(x = vs_trial_df,\n",
    "            file = \"/Volumes/KeithSSD/CB_V4/otu_data/beta_diversity_analysis/beta_diversity_varsel_final3.txt\", \n",
    "            row.names = T, \n",
    "            col.names = T, \n",
    "            sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7.5, repr.plot.height=5)\n",
    "to_plot1 = t(vs_trial_df[,1:6])\n",
    "to_plot2 = apply(as.data.frame(to_plot1), 2, as.numeric)\n",
    "to_plot2 = cbind.data.frame(to_plot2, factor(c(rep('Full', 3), rep('Reduced', 3))), \n",
    "                 factor(rep(names(data_objects), 2)))\n",
    "colnames(to_plot2) <- c(colnames(to_plot1), 'Model.Size', 'Data.Type')\n",
    "to_plot3 = melt(to_plot2, id.vars=c(\"Data.Type\", 'Model.Size'), \n",
    "                measure.vars=colnames(to_plot1), variable.name='Variable',\n",
    "                value.name='Error')\n",
    "to_plot4 = to_plot3[which(to_plot3$Model.Size != 'Reduced'),]\n",
    "#to_plot4 = to_plot4[which(to_plot4$Variable != 'Pct.Tot.Depth'),]\n",
    "#to_plot4$Variable <- droplevels(to_plot4$Variable)\n",
    "#levels(to_plot4$Variable)\n",
    "to_plot4$Data.Type <- factor(to_plot4$Data.Type, levels=c('ASV.OEU', 'Taxa.Family', 'Fxn.SILVA'))\n",
    "levels(to_plot4$Data.Type) <- c('OEU', 'Fxn', 'Taxa')\n",
    "\n",
    "to_plot4$Variable <-  factor(to_plot4$Variable, levels=c('Years', 'Season', 'Discharge', 'WTEMP', \n",
    "                                                         'Salinity', 'Station.Group', 'Density', \n",
    "                                                         'Habitat', 'NO3', 'NO2', 'NH4', 'CHLA', \n",
    "                                                         'PHEO', 'TP'))\n",
    "\n",
    "bp <- ggplot(to_plot4, aes(x=Data.Type, y=Error, fill=Data.Type)) + \n",
    "      geom_bar(stat=\"identity\", position=position_dodge()) + \n",
    "      facet_wrap(~ Variable, ncol=5) + scale_fill_grey() + \n",
    "      theme_minimal() + theme(text = element_text(size=12), axis.text.x=element_blank())\n",
    "bp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.aov <- aov(Error ~ Data.Type, data = to_plot4)\n",
    "summary(res.aov)\n",
    "TukeyHSD(res.aov)\n",
    "aggregate(to_plot4[,'Error'], by=list(to_plot4[,'Data.Type']), mean)\n",
    "var_perf = aggregate(to_plot4[,'Error'], by=list(to_plot4[,'Variable']), min)\n",
    "var_perf[,'best'] <- NA\n",
    "for (v in 1:nrow(var_perf)){\n",
    "    vsub = to_plot4[which(to_plot4$Variable == var_perf[v, 'Group.1']),]\n",
    "    vstr = paste(vsub[(which(vsub$Error == min(vsub$Error))), 'Data.Type'], collapse=\", \")\n",
    "    var_perf[v ,'best'] <- vstr\n",
    "}\n",
    "var_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7.5, repr.plot.height=5)\n",
    "trial_df = vs_trial_df\n",
    "fxnNames = colnames(data_objects[['Fxn.SILVA']])\n",
    "familyNames = colnames(data_objects[['Taxa.Family']])\n",
    "oeuNames = colnames(data_objects[['ASV.OEU']])\n",
    "Max.VI.Taxa = apply(trial_df[,familyNames], MARGIN=1, FUN=function(x) {max(x, na.rm=T)})\n",
    "Max.VI.Fxns = apply(trial_df[,fxnNames], MARGIN=1, FUN=function(x) {max(x, na.rm=T)})\n",
    "Max.VI.OEU = apply(trial_df[,oeuNames], MARGIN=1, FUN=function(x) {max(x, na.rm=T)})\n",
    "Mean.VI.Taxa = apply(trial_df[,familyNames], MARGIN=1, FUN=function(x) {mean(x, na.rm=T)})\n",
    "Mean.VI.Fxns = apply(trial_df[,fxnNames], MARGIN=1, FUN=function(x) {mean(x, na.rm=T)})\n",
    "Mean.VI.OEU = apply(trial_df[,oeuNames], MARGIN=1, FUN=function(x) {mean(x, na.rm=T)})\n",
    "\n",
    "dropped_df = cbind.data.frame(Max.VI.Taxa, Max.VI.Fxns, Max.VI.OEU,\n",
    "                              Mean.VI.Taxa, Mean.VI.Fxns, Mean.VI.OEU)\n",
    "dropped_df$Variables <- rownames(dropped_df)\n",
    "to_plot5 = to_plot4\n",
    "colnames(to_plot5) <- c('Data.Type', 'Stat', 'Variable', 'Var.Importance')\n",
    "data.types_ = c('Taxa', 'Fxn', 'OEU')\n",
    "to_plot6 = to_plot5\n",
    "to_plot5[,'Stat'] = 'Max'\n",
    "to_plot6[,'Stat'] = 'Mean'\n",
    "for (r in rownames(dropped_df)){\n",
    "    for (c in 1:3){\n",
    "        b1 = to_plot5$Data.Type == data.types_[c]\n",
    "        b2 = to_plot5$Variable == r\n",
    "        to_plot5[which(b1 & b2), 'Var.Importance'] = dropped_df[r, c]\n",
    "        to_plot5[which(b1 & b2), 'Stat'] = 'Max'\n",
    "        to_plot6[which(b1 & b2), 'Var.Importance'] = dropped_df[r, c+3]\n",
    "        to_plot6[which(b1 & b2), 'Stat'] = 'Mean'\n",
    "    }\n",
    "}\n",
    "to_plot7 = rbind.data.frame(to_plot5, to_plot6)\n",
    "\n",
    "bp <- ggplot(to_plot5, aes(x=Data.Type, y=Var.Importance, fill=Data.Type)) + \n",
    "      geom_bar(stat=\"identity\", position=position_dodge()) + \n",
    "      facet_wrap(~ Variable, ncol=5) + scale_fill_grey() + \n",
    "      theme_minimal() + theme(text = element_text(size=12), axis.text.x=element_blank())\n",
    "bp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHLA NO2, WTEMP, Fxn fxnNames\n",
    "# Discharge Taxa familyNames\n",
    "# PHEO and NO3 OEU oeuNames\n",
    "response_set2 = response_set\n",
    "\n",
    "varimport_df = trial_df[,c(familyNames, fxnNames, oeuNames)]\n",
    "good_wc_columns = c('CHLA', 'PHEO', 'NO3', 'NO2', \"Discharge\", \"WTEMP\", 'NH4', 'Season')\n",
    "\n",
    "# extract correlations between environmental data and 16S-based predictors\n",
    "feat_corrs = data.frame(matrix(nrow=ncol(varimport_df), ncol=length(good_wc_columns)))\n",
    "colnames(feat_corrs) <- good_wc_columns\n",
    "rownames(feat_corrs) <- colnames(varimport_df)\n",
    "\n",
    "all_features = cbind.data.frame(data_objects[['Taxa.Family']], \n",
    "                                data_objects[['Fxn.SILVA']], \n",
    "                                data_objects[['ASV.OEU']])\n",
    "for (feat in rownames(feat_corrs)){\n",
    "    for (a_var in colnames(feat_corrs)){\n",
    "        env_pred = cbind.data.frame(response_set2[,a_var], all_features[,feat])\n",
    "        feat_corrs[feat, a_var] = cor(env_pred)[1,2]\n",
    "    }\n",
    "}\n",
    "\n",
    "corr_signs = feat_corrs\n",
    "corr_signs = ifelse(corr_signs < 0, -1, 1)\n",
    "sum(colSums(feat_corrs == 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidf_num_rows=length(data_objects)*length(good_wc_columns)*3\n",
    "\n",
    "var.import.df = data.frame(matrix(nrow=vidf_num_rows, ncol=5))\n",
    "counter = 0\n",
    "colnames(var.import.df) <- c('Data.Type', 'Variable', 'Name', 'Importance', 'Correlation')\n",
    "colsets = list('Taxa'=familyNames, 'Fxn'=fxnNames, 'OEU'=oeuNames)\n",
    "for (a_var in good_wc_columns){\n",
    "    for (data.tipo in names(colsets)){\n",
    "        all_feats = trial_df[a_var, colsets[[data.tipo]]]\n",
    "        all_feats = all_feats[,which(!is.na(all_feats[a_var,]))]\n",
    "        srt_feats = base::sort(all_feats, decreasing = T)[,1:3]\n",
    "        for (rank in 1:3){\n",
    "            counter = counter + 1\n",
    "            var.import.df[counter, 'Data.Type'] = data.tipo\n",
    "            var.import.df[counter, 'Variable'] = a_var\n",
    "            var.import.df[counter, 'Name'] = names(srt_feats)[rank]\n",
    "            var.import.df[counter, 'Importance'] = srt_feats[rank]\n",
    "            var.import.df[counter, 'Correlation'] = feat_corrs[names(srt_feats)[rank], a_var]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "var.import.df[1:25, ]\n",
    "var.import.df[25:50, ]\n",
    "var.import.df[50:72, ]\n",
    "table(var.import.df$Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tenv_data$Cruise.Num <- NA\n",
    "varJD = tenv_data$julian_day\n",
    "tenv_data[ (varJD < 572), 'Cruise.Num'] <- '1'\n",
    "tenv_data[ (varJD > 571) & (varJD < 601), 'Cruise.Num'] <- '2'\n",
    "tenv_data[ (varJD > 600) & (varJD < 621), 'Cruise.Num'] <- '3'\n",
    "tenv_data[ (varJD > 841) & (varJD < 870), 'Cruise.Num'] <- '4'\n",
    "tenv_data[ (varJD > 869) & (varJD < 899), 'Cruise.Num'] <- '5'\n",
    "tenv_data[ (varJD > 898) & (varJD < 919), 'Cruise.Num'] <- '6'\n",
    "tenv_data[ (varJD > 918) & (varJD < 962), 'Cruise.Num'] <- '6'\n",
    "tenv_data[ (varJD > 961) & (varJD < 984), 'Cruise.Num'] <- '7'\n",
    "tenv_data[ (varJD > 983) & (varJD < 1006), 'Cruise.Num'] <- '8'\n",
    "tenv_data[ (varJD > 1005), 'Cruise.Num'] <- '9'\n",
    "table(tenv_data$Cruise.Num)\n",
    "\n",
    "resp_set2 = response_set\n",
    "resp_set2$Cruise.Num = factor(tenv_data$Cruise.Num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=5)\n",
    "\n",
    "cruise.stat = paste(\"C\", resp_set2$Cruise.Num, \n",
    "                    \"S\", as.character(resp_set2$Station.Group), sep=\".\")\n",
    "length(unique(cruise.stat))\n",
    "cruise.stat.levels = expand.grid(paste(\"S\", 1:6, sep=\".\"), paste(\"C\",1:9, sep=\".\"))\n",
    "cruise.stat.levels = paste(cruise.stat.levels$Var2, cruise.stat.levels$Var1, sep=\".\")\n",
    "\n",
    "resp_set2[,'Cruise.Station'] <- droplevels(factor(cruise.stat, levels=cruise.stat.levels))\n",
    "srt_response = resp_set2[with(resp_set2, order(Cruise.Station)),]\n",
    "colorder = rownames(srt_response)\n",
    "oeu_df2 = data_objects[['ASV.OEU']]\n",
    "\n",
    "oeu_df3 = cbind.data.frame(oeu_df2[colorder,], resp_set2[,'Cruise.Station'])\n",
    "colnames(oeu_df3) <- c(colnames(oeu_df2), 'Cruise.Station')\n",
    "oeu_df3[,'id'] <- rownames(oeu_df3)\n",
    "oeu_melt = melt(oeu_df3, id.vars=c(\"id\", 'Cruise.Station'), \n",
    "                measure.vars=colnames(oeu_df2), variable.name='OEU',\n",
    "                value.name='Abund')\n",
    "\n",
    "oeu_melt_agg = aggregate(Abund~Cruise.Station+OEU, oeu_melt, FUN=mean)\n",
    "oeu_df4 = dcast(oeu_melt_agg, Cruise.Station ~ OEU, value.var='Abund')\n",
    "\n",
    "\n",
    "rownames(oeu_df4) <- oeu_df4$Cruise.Station\n",
    "oeu_df4$Cruise.Station <- NULL\n",
    "oeu_vars = apply(oeu_df4, 2, var)\n",
    "sorted_oeu_vars = base::sort(oeu_vars, index.return=T, decreasing=T)\n",
    "hi_var_oeus = names(oeu_vars)[sorted_oeu_vars$ix[1:100]]\n",
    "oeu_df5 = oeu_df4[,hi_var_oeus]\n",
    "srt_oeu = as.matrix(t(oeu_df5))\n",
    "\n",
    "hmanno = data.frame('Season'=c(), 'Latitude'=c())\n",
    "\n",
    "for (n in colnames(srt_oeu)){\n",
    "    samps.lats = tenv_data[which(resp_set2$Cruise.Station == n), 'Latitude']\n",
    "    hmanno[n, 'Latitude'] = mean(samps.lats)\n",
    "    stat.seasonx = tenv_data[which(resp_set2$Cruise.Station == n), 'anti_day_length']\n",
    "    hmanno[n, 'Season'] = mean(stat.seasonx)\n",
    "    stat.month = (as.numeric(tenv_data[which(resp_set2$Cruise.Station == n), 'julian_day'])-365)/365\n",
    "    hmanno[n, 'Years'] = mean(stat.month)\n",
    "}\n",
    "\n",
    "row_ha = HeatmapAnnotation(\n",
    "    'Years' = hmanno$Years,\n",
    "    'Season' = hmanno$Season, \n",
    "    'Latitude' = hmanno$Latitude, \n",
    "    col = list('Years' = colorRamp2(c(min(hmanno$Years),max(hmanno$Years)), c(\"#FEBAB9\", \"#A90301\")),\n",
    "               'Season' = colorRamp2(c(-.93,0.99), c('#F4D6FC', '#780197')),\n",
    "               'Latitude' = colorRamp2(as.vector(quantile(hmanno$Latitude, c(0.05, 0.95))),\n",
    "                                       c(\"#A7FDA7\", \"#018501\")))\n",
    ")\n",
    "\n",
    "\n",
    "col_fun = colorRamp2(as.vector(quantile(srt_oeu, c(0.1, 0.5, 0.9))), c(\"#FFFFFF\", '#888888', \"#000000\"))\n",
    "\n",
    "Heatmap(srt_oeu, col = col_fun, name='LogRPM', top_annotation = row_ha, \n",
    "        cluster_columns=F,  show_row_names = F, show_column_names=F,\n",
    "        column_title_side='bottom', column_title='Samples', row_title='OEUs',\n",
    "        row_title_side='left', gap = unit(1, \"mm\"))\n",
    "?Heatmap\n",
    "#colSide <- brewer.pal(9, \"Set1\")[my_group]\n",
    "#colMain <- colorRampPalette(brewer.pal(8, \"Blues\"))(25)\n",
    "#heatmap(data, Colv = NA, Rowv = NA, scale=\"column\" , RowSideColors=colSide, col=colMain   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=8)\n",
    "Heatmap(srt_oeu, col = col_fun, name='LogAbund', top_annotation = row_ha, \n",
    "        cluster_columns=F,  show_row_names = T, show_column_names=F, row_names_gp = gpar(fontsize = 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_file2 = \"/Volumes/KeithSSD/CB_V4/otu_data/dada2_outputs/taxa_table_with_OTUs.txt\"\n",
    "pre_taxa_df = read.delim(taxa_file2, row.names=1)\n",
    "pre_taxa_df[pre_taxa_df==\"\"] <- NA\n",
    "\n",
    "otu_table_file2 = '/Volumes/KeithSSD/CB_V4/otu_data/final_unrarefied_table.txt'\n",
    "otu_table2 = read.delim(otu_table_file2, row.names=1)[rownames(response_set),]\n",
    "otu_table2 = otu_table2[,which(colSums(otu_table2) > 0)]\n",
    "\n",
    "dim(otu_table2)\n",
    "all_unrare_asvs = colnames(otu_table2)\n",
    "all_unrare_asvs[1:5]\n",
    "taxa_present = pre_taxa_df[all_unrare_asvs,]\n",
    "\n",
    "all_taxa_levels_present = c(as.character(taxa_present$Phylum), \n",
    "                            as.character(taxa_present$Class), \n",
    "                            as.character(taxa_present$Order), \n",
    "                            as.character(taxa_present$Family), \n",
    "                            as.character(taxa_present$Genus))\n",
    "all_taxa_levels_present = all_taxa_levels_present[!is.na(all_taxa_levels_present)]\n",
    "all_taxa_levels_present[1:5]\n",
    "nitro_asvs = which(all_taxa_levels_present %in% c('Nitrospiraceae'))\n",
    "length(nitro_asvs)\n",
    "nitro_asvs = which(all_taxa_levels_present %in% c('Nitrosococcus'))\n",
    "length(nitro_asvs)\n",
    "nitro_asvs = which(all_taxa_levels_present %in% c('Nitrosomonas'))\n",
    "length(nitro_asvs)\n",
    "nitro_asvs = which(all_taxa_levels_present %in% c('Nitrosopumilaceae'))\n",
    "length(nitro_asvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the number of functions and taxa with z-val > 1 \n",
    "num_sig_feats = data.frame()\n",
    "sig_tax = list(); sig_fxn = list();\n",
    "sig_func_set = c(); sig_tax_set = c();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxn_by_taxa_df = read.delim(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/FAPROTAX_Functions_by_family.txt\", \n",
    "                            row.names=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_func_tally = c(); sig_taxa_tally = c();\n",
    "\n",
    "for (gcww in good_wc_columns){\n",
    "    for (gcww_i in gcww){\n",
    "        all_fxns = varimport_df[gcww_i, fxn_cols] #\n",
    "        all_tax = varimport_df[gcww_i, taxa_cols] #\n",
    "        srt_fxn = base::sort(all_fxns, decreasing = T) #\n",
    "        srt_tax = base::sort(all_tax, decreasing = T) #\n",
    "        srt_fxn_sign = srt_fxn; srt_tax_sign = srt_tax; #\n",
    "        for (n in names(srt_fxn_sign)){ srt_fxn_sign[n] = srt_fxn[n]* corr_signs[n, gcww_i] }#\n",
    "        for (n in names(srt_tax_sign)){ srt_tax_sign[n] = srt_tax[n]* corr_signs[n, gcww_i] }#\n",
    "        \n",
    "        # carryout list \n",
    "        sig_fxn[[gcww_i]] = as.vector(names(srt_fxn_sign))\n",
    "        sig_tax[[gcww_i]] = as.vector(names(srt_tax_sign))\n",
    "        # update dataframe\n",
    "        num_sig_feats[gcww_i, 'nfxn'] = length(sig_fxn[[gcww_i]])\n",
    "        num_sig_feats[gcww_i, 'ntax'] = length(sig_tax[[gcww_i]])\n",
    "        # update set\n",
    "        sig_func_set = union(sig_func_set, sig_fxn[[gcww_i]])\n",
    "        sig_tax_set = union(sig_tax_set, sig_tax[[gcww_i]])\n",
    "        \n",
    "        sig_func_tally = c(sig_func_tally, sig_fxn[[gcww_i]])\n",
    "        sig_taxa_tally = c(sig_taxa_tally, sig_tax[[gcww_i]])\n",
    "        \n",
    "        # add annotation data\n",
    "        annotated_taxa = sig_tax[[gcww_i]][sig_tax[[gcww_i]] %in% colnames(fxn_by_taxa_df)]\n",
    "        missing_taxa = sig_tax[[gcww_i]][(sig_tax[[gcww_i]] %in% colnames(fxn_by_taxa_df)) == FALSE]\n",
    "        annotation_submat = fxn_by_taxa_df[sig_fxn[[gcww_i]], annotated_taxa]\n",
    "        if (is.null(dim(annotation_submat))){\n",
    "            annotation_bool = sapply(annotation_submat, sum) > 0\n",
    "        } else{\n",
    "            annotation_bool = apply(annotation_submat, MARGIN=2, sum) > 0\n",
    "        }\n",
    "        \n",
    "        annotation_hits = annotated_taxa[annotation_bool]\n",
    "        annotaion_misses = c(missing_taxa, annotated_taxa[!(annotation_bool)])\n",
    "\n",
    "        num_sig_feats[gcww_i, 'n_tax_classif'] = length(annotation_hits)\n",
    "        num_sig_feats[gcww_i, 'n_tax_not_classif'] = length(annotaion_misses)\n",
    "        \n",
    "        \n",
    "        # pull out some names and numbers \n",
    "        cols_remaining = c('sig_fxns', 'sig_taxa_grps')\n",
    "        \n",
    "        func_corrs = srt_fxn; tax_corrs = srt_tax;\n",
    "        for (n in names(srt_fxn_sign)){ func_corrs[n] = corr_scores[n, gcww_i] }#\n",
    "        for (n in names(srt_tax_sign)){ tax_corrs[n] = corr_scores[n, gcww_i] }#\n",
    "        \n",
    "        sig_fxn_list = paste(sig_fxn[[gcww_i]], round(srt_fxn_sign[sig_fxn[[gcww_i]]],  2), sep=\":\")\n",
    "        cor_fxn_list = paste(sig_fxn[[gcww_i]], round(func_corrs[sig_fxn[[gcww_i]]],  2), sep=\":\")\n",
    "        \n",
    "        sig_taxa_names = sapply(sig_tax[[gcww_i]], FUN=function(y) {strsplit(y, '.', fixed=T)[[1]][2]})\n",
    "        sig_taxa_list = paste(sig_taxa_names, round(srt_tax_sign[sig_tax[[gcww_i]]],  2), sep=\":\")\n",
    "        cor_taxa_list = paste(sig_taxa_names, round(tax_corrs[sig_tax[[gcww_i]]],  2), sep=\":\")\n",
    "        \n",
    "        num_sig_feats[gcww_i, 'sig_fxns'] = paste(sig_fxn_list, collapse=\", \")\n",
    "        num_sig_feats[gcww_i, 'corr_fxns'] = paste(cor_fxn_list, collapse=\", \")\n",
    "        num_sig_feats[gcww_i, 'sig_tax_grps'] = paste(sig_taxa_list, collapse=\", \")\n",
    "        num_sig_feats[gcww_i, 'cor_tax_grps'] = paste(cor_taxa_list, collapse=\", \")\n",
    "        \n",
    "        num_sig_feats[gcww_i, 'classif_tax'] = paste(annotation_hits, collapse=\", \")\n",
    "        num_sig_feats[gcww_i, 'unclassif_tax'] = paste(annotaion_misses, collapse=\", \")\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # see how many overlap between multifunctions\n",
    "    if (length(gcww) > 1) {\n",
    "        fxn_ol = sig_fxn[[gcww[1]]]; tax_ol = sig_tax[[gcww[1]]];\n",
    "        for (i in 2:length(gcww)){\n",
    "            fxn_ol = intersect(sig_fxn[[gcww[i]]], fxn_ol) \n",
    "            tax_ol = intersect(sig_tax[[gcww[i]]], tax_ol)\n",
    "        }\n",
    "        num_sig_feats[gcww, 'ntax_overlap'] <- length(fxn_ol)\n",
    "        num_sig_feats[gcww, 'nfxn_overlap'] <- length(tax_ol)\n",
    "    }\n",
    "}\n",
    "\n",
    "head(base::sort(table(sig_func_tally), decreasing=T))\n",
    "head(base::sort(table(sig_taxa_tally), decreasing=T))\n",
    "\n",
    "print(length(sig_func_set))\n",
    "print(length(sig_tax_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sig_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.table(num_sig_feats, sep=\"\\t\", row.names=T, \n",
    "            file=\"../otu_data/beta_diversity_analysis/variable_importances.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=8)\n",
    "library(corrplot)\n",
    "library(RColorBrewer)\n",
    "\n",
    "subset_bool = complete.cases(env_data)\n",
    "sig_fxn_abunds = fxn_abund_df[subset_bool,sig_func_set]\n",
    "dist_fxn <- dist(t(sig_fxn_abunds), method = 'euclidean')\n",
    "hclust_fxn <- hclust(dist_fxn, method = 'ward.D2')\n",
    "fxns_ordered = colnames(sig_fxn_abunds)[hclust_fxn$order]\n",
    "\n",
    "sig_taxa_abunds = taxa_abund_df[subset_bool,sig_tax_set]\n",
    "dist_tax <- dist(t(sig_taxa_abunds), method = 'euclidean')\n",
    "hclust_tax <- hclust(dist_tax, method = 'ward.D2')\n",
    "tax_ordered = colnames(sig_taxa_abunds)[hclust_tax$order]\n",
    "\n",
    "good_wc_columns2 = c('CHLA', 'PHEO', 'DO', 'NO3F', 'NO2F', \"PC\", 'SALINITY', 'Latitude',\n",
    "                    'faith_pd', 'Year', \"NH4F\", 'Discharge_Susquehanna_14', 'anti_day_length',\n",
    "                    'Month', 'WTEMP', 'StatName', 'DOP', 'habitat')\n",
    "\n",
    "env_numeric = apply(env_data[subset_bool,good_wc_columns2], 2, as.numeric)\n",
    "dist_env <- dist(t(env_numeric), method = 'euclidean')\n",
    "hclust_env <- hclust(dist_env, method = 'ward.D2')\n",
    "env_ordered = colnames(env_numeric)[hclust_env$order]\n",
    "\n",
    "\n",
    "\n",
    "cor.mtest <- function(mat1, mat2) {\n",
    "    mat1 <- as.matrix(mat1)\n",
    "    mat2 <- as.matrix(mat2)\n",
    "    p.mat <- matrix(NA, ncol(mat1), ncol(mat2))\n",
    "    for (i in 1:ncol(mat1)) {\n",
    "        for (j in 1:ncol(mat2)) {\n",
    "            tmp <- cor.test(mat1[, i], mat2[, j])\n",
    "            p.mat[i, j] <- tmp$p.value\n",
    "        }\n",
    "    }\n",
    "    colnames(p.mat) <- colnames(mat2)\n",
    "    rownames(p.mat) <- colnames(mat1)\n",
    "    return(p.mat)\n",
    "}\n",
    "\n",
    "M <- cor(sig_taxa_abunds[,tax_ordered], env_numeric[,env_ordered])\n",
    "M.p <- cor.mtest(sig_taxa_abunds[,tax_ordered], env_numeric[,env_ordered])\n",
    "rownames(M)[rownames(M) == 'nan'] <- 'nan.Unknown'\n",
    "\n",
    "orig_names = rownames(M)\n",
    "clean_taxa_names = unlist(lapply(rownames(M), FUN=function(y) {regmatches(y, regexpr(\"\\\\.\",y), invert = TRUE)[[1]][2]}))\n",
    "\n",
    "rownames(M) <- clean_taxa_names\n",
    "rownames(M.p) <- clean_taxa_names\n",
    "\n",
    "M.2 = M[rowSums(M.p < 0.05) > (0.5)*length(env_ordered), ]\n",
    "M.p.2 = M.p[rowSums(M.p < 0.05) > (0.5)*length(env_ordered), ]\n",
    "\n",
    "orig_names = orig_names[rowSums(M.p < 0.05) > (0.5)*length(env_ordered)]\n",
    "\n",
    "plotname = \"../otu_data/beta_diversity_analysis/env_taxa_correlations2.png\"\n",
    "png(filename = plotname, width = 480*5.3, height = 5.3*620, units = \"px\", res = 300)\n",
    "\n",
    "#tl.cex=2, \n",
    "corrplot(M.2, method='ellipse', order=\"original\", tl.col = \"black\", \n",
    "         col=brewer.pal(n=8, name=\"PuOr\"), p.mat = M.p.2, sig.level = 0.05, insig='blank', \n",
    "         addgrid.col=NA, cl.cex=1.8, tl.cex=0.8, cl.align.text='l')\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fxn = c(\"oxygenic_photoautotrophy\", 'hydrocarbon_degradation')\n",
    "a_taxa = c(\"Planctomycetales.Gimesiaceae\")\n",
    "env_vars = c(\"DO\", 'NH4F', 'CHLA', 'PC')\n",
    "pre_env = cbind.data.frame(env_data[,env_vars], taxa_abund_df[,a_taxa], fxn_abund_df[,a_fxn])\n",
    "colnames(pre_env) <- c(env_vars, a_taxa, a_fxn)\n",
    "pre_env2 = pre_env[rowSums(is.na(pre_env)) == 0,]\n",
    "cor(pre_env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1 = orig_names[69:76]\n",
    "clust1.1 = orig_names[66:68]\n",
    "clust2 = orig_names[60:65]\n",
    "clust3 = orig_names[52:59]\n",
    "clust4 = orig_names[41:51]\n",
    "clust5 = orig_names[32:40]\n",
    "clust6 = orig_names[18:31]\n",
    "clust7 = orig_names[6:17]\n",
    "clust8 = orig_names[1:5]\n",
    "\n",
    "cluster_names = list(c(clust1, clust1.1), clust2, clust3, clust4, clust5, clust6, clust7, clust8)\n",
    "\n",
    "indicator_fxn = c('SALINITY', 'WTEMP', 'DO', 'NH4F')\n",
    "\n",
    "env_data$depth_segment <- as.character(env_data$depth_segment)\n",
    "env_data[is.na(env_data$depth_segment), 'depth_segment'] = 'Mid'\n",
    "env_data$depth_segment <- as.integer(factor(env_data$depth_segment, levels = c('Bottom', 'Mid', 'Top')))\n",
    "\n",
    "indicator_numeric = apply(env_data[subset_bool, indicator_fxn], 2, as.numeric)\n",
    "summary_df = data.frame(matrix(nrow=length(cluster_names), ncol=length(indicator_fxn)*2))\n",
    "rownames(summary_df) <- paste('Cluster', 1:length(cluster_names), sep=\".\")\n",
    "namestobe <- c(paste('Mean', indicator_fxn, sep=\".\"), paste('SD', indicator_fxn, sep=\".\"))\n",
    "name_order = base::sort(namestobe, decreasing=T)\n",
    "colnames(summary_df) <- name_order\n",
    "\n",
    "for (i in 1:length(cluster_names)){\n",
    "    this_clust = cluster_names[[i]]\n",
    "    this_clust = this_clust[this_clust != 'nan.Unknown']\n",
    "    clust_abunds = rowSums(sig_taxa_abunds[,this_clust])\n",
    "    hotspots = clust_abunds > unlist(quantile(clust_abunds, c(.75)))[[1]]\n",
    "    summary_data = c(apply(indicator_numeric[hotspots,], 2, mean), apply(indicator_numeric[hotspots,], 2, sd))  \n",
    "    names(summary_data) <- c(paste('Mean', indicator_fxn, sep=\".\"), paste('SD', indicator_fxn, sep=\".\"))\n",
    "    name_order = base::sort(names(summary_data), decreasing=T)\n",
    "    summary_df[rownames(summary_df)[i], ] = summary_data[name_order]\n",
    "    \n",
    "}\n",
    "\n",
    "summary_df_toprint = round(summary_df[order(summary_df$Mean.WTEMP),c(5, 1, 6, 2, 7, 3, 8, 4)], 2)\n",
    "write.table(summary_df_toprint, file=\"../otu_data/beta_diversity_analysis/cluster_env_table.txt\", sep=\"\\t\",\n",
    "           row.names=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxn_by_taxa_df = read.delim(\"/Users/login/Google Drive/SiYi_Xiaotong_Materials/FAPROTAX_Functions_by_family.txt\", row.names=1)\n",
    "print(sum(fxns_ordered %in% rownames(fxn_by_taxa_df)))\n",
    "print(sum(tax_ordered %in% colnames(fxn_by_taxa_df)))\n",
    "\n",
    "missing_taxa = tax_ordered[(tax_ordered %in% colnames(fxn_by_taxa_df)) == FALSE]\n",
    "annotated_taxa = tax_ordered[tax_ordered %in% colnames(fxn_by_taxa_df)]\n",
    "\n",
    "sig_fbt = fxn_by_taxa_df[fxns_ordered, annotated_taxa]\n",
    "insig_fbt = fxn_by_taxa_df[fxns_ordered,(colnames(fxn_by_taxa_df) %in% annotated_taxa)==F]\n",
    "\n",
    "sig_fbt_pa = sig_fbt\n",
    "sig_fbt_pa[sig_fbt_pa>0] = 1\n",
    "\n",
    "fxn_by_taxa_df_pa = fxn_by_taxa_df[fxns_ordered,]\n",
    "fxn_by_taxa_df_pa[fxn_by_taxa_df_pa>0] = 1\n",
    "aggregated_annots = data.frame('hits.to.sig.predictive.taxa' = c(), 'other.hits' = c())\n",
    "\n",
    "for (sigf in fxns_ordered){    \n",
    "    sig_fbt_sigf = base::sort(unlist(sig_fbt[sigf,]), decreasing =T)\n",
    "    sigfilt = sig_fbt_sigf[sig_fbt_sigf > 0]\n",
    "    sigfilt_str = paste(names(sigfilt), round(sigfilt, 2), sep=\":\")\n",
    "    aggregated_annots[sigf, 'hits.to.sig.predictive.taxa'] = paste(sigfilt_str, collapse=\", \")\n",
    "\n",
    "    insig_fbt_sigf = base::sort(unlist(insig_fbt[sigf,]), decreasing = T )\n",
    "    insigfilt = insig_fbt_sigf[insig_fbt_sigf > 0]\n",
    "    insigfilt_str = paste(names(insigfilt), round(insigfilt, 2), sep=\":\")\n",
    "    aggregated_annots[sigf, 'other.hits'] = paste(insigfilt_str, collapse=\", \")\n",
    "}\n",
    "\n",
    "sig_fbt_relabund <- t(scale(t(sig_fbt), center=F, scale=rowSums(fxn_by_taxa_df[fxns_ordered,])))\n",
    "percent_annotated_useful = cbind.data.frame(round(rowSums(sig_fbt_relabund),2), # percent of significant predictors annotated in\n",
    "                                            rowSums(fxn_by_taxa_df[fxns_ordered,]), # total counts of annotated OTUS\n",
    "                                            round(rowSums(sig_fbt_pa)/rowSums(fxn_by_taxa_df_pa), 2), # number of significantOrders with annotated members \n",
    "                                            rowSums(fxn_by_taxa_df_pa)) # total genera annotated \n",
    "\n",
    "\n",
    "#aggregated_annots\n",
    "colnames(percent_annotated_useful) <- c('percent.OTUS.sig.predictors.classified',\n",
    "                                        'total.counts.classified.OTUS',\n",
    "                                        'counts.sig.orders.classified',\n",
    "                                        'counts.orders.classified')\n",
    "full_pack = cbind.data.frame(percent_annotated_useful, aggregated_annots)\n",
    "\n",
    "write.table(full_pack, file = \"../otu_data/beta_diversity_analysis/classification_scoring.txt\",\n",
    "            sep=\"\\t\", row.names=T)\n",
    "full_pack"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
