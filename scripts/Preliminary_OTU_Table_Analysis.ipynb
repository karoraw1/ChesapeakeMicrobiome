{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this file is to load the ASV table, load and compare clustered sequences, check each for structural artifacts with a PCA paired with a Chi-Squared test, make some diagnostic figures, remove low frequency features, compare UNIFRAC-type and conventional distances between samples, and then make a hierarchy visualize the similarities between samples, and save the final output matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load packages and OTU abundance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in abundance table with 441 rows and 54467 columns\n",
      "Read in metadata table with 413 rows and 77 columns\n",
      "After removing others' samples abundance table size is (407, 54467)\n",
      "['Index', 'Short sample name', 'SampleID', 'BioSampleID', 'BiologicalReplicate', 'TechnicalReplicate', 'StationName', 'DateMMDDYY', 'DepthName', 'Treatment', '16S region', 'Sampling date (MMDDYY)', 'Sampling notes', 'Storage Notes', 'Filter date', 'Filter ID', 'Filter group', 'Filter Position', 'Filter notes', 'DNA extraction date', 'DNA extraction ID', 'DNA extraction notes', 'qubit date', 'qubit ID', 'qubit notes', 'qPCR date', 'qPCR ct', 'qPCR ID', 'qPCR notes', '1st step date', '1st step cycle number', '1st step plate', '1st step well', '1st step cycle ID', '1st step cycle notes', 'cleanup date', 'cleanup type', 'cleanup id', 'cleanup notes', '2nd step date', '2nd step barcode sequence', '2nd step barcode well', '2nd step well', '2nd step plate', '2nd step cycle no', '2nd step cycle ID', '2nd step cycle notes', '2nd step clean up date', '2nd step clean up initials', '2nd step clean up notes', 'Multiplex date', 'Multiplex initials', 'Multiplex notes', 'sequencing date', 'sequencing platform', 'sequencing length', 'sequencing reads', 'sequencing ID', 'sequencing notes', 'data folder name', 'sequencing file forward name', 'sequencing file reverse name', 'sequencing file index name', 'mapping file name', 'Resequencing files', 'Shotgun metagenomics date', 'Shotgun metagenomics Library Prep', 'Shotgun Metagenomics ID', 'Shotgun Metagenomics sequencing plaform', 'Forward read length', 'Reverse read length', 'Shotgun Metagenomics sequencing folder', 'Shotgun Metagenomics library barcode', 'Shotgun Metagenomics forward read', 'Shotgun Metagenomics reverse read', 'new_date', 'Demux_Bool']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "# load raw abundance data \n",
    "abund_feather = '/Volumes/KeithSSD/CB_V4/otu_data/tree_data/hq_asv_table.feather'\n",
    "abund_tsv = \"/Volumes/KeithSSD/CB_V4/otu_data/tree_data/hq_asv_table.tsv\"\n",
    "if not os.path.exists(abund_feather):\n",
    "    abund_df = pd.read_csv(abund_tsv, sep=\"\\t\")\n",
    "    abund_df.to_feather(abund_feather)\n",
    "else:\n",
    "    abund_df = pd.read_feather(abund_feather, use_threads=True).set_index('Samples')\n",
    "    \n",
    "print(\"Read in abundance table with {} rows and {} columns\".format(abund_df.shape[0], abund_df.shape[1]))\n",
    "\n",
    "# load full sample sheet\n",
    "config_file = \"/Volumes/KeithSSD/CB_V4/otu_scripts/config.yml\"\n",
    "with open(config_file, 'r') as stream:\n",
    "    cfg_dict = yaml.safe_load(stream)\n",
    "\n",
    "data_dir = cfg_dict['data_directory']\n",
    "sample_sheet_fn = cfg_dict['sample_sheet']\n",
    "sample_sheet = pd.read_csv('/Volumes/KeithSSD/CB_V4/otu_data/SampleSheet_052019.tsv', sep=\"\\t\")\n",
    "print(\"Read in metadata table with {} rows and {} columns\".format(sample_sheet.shape[0], sample_sheet.shape[1]))\n",
    "\n",
    "# Fix weird date\n",
    "sample_sheet.loc[sample_sheet['DateMMDDYY'] == 'Mix9', 'DateMMDDYY'] = '100516'\n",
    "\n",
    "# make weird samples (not mine or controls, some not in sample sheet) their own group \n",
    "weird_samples = set(abund_df.index) - set(sample_sheet.SampleID.unique())\n",
    "abund_df_jm = abund_df.drop(weird_samples, axis=0)\n",
    "print(\"After removing others' samples abundance table size is {}\".format(abund_df_jm.shape))\n",
    "print(list(sample_sheet.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Prep the sample sheet \n",
    "\n",
    "In this block, we make sure every sample has a Depth value and correct a few mislabellings. We then add geoposition, collection agency, library sizes, collection agency, and recoded sequencing run IDs to the metadata table. In the last part, we create a list of control libraries for detecting contamination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 and 387 libraries incremented\n",
      "390 and 375 libraries incremented\n",
      "375 and 341 libraries incremented\n",
      "353 and 297 libraries incremented\n",
      "310 and 185 libraries incremented\n",
      "154 and 31 libraries incremented\n",
      "43 and 6 libraries incremented\n",
      "2 and 0 libraries incremented\n",
      "0 and 0 libraries incremented\n",
      "119 125 0\n",
      "148 0 0\n",
      "15 (407,)\n",
      "['esakows1_132789' 'esakows1_152133_plate_1' 'esakows1_152133_plate_2'\n",
      " 'Keith_Maeve1_138650' 'Miseq_data_SarahPreheim_Sept2016'\n",
      " 'sprehei1_123382' 'sprehei1_149186']\n",
      "['e_13' 'e_15_1' 'e_15_2' 'KM' 'Miseq_sp' 'spr12' 'spr14']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/login/miniconda3/envs/skbio2/lib/python3.6/site-packages/ipykernel_launcher.py:51: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    }
   ],
   "source": [
    "sample_sheet2 = sample_sheet.set_index('SampleID')\n",
    "sub_sample_sheet = sample_sheet2.loc[abund_df_jm.index, :]\n",
    "sub_sample_sheet.loc[sub_sample_sheet.DepthName.isnull(), 'DepthName'] = 'LAB'\n",
    "sub_sample_sheet.loc[sub_sample_sheet.DepthName == 'Surface', 'DepthName'] = '0'\n",
    "sub_sample_sheet.loc[[i for i in sub_sample_sheet.index if 'FiltCtrl' in i], 'StationName'] = 'CB44'\n",
    "select_metadata = ['DateMMDDYY', 'StationName', 'DepthName', 'sequencing ID']\n",
    "\n",
    "lat_lon_fn = \"/Volumes/KeithSSD/CB_V4/otu_data/CB_Locations.tsv\"\n",
    "lat_lon = pd.read_csv(lat_lon_fn, sep=\"\\t\", index_col=0)\n",
    "cb2lat, cb2lon = {'LAB': 00.0}, {'LAB': 00.0}\n",
    "\n",
    "for stat_ in lat_lon.index:\n",
    "    if stat_.replace(\".\", \"\") in sub_sample_sheet.StationName.unique():\n",
    "        cb2lat[stat_.replace(\".\", \"\")] = round(lat_lon.loc[stat_, 'Latitude'], 5) \n",
    "        cb2lon[stat_.replace(\".\", \"\")] = round(lat_lon.loc[stat_, 'Longitude'], 5) \n",
    "\n",
    "sub_sample_sheet['Latitude'] = sub_sample_sheet.StationName.map(cb2lat)\n",
    "sub_sample_sheet['Longitude'] = sub_sample_sheet.StationName.map(cb2lon)\n",
    "\n",
    "read_counts = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/trim_stats/read_counts.tsv\", sep=\"\\t\", header=None, index_col=0)\n",
    "sub_reads = read_counts.loc[sub_sample_sheet.index, :]\n",
    "sub_reads.columns = ['RawCount', 'TrimCount']\n",
    "\n",
    "read_bins = [5e3, 1e4, 2.5e4, 5e4, 1e5, 2.5e5, 5e5, 1e6, 2.5e6]\n",
    "read_discrete = pd.DataFrame(index=sub_reads.index, columns=['RawCount_b', 'TrimCount_b'],\n",
    "                            data=np.zeros(sub_reads.shape))\n",
    "for rb in read_bins:\n",
    "    r_bool = sub_reads['RawCount'] >= rb\n",
    "    read_discrete.loc[r_bool, 'RawCount_b'] += 1\n",
    "    t_bool = sub_reads['TrimCount'] >= rb\n",
    "    read_discrete.loc[t_bool, 'TrimCount_b'] += 1\n",
    "    print(\"{} and {} libraries incremented\".format(r_bool.sum(), t_bool.sum()))\n",
    "\n",
    "meta_data_df = sub_sample_sheet.join(sub_reads).join(read_discrete)\n",
    "\n",
    "md_df = meta_data_df.copy()\n",
    "odu_set = set(md_df[md_df['Short sample name'].str.contains(\"ODU\") | \n",
    "                    md_df[\"Sampling notes\"].str.contains(\"ODU\")].index)\n",
    "dnr_set = set(md_df[md_df['Short sample name'].str.contains(\"DNR\") | \n",
    "                    md_df[\"Sampling notes\"].str.contains(\"DNR\")].index)\n",
    "\n",
    "dnr_set.update([i for i in md_df.index if 'FiltCtrl' in i])\n",
    "print(len(odu_set), len(dnr_set), len(odu_set.intersection(dnr_set)))\n",
    "\n",
    "non_pl = odu_set.union(dnr_set)\n",
    "md_df_other = md_df.loc[~md_df.index.isin(non_pl), :]\n",
    "possibly_pl = set(md_df_other[md_df_other.StationName == 'CB33C'].index)\n",
    "\n",
    "print(len(possibly_pl), len(possibly_pl.intersection(odu_set)), len(possibly_pl.intersection(dnr_set)))\n",
    "\n",
    "meta_data_df = meta_data_df.join(pd.Series(index=meta_data_df.index, name=\"CollectionAgency\"))\n",
    "meta_data_df.loc[odu_set, 'CollectionAgency'] = 'ODU'\n",
    "meta_data_df.loc[dnr_set, 'CollectionAgency'] = 'DNR'\n",
    "meta_data_df.loc[possibly_pl, 'CollectionAgency'] = 'Preheim'\n",
    "\n",
    "print(meta_data_df.CollectionAgency.isnull().sum(), meta_data_df.CollectionAgency.shape)\n",
    "\n",
    "sid_map = {'esakows1_132789': 'e_13',\n",
    "           'controls': 'controls',\n",
    "           'esakows1_152133_plate_1': 'e_15_1',\n",
    "           'esakows1_152133_plate_2': 'e_15_2',\n",
    "           'Keith_Maeve1_138650': 'KM',\n",
    "           'Miseq_data_SarahPreheim_Sept2016': 'Miseq_sp',\n",
    "           'sprehei1_123382': 'spr12',\n",
    "           'sprehei1_149186': 'spr14'}\n",
    "\n",
    "print(meta_data_df[\"sequencing ID\"].unique())\n",
    "meta_data_df.loc[:, 'sequencing_ID'] = meta_data_df.loc[:, 'sequencing ID'].map(sid_map)\n",
    "select_metadata.remove(\"sequencing ID\"); select_metadata.append(\"sequencing_ID\");\n",
    "print(meta_data_df.loc[:, \"sequencing_ID\"].unique())\n",
    "\n",
    "\n",
    "control_libs = (['178A_WaterBathControlA', '178B_WaterBathControlB'])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"Blank\")])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"Mix9\")])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"CDSBBR\")])\n",
    "control_libs += list(abund_df_jm.index[abund_df_jm.index.str.contains(\"EMPTY\")])\n",
    "control_libs += list(abund_df_jm.index[meta_data_df['Short sample name'].str.contains(\"_Neg\")])\n",
    "control_libs += list(abund_df_jm.index[meta_data_df['Short sample name'].str.contains(\"ML0\")])\n",
    "control_libs += list(abund_df_jm.index[meta_data_df['Short sample name'].str.contains(\"_Pos\")])\n",
    "control_libs = list(set(control_libs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contamination filer \n",
    "\n",
    "Here we can see the log mean relative abundance of only those OTUs present in control libraries plotted against the same value in experimental libraries. Those with a higher mean in the controls(colored red) were removed while the green ones are kept. Those with a zero mean in the experimental samples are artificially placed at -20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; import matplotlib.pyplot as plt;\n",
    "from scipy.stats import gmean\n",
    "sns.set(style=\"white\", color_codes=True);\n",
    "dens_name = \"../otu_data/pca_plots/contamination_filters_nc.png\"\n",
    "point_name = \"../otu_data/pca_plots/contamination_filters.png\"\n",
    "if os.path.exists(dens_name) or os.path.exists(point_name):\n",
    "    ctrls = control_libs\n",
    "    abund_table = abund_df.copy()\n",
    "    abund_ra = pd.DataFrame(index=abund_table.index, \n",
    "                            data=np.log(abund_table.values + 1), \n",
    "                            columns=abund_table.columns).astype(float)\n",
    "    #abund_ra = abund_table.div(abund_table.sum(1), axis=0)\n",
    "\n",
    "    # mean relative abundance greater in controls\n",
    "    control_otus3 = abund_table.columns[abund_table.loc[ctrls, :].sum() > 0]\n",
    "    total_in_controls3 = abund_ra.loc[ctrls, control_otus3].apply(np.median)\n",
    "    total_in_non_controls3 = abund_ra.loc[~abund_ra.index.isin(ctrls), control_otus3].apply(np.median)\n",
    "    mostly_in_controls3 = control_otus3[(total_in_controls3 >= total_in_non_controls3)]\n",
    "    otus_to_strip3 = set(mostly_in_controls3)\n",
    "    print(len(otus_to_strip3), \"otus set to strip\")\n",
    "    \n",
    "    to_plot = pd.concat((total_in_controls3, total_in_non_controls3), axis=1)\n",
    "    to_plot.columns = ['control median', 'non-control median']\n",
    "    col_switch = lambda x: 1 if x in otus_to_strip3 else 0\n",
    "    to_plot = to_plot.reset_index()\n",
    "    to_plot['currentMethod'] = to_plot['index'].apply(col_switch)\n",
    "    print(to_plot.isnull().sum())\n",
    "\n",
    "    g = sns.jointplot(x=\"control median\", y=\"non-control median\", data=to_plot, kind=\"hex\", height=7)\n",
    "    g.savefig(dens_name, dpi=150)\n",
    "\n",
    "    g.ax_joint.cla()\n",
    "\n",
    "    for i in to_plot.index:\n",
    "        if to_plot.loc[i, 'currentMethod'] == 1:\n",
    "            g.ax_joint.plot(to_plot.loc[i, \"control median\"], \n",
    "                            to_plot.loc[i, \"non-control median\"], color='red', marker='o', alpha=.6, markersize=5)\n",
    "        else:\n",
    "            g.ax_joint.plot(to_plot.loc[i, \"control median\"], \n",
    "                            to_plot.loc[i, \"non-control median\"], color='green', marker='o', alpha=.6, markersize=5)\n",
    "\n",
    "    g.set_axis_labels('log control median', \"log non-control median\", fontsize=16)\n",
    "    g.savefig(point_name, dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we will define a function that drops features present mostly in controls and samples with low yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing sparsity of full table:\n",
      "4155 are to be removed as contaminants\n",
      "14286 are to be removed after adding low abundance OTUs\n",
      "36946 are to be removed after adding low freq OTUs\n",
      "Removing samples with fewer features than 100\n",
      "Removed libraries are:\n",
      "\tName\tNonzero Features\tTotal Abundances\n",
      "\tSB072516TAWCSCB33CDSBBR1TR1I125\t260\t31966\n",
      "\tSB061815TAWCSCB33CD3BR1TR1I6\t56\t37918\n",
      "\tSB082015TAWCSCB33CD6BR1TR1I57\t10\t2858\n",
      "\t178B_WaterBathControlB\t71\t31650\n",
      "\tSB072215TAWCSCB33CD8BR1TR1I35\t98\t2778\n",
      "\tSB072215TAWCSCB33CD5BR1TR1I32\t3\t12\n",
      "\tSB072215TAWCSCB33CD15BR1TR1I43\t82\t10113\n",
      "\tSB082015TAWCSCB33CDSBBR1TR1I50\t860\t15911\n",
      "\tSB062917TAWCSLABDPCBR1TR1I607\t957\t168231\n",
      "\tSB092116TAWCSLABDNCBR1TR1I247\t336\t77790\n",
      "\tSB071116TAWCSCB41CD31BR1TR1I149\t121\t1479\n",
      "\tSB062716TAWCSCB33CDSBBR1TR1I93\t60\t46131\n",
      "\tSB091817TAWCSCB54D25BR1TR1I504\t1\t1\n",
      "\tSB071017TAWCSCB62D9BR1TR1I414\t1\t1\n",
      "\t96_ZymoControl_R1\t12\t79268\n",
      "\tSB072215TAWCSCB33CD22BR1TR1I46\t90\t13137\n",
      "\tSB062716TAWCSCB33CD0BR2TR1I78\t38\t113839\n",
      "\t93_PBS_Blank_Control\t501\t127166\n",
      "\tSB081216TAWCSCB61D11BR2TR1I193\t113\t1312\n",
      "\tSB091817TAWCSCB54D25BR2TR1I515\t115\t1237\n",
      "\tSB062716TAWCSLABDPCBR1TR1I602\t46\t136777\n",
      "\tSB061815TAWCSCB33CD16BR1TR1I20\t1\t1\n",
      "\tSB062716TAWCSLABDNCBR1TR1I601\t806\t128339\n",
      "\tSB062917TAWCSLABDPCBR1TR3I608\t1123\t270484\n",
      "\t92_Mix93_Control_R2\t668\t88132\n",
      "\tSBMix9TAWCSLABDPCBR1TR2I270\t39\t88282\n",
      "\tSB062917TAWCSLABDPCBR1TR2I606\t1003\t253815\n",
      "\t92_Mix93_Control_R1\t783\t211132\n",
      "\tSB061815TAWCSCB33CDSBBR1TR1I2\t649\t10424\n",
      "\tSB061815TAWCSCB33CD15BR1TR1I19\t15\t190\n",
      "\tSB061815TAWCSCB33CD12BR1TR1I16\t2\t5\n",
      "\tSB082015TAWCSCB33CD0BR1TR1I51\t24\t23782\n",
      "\tSB062917TAWCSLABDPCBR1TR4I609\t541\t27388\n",
      "\tSB061815TAWCSCB33CD10BR2TR1I14\t1\t4\n",
      "\tSB061815TAWCSCB33CD5BR1TR1I8\t1\t1\n",
      "\tSB072215TAWCSCB33CD14BR1TR1I42\t70\t19656\n",
      "\tSB091817TAWCSCB73D12BR1TR1I512\t41\t278\n",
      "\tSB072215TAWCSCB33CDSBBR1TR1I26\t353\t5852\n",
      "\tSB072215TAWCSCB33CD1BR1TR1I28\t1\t1\n",
      "\tSB072215TAWCSCB33CDEBBR1TR1I47\t55\t92961\n",
      "\tSB062716TAWCSCB33CD20BR1TR1I91\t36\t102667\n",
      "\tSB092616TAWCSLABDNABR1TR1I268\t94\t3554\n",
      "\t178A_WaterBathControlA\t92\t61938\n",
      "\tSB082015TAWCSCB33CD21BR1TR1I75\t1\t2\n",
      "\tSB082916TAWCSCB71D20BR1TR1I239\t34\t274\n",
      "\n",
      "(407, 54467), (362, 17521) are shapes after bad otus and low yield samples\n"
     ]
    }
   ],
   "source": [
    "def decrease_sparsity(abund_table, ctrls, abund_thresh=0.002, div_thresh=100, rare_thresh=3000, addl_keys=[]):\n",
    "    \"\"\"Takes an abundance table and a list of control indexes\n",
    "       Removes OTUs with 50% or more of their abundances in controls.\n",
    "       Removes OTUS below user set abundances threshold.\n",
    "       Removes features below rarefaction threshold.\n",
    "       Removes additional features according to string matched key\"\"\"\n",
    "    otus_to_strip = set()\n",
    "    \n",
    "    abund_ra = pd.DataFrame(index=abund_table.index, data=np.log(abund_table.values + 1), \n",
    "                            columns=abund_table.columns).astype(float)\n",
    "    #control_otus = abund_ra.columns[abund_ra.loc[ctrls, :].sum() > 0]\n",
    "    control_otus = abund_table.columns[abund_table.loc[ctrls, :].sum() > 0]\n",
    "    #total_in_controls = abund_ra.loc[ctrls, control_otus].apply(np.mean)\n",
    "    total_in_controls = abund_ra.loc[ctrls, control_otus].apply(np.median)\n",
    "    #total_in_non_controls = abund_ra.loc[~abund_ra.index.isin(ctrls), control_otus].apply(np.mean)\n",
    "    total_in_non_controls = abund_ra.loc[~abund_ra.index.isin(ctrls), control_otus].apply(np.median)\n",
    "    #mostly_in_controls = control_otus[(total_in_controls >= total_in_non_controls)]\n",
    "    mostly_in_controls = control_otus[(total_in_controls >= total_in_non_controls)]\n",
    "    # mean relative abundance greater in controls\n",
    "    otus_to_strip.update(list(mostly_in_controls))\n",
    "    print(\"{} are to be removed as contaminants\".format(len(otus_to_strip)))\n",
    "    abund_ra = abund_table.div(abund_table.sum(1), axis=0)\n",
    "    low_abund = abund_table.columns[(abund_ra > abund_thresh).sum() == 0]\n",
    "    otus_to_strip.update(low_abund)\n",
    "    print(\"{} are to be removed after adding low abundance OTUs\".format(len(otus_to_strip)))\n",
    "    presence_absence = ((abund_table > 0)).astype(int)\n",
    "    infrequent_appearances = abund_table.columns[presence_absence.sum() < 2]\n",
    "    otus_to_strip.update(infrequent_appearances)\n",
    "    print(\"{} are to be removed after adding low freq OTUs\".format(len(otus_to_strip)))\n",
    "    div_samples = presence_absence.sum(1)\n",
    "    print(\"Removing samples with fewer features than {}\".format(div_thresh))\n",
    "    abund_to_return = abund_table.copy()\n",
    "    abund_tot = abund_to_return.sum(1)\n",
    "    for ak in addl_keys: \n",
    "        ctrls += list(abund_table.index[abund_table.index.str.contains(ak)])\n",
    "    \n",
    "    ctrls += list(abund_table.index[div_samples < div_thresh])\n",
    "    ctrls += list(abund_table.index[ abund_tot < rare_thresh])\n",
    "    preamble = \"\\tName\\tNonzero Features\\tTotal Abundances\\n\"\n",
    "    fmt_libs = preamble+\"\".join([\"\\t{}\\t{}\\t{}\\n\".format(cl, div_samples[cl], abund_tot[cl]) for cl in set(ctrls)])\n",
    "    print(\"Removed libraries are:\\n{}\".format(fmt_libs))\n",
    "    \n",
    "    abund_to_return.loc[~(abund_table.index.isin(ctrls)), abund_table.columns.isin(otus_to_strip)].to_csv(\"/Volumes/KeithSSD/ChesapeakeMicrobiome/data/otu_tables/contaminant_table.txt\", sep=\"\\t\")\n",
    "    abund_to_return = abund_to_return.loc[~(abund_table.index.isin(ctrls)), \n",
    "                                          ~abund_table.columns.isin(otus_to_strip)]\n",
    "    \n",
    "    print(\"{}, {} are shapes after bad otus and low yield samples\".format(abund_table.shape, abund_to_return.shape))\n",
    "    return abund_to_return\n",
    "\n",
    "print(\"Decreasing sparsity of full table:\")\n",
    "\n",
    "abund_df_tr = decrease_sparsity(abund_df_jm.copy(), control_libs, abund_thresh=0., addl_keys=['Zymo'])\n",
    "\n",
    "#abund_df_og_s1 = decrease_sparsity(abund_df_jm.copy(), control_libs, abund_thresh=0.002, addl_keys=['Zymo'])\n",
    "\n",
    "\n",
    "#from skbio.stats.distance import mantel\n",
    "#from skbio.diversity import beta_diversity\n",
    "\n",
    "#bc_dists2 = beta_diversity(\"braycurtis\", abund_df_tr.loc[abund_df_og_s1.index, :].values, abund_df_og_s1.index)\n",
    "#bc_dists1 = beta_diversity(\"braycurtis\", abund_df_og_s1.values, abund_df_og_s1.index)\n",
    "#r_pp, p_value_pp, n_pp = mantel(bc_dists2, bc_dists1, method='pearson')\n",
    "#print(r_pp, p_value_pp, n_pp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 54467)\n",
      "(362, 17521)\n",
      "(362, 1031)\n",
      "(362, 1561)\n",
      "1025\n",
      "Samples\n",
      "SB062716TAWCSCB33CD12BR2TR1I87     168881.0\n",
      "SB062716TAWCSCB33CD1BR2TR1I80      114534.0\n",
      "SB071116TAWCSCB22D11BR2TR2I616     149527.0\n",
      "SB071116TAWCSCB33CD24BR1TR2I611    146711.0\n",
      "SB071116TAWCSCB51D33BR1TR1I153     171377.0\n",
      "dtype: float64\n",
      "                                   OTU1   OTU2    OTU3    OTU4    OTU5\n",
      "Samples                                                               \n",
      "SB062716TAWCSCB33CD12BR2TR1I87   1419.0   96.0  1522.0  2406.0  3684.0\n",
      "SB062716TAWCSCB33CD1BR2TR1I80    4556.0   19.0  1099.0  1907.0  4937.0\n",
      "SB071116TAWCSCB22D11BR2TR2I616    861.0   12.0   408.0  4032.0  1514.0\n",
      "SB071116TAWCSCB33CD24BR1TR2I611   367.0  525.0  1744.0  4655.0  4787.0\n",
      "SB071116TAWCSCB51D33BR1TR1I153    191.0  393.0  2431.0  2252.0  2010.0\n",
      "OTU28186    0.000054\n",
      "OTU11966    0.000095\n",
      "OTU18996    0.000134\n",
      "OTU5675     0.000149\n",
      "OTU16262    0.000163\n",
      "dtype: float64\n",
      "OTU10     0.160094\n",
      "OTU1      0.183840\n",
      "OTU366    0.204166\n",
      "OTU21     0.227783\n",
      "OTU2      0.369178\n",
      "dtype: float64\n",
      "OTU45541    0.000004\n",
      "OTU41959    0.000005\n",
      "OTU44747    0.000005\n",
      "OTU41421    0.000006\n",
      "OTU36428    0.000006\n",
      "dtype: float64\n",
      "OTU1221    0.001962\n",
      "OTU2189    0.001962\n",
      "OTU6940    0.001962\n",
      "OTU5004    0.001963\n",
      "OTU5256    0.001966\n",
      "OTU1843    0.001973\n",
      "OTU5967    0.001976\n",
      "OTU6260    0.001976\n",
      "OTU690     0.001976\n",
      "OTU8662    0.001978\n",
      "OTU3251    0.001981\n",
      "OTU5859    0.001982\n",
      "OTU760     0.001982\n",
      "OTU1891    0.001986\n",
      "OTU1047    0.003457\n",
      "OTU320     0.006305\n",
      "OTU155     0.013762\n",
      "OTU138     0.024643\n",
      "OTU18      0.098345\n",
      "OTU6       0.118573\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rare_fn = \"../otu_data/final_rarefied_table.tsv\"\n",
    "rare_abund = pd.read_csv(rare_fn, sep=\"\\t\", index_col=0)\n",
    "\n",
    "print(abund_df_jm.shape)\n",
    "print(abund_df_tr.shape)\n",
    "print(abund_df_og_s1.shape)\n",
    "print(rare_abund.shape)\n",
    "rarified_otus = set(rare_abund.columns)\n",
    "print(len(rarified_otus.intersection(set(abund_df_og_s1.columns))))\n",
    "\n",
    "rare_total = abund_df_jm.loc[rare_abund.index, :].sum(1).astype(float)\n",
    "didntmakeit = set(abund_df_tr.columns) - rarified_otus\n",
    "print(rare_total.head())\n",
    "print(abund_df_jm.loc[rare_abund.index, rare_abund.columns].iloc[:5, :5].astype(float))\n",
    "relabund_jm = abund_df_jm.loc[rare_abund.index, rare_abund.columns].astype(float).div(rare_total, axis=0)\n",
    "print(relabund_jm.max().sort_values().head())\n",
    "print(relabund_jm.max().sort_values().tail())\n",
    "\n",
    "\n",
    "relabund_jm2 = abund_df_jm.loc[rare_abund.index, didntmakeit].astype(float).div(rare_total, axis=0)\n",
    "print(relabund_jm2.max().sort_values().head())\n",
    "print(relabund_jm2.max().sort_values().tail(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More metadata prep work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(362, 17605) (362, 17521) (407, 84)\n",
      "Month         0\n",
      "Year          0\n",
      "Month_Year    0\n",
      "dtype: int64\n",
      "(362, 1115) (362, 1031) (407, 84)\n",
      "Month         0\n",
      "Year          0\n",
      "Month_Year    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sal_dict = {\"Oligohaline\": ['CB22', 'CB31'],\n",
    "            \"Mesohaline\": ['CB32', 'CB33C', 'CB41C', 'CB42C', 'CB43C', 'CB44', 'CB51', 'CB52', 'CB53', 'CB54'],\n",
    "            \"Polyhaline\": ['CB61', 'CB62', 'CB63', 'CB64', 'CB71', 'CB72', 'CB73', 'CB74']}\n",
    "sal_dictx = {k:i for i, j in sal_dict.items() for k in j}\n",
    "\n",
    "some_columns = ['DepthName', 'Month', 'DateMMDDYY', 'Longitude', 'CollectionAgency', 'Salinity_Group', 'sequencing_ID']\n",
    "\n",
    "def merge_data_and_fix_dates_depths_sal_regions(abund_df, md_df, sal_dict):\n",
    "    super_df = pd.concat([abund_df, md_df.loc[abund_df.index, :]], axis=1)\n",
    "    print(super_df.shape, abund_df.shape, md_df.shape)\n",
    "    super_df.DepthName = super_df.DepthName.apply(lambda x: \"0\"+x if len(x) == 1 else x)\n",
    "    super_df['Month'] = super_df.DateMMDDYY.apply(lambda x: x[:2])\n",
    "    super_df['Year'] = super_df.DateMMDDYY.apply(lambda x: x[-2:])\n",
    "    super_df['Month_Year'] = super_df.loc[:, ['Month', 'Year']].apply(lambda x: \" \".join(x), axis=1)\n",
    "    sort_vars = ['Year', 'Month', 'Latitude', 'DepthName']\n",
    "    super_df.sort_values(sort_vars, ascending=[True, True, False, True], inplace=True)\n",
    "    # add salinity group\n",
    "    super_df['Salinity_Group'] = super_df.StationName.map(sal_dict)\n",
    "    print(super_df[['Month', 'Year', 'Month_Year']].isnull().sum())\n",
    "    return super_df\n",
    "\n",
    "super_df_plus_garbage = merge_data_and_fix_dates_depths_sal_regions(abund_df_tr, meta_data_df, sal_dictx)\n",
    "super_df = merge_data_and_fix_dates_depths_sal_regions(abund_df_og_s1, meta_data_df, sal_dictx)\n",
    "\n",
    "some_indexes = super_df.index[[0,1,2,100,101,102,200,201,202,-3,-2,-1]]\n",
    "\n",
    "#super_df.loc[some_indexes, some_columns]\n",
    "\n",
    "super_df_plus_garbage[some_columns].sample(15, random_state=22)\n",
    "\n",
    "\n",
    "super_df_plus_garbage.loc['SB072516TAWCSCB33CD0BR2TR1I96', 'DepthName'] = '02'\n",
    "super_df.loc['SB072516TAWCSCB33CD0BR2TR1I96', 'DepthName'] = '02'\n",
    "\n",
    "super_df_plus_garbage.loc['SB060117TAWCSCB33CD8BR2TR1I546', 'DepthName'] = '09'\n",
    "super_df.loc['SB060117TAWCSCB33CD8BR2TR1I546', 'DepthName'] = '09'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give the gift of environmental metadata from our EXO probe to the rows collected by our lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty times and locations\n",
      "Time         0\n",
      "Latitude2    0\n",
      "dtype: int64\n",
      "Probe measurment rows: 157\n",
      "Unique depth time pairs: 152\n",
      "Measurment rows after replicates averaged: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/login/miniconda3/envs/skbio2/lib/python3.6/site-packages/ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "072516 matched to ['072516'] (14, 1122)\n",
      "0.0 matched to ['00'] (2, 1122) \n",
      "\n",
      "072516 matched to ['072516'] (14, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "072516 matched to ['072516'] (14, 1122)\n",
      "4.0 matched to ['04'] (2, 1122) \n",
      "\n",
      "072516 matched to ['072516'] (14, 1122)\n",
      "6.0 matched to ['06'] (2, 1122) \n",
      "\n",
      "072516 matched to ['072516'] (14, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n",
      "Measurement at 072516 - 10.0 not matched to a sample\n",
      "\n",
      "Measurement at 072516 - 12.0 not matched to a sample\n",
      "\n",
      "072516 matched to ['072516'] (14, 1122)\n",
      "14.0 matched to ['14'] (1, 1122) \n",
      "\n",
      "Measurement at 071717 - 0.0 not matched to a sample\n",
      "\n",
      "071717 matched to ['071717'] (5, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "Measurement at 071717 - 4.0 not matched to a sample\n",
      "\n",
      "071717 matched to ['071717'] (5, 1122)\n",
      "6.0 matched to ['06'] (1, 1122) \n",
      "\n",
      "Measurement at 071717 - 8.0 not matched to a sample\n",
      "\n",
      "071717 matched to ['071717'] (5, 1122)\n",
      "10.0 matched to ['10'] (1, 1122) \n",
      "\n",
      "071717 matched to ['071717'] (5, 1122)\n",
      "12.0 matched to ['12'] (1, 1122) \n",
      "\n",
      "071717 matched to ['071717'] (5, 1122)\n",
      "14.0 matched to ['14'] (1, 1122) \n",
      "\n",
      "Measurement at 071717 - 16.0 not matched to a sample\n",
      "\n",
      "Measurement at 071717 - 18.0 not matched to a sample\n",
      "\n",
      "Measurement at 071717 - 20.0 not matched to a sample\n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "0.0 matched to ['00'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "1.0 matched to ['01'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "2.0 matched to ['02'] (2, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "3.0 matched to ['03'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "4.0 matched to ['04'] (2, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "5.0 matched to ['05'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "6.0 matched to ['06'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "7.0 matched to ['07'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "9.0 matched to ['09'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "10.0 matched to ['10'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "11.0 matched to ['11'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "12.0 matched to ['12'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "13.0 matched to ['13'] (1, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "14.0 matched to ['14'] (2, 1122) \n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "15.0 matched to ['15'] (1, 1122) \n",
      "\n",
      "Measurement at 060117 - 16.0 not matched to a sample\n",
      "\n",
      "Measurement at 060117 - 17.0 not matched to a sample\n",
      "\n",
      "060117 matched to ['060117'] (21, 1122)\n",
      "21.0 matched to ['21'] (1, 1122) \n",
      "\n",
      "Measurement at 060117 - 20.0 not matched to a sample\n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "0.0 matched to ['00'] (1, 1122) \n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "Measurement at 081216 - 4.0 not matched to a sample\n",
      "\n",
      "Measurement at 081216 - 6.0 not matched to a sample\n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "10.0 matched to ['10'] (1, 1122) \n",
      "\n",
      "Measurement at 081216 - 12.0 not matched to a sample\n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "13.0 matched to ['13'] (1, 1122) \n",
      "\n",
      "Measurement at 081216 - 14.0 not matched to a sample\n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "15.0 matched to ['15'] (1, 1122) \n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "16.0 matched to ['16'] (2, 1122) \n",
      "\n",
      "081216 matched to ['081216'] (9, 1122)\n",
      "18.0 matched to ['18'] (1, 1122) \n",
      "\n",
      "Measurement at 081216 - 20.0 not matched to a sample\n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "0.0 matched to ['00'] (1, 1122) \n",
      "\n",
      "Measurement at 072215 - 1.0 not matched to a sample\n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "3.0 matched to ['03'] (1, 1122) \n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "4.0 matched to ['04'] (1, 1122) \n",
      "\n",
      "Measurement at 072215 - 5.0 not matched to a sample\n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "6.0 matched to ['06'] (1, 1122) \n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "7.0 matched to ['07'] (1, 1122) \n",
      "\n",
      "Measurement at 072215 - 8.0 not matched to a sample\n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "9.0 matched to ['09'] (1, 1122) \n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "10.0 matched to ['10'] (2, 1122) \n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "12.0 matched to ['12'] (1, 1122) \n",
      "\n",
      "Measurement at 072215 - 15.0 not matched to a sample\n",
      "\n",
      "Measurement at 072215 - 16.0 not matched to a sample\n",
      "\n",
      "Measurement at 072215 - 17.0 not matched to a sample\n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "18.0 matched to ['18'] (1, 1122) \n",
      "\n",
      "Measurement at 072215 - 19.0 not matched to a sample\n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "20.0 matched to ['20'] (1, 1122) \n",
      "\n",
      "Measurement at 072215 - 21.0 not matched to a sample\n",
      "\n",
      "Measurement at 072215 - 23.0 not matched to a sample\n",
      "\n",
      "Measurement at 072215 - 22.0 not matched to a sample\n",
      "\n",
      "Measurement at 072215 - 14.0 not matched to a sample\n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "13.0 matched to ['13'] (1, 1122) \n",
      "\n",
      "072215 matched to ['072215'] (14, 1122)\n",
      "11.0 matched to ['11'] (1, 1122) \n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "1.0 matched to ['01'] (1, 1122) \n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "Measurement at 061815 - 3.0 not matched to a sample\n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "4.0 matched to ['04'] (1, 1122) \n",
      "\n",
      "Measurement at 061815 - 5.0 not matched to a sample\n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "6.0 matched to ['06'] (1, 1122) \n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "7.0 matched to ['07'] (1, 1122) \n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n",
      "Measurement at 061815 - 9.0 not matched to a sample\n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "10.0 matched to ['10'] (1, 1122) \n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "11.0 matched to ['11'] (1, 1122) \n",
      "\n",
      "Measurement at 061815 - 12.0 not matched to a sample\n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "13.0 matched to ['13'] (1, 1122) \n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "14.0 matched to ['14'] (1, 1122) \n",
      "\n",
      "Measurement at 061815 - 15.0 not matched to a sample\n",
      "\n",
      "Measurement at 061815 - 16.0 not matched to a sample\n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "17.0 matched to ['17'] (1, 1122) \n",
      "\n",
      "061815 matched to ['061815'] (13, 1122)\n",
      "18.0 matched to ['18'] (1, 1122) \n",
      "\n",
      "061517 matched to ['061517'] (6, 1122)\n",
      "0.0 matched to ['00'] (1, 1122) \n",
      "\n",
      "061517 matched to ['061517'] (6, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "Measurement at 061517 - 4.0 not matched to a sample\n",
      "\n",
      "061517 matched to ['061517'] (6, 1122)\n",
      "6.0 matched to ['06'] (1, 1122) \n",
      "\n",
      "061517 matched to ['061517'] (6, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n",
      "Measurement at 061517 - 10.0 not matched to a sample\n",
      "\n",
      "Measurement at 061517 - 12.0 not matched to a sample\n",
      "\n",
      "061517 matched to ['061517'] (6, 1122)\n",
      "14.0 matched to ['14'] (1, 1122) \n",
      "\n",
      "Measurement at 061517 - 16.0 not matched to a sample\n",
      "\n",
      "Measurement at 061517 - 18.0 not matched to a sample\n",
      "\n",
      "Measurement at 061517 - 20.0 not matched to a sample\n",
      "\n",
      "061517 matched to ['061517'] (6, 1122)\n",
      "21.5 matched to ['21'] (1, 1122) \n",
      "\n",
      "Measurement at 062716 - 0.0 not matched to a sample\n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "1.0 matched to ['01'] (3, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "4.0 matched to ['04'] (1, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "6.0 matched to ['06'] (1, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "10.0 matched to ['10'] (1, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "12.0 matched to ['12'] (3, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "14.0 matched to ['14'] (1, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "16.0 matched to ['16'] (1, 1122) \n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "18.0 matched to ['18'] (1, 1122) \n",
      "\n",
      "Measurement at 062716 - 20.0 not matched to a sample\n",
      "\n",
      "062716 matched to ['062716'] (15, 1122)\n",
      "21.0 matched to ['21'] (1, 1122) \n",
      "\n",
      "Measurement at 081417 - 0.0 not matched to a sample\n",
      "\n",
      "081417 matched to ['081417'] (5, 1122)\n",
      "2.0 matched to ['02'] (1, 1122) \n",
      "\n",
      "Measurement at 081417 - 4.0 not matched to a sample\n",
      "\n",
      "081417 matched to ['081417'] (5, 1122)\n",
      "6.0 matched to ['06'] (1, 1122) \n",
      "\n",
      "081417 matched to ['081417'] (5, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "081417 matched to ['081417'] (5, 1122)\n",
      "10.0 matched to ['10'] (1, 1122) \n",
      "\n",
      "Measurement at 081417 - 12.0 not matched to a sample\n",
      "\n",
      "Measurement at 081417 - 14.0 not matched to a sample\n",
      "\n",
      "Measurement at 081417 - 16.0 not matched to a sample\n",
      "\n",
      "Measurement at 081417 - 18.0 not matched to a sample\n",
      "\n",
      "081417 matched to ['081417'] (5, 1122)\n",
      "20.0 matched to ['20'] (1, 1122) \n",
      "\n",
      "Measurement at 082015 - 0.0 not matched to a sample\n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "1.0 matched to ['01'] (1, 1122) \n",
      "\n",
      "Measurement at 082015 - 2.0 not matched to a sample\n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "3.0 matched to ['03'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "4.0 matched to ['04'] (1, 1122) \n",
      "\n",
      "Measurement at 082015 - 5.0 not matched to a sample\n",
      "\n",
      "Measurement at 082015 - 6.0 not matched to a sample\n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "7.0 matched to ['07'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "8.0 matched to ['08'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "9.0 matched to ['09'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "10.0 matched to ['10'] (2, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "11.0 matched to ['11'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "12.0 matched to ['12'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "13.0 matched to ['13'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "14.0 matched to ['14'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "15.0 matched to ['15'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "16.0 matched to ['16'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "17.0 matched to ['17'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "18.0 matched to ['18'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "19.0 matched to ['19'] (1, 1122) \n",
      "\n",
      "082015 matched to ['082015'] (18, 1122)\n",
      "20.0 matched to ['20'] (1, 1122) \n",
      "\n",
      "Measurement at 082015 - 21.0 not matched to a sample\n",
      "\n",
      "113/120 mbio rows matched from 152 measurement rows\n",
      "Time                             244\n",
      "Actual Depth (m)                 244\n",
      "Temp (°C)                        244\n",
      "Conductivity (μS/cm)             244\n",
      "Specific Conductivity (μS/cm)    244\n",
      "Total Dissolved Solids (mg/L)    244\n",
      "Salinity (PSU)                   244\n",
      "ODO (% sat)                      257\n",
      "ODO (mg/L)                       258\n",
      "Pressure (psi)                   258\n",
      "pH                               246\n",
      "ORP (mV)                         246\n",
      "Turbidity (FNU)                  246\n",
      "Latitude2                        244\n",
      "Longitude2                       244\n",
      "dtype: int64\n",
      "Time                             249\n",
      "Actual Depth (m)                 249\n",
      "Temp (°C)                        249\n",
      "Conductivity (μS/cm)             249\n",
      "Specific Conductivity (μS/cm)    249\n",
      "Total Dissolved Solids (mg/L)    249\n",
      "Salinity (PSU)                   249\n",
      "ODO (% sat)                      262\n",
      "ODO (mg/L)                       263\n",
      "Pressure (psi)                   263\n",
      "pH                               251\n",
      "ORP (mV)                         251\n",
      "Turbidity (FNU)                  251\n",
      "Latitude2                        249\n",
      "Longitude2                       249\n",
      "dtype: int64\n",
      "(357, 1135) (362, 17624) (113, 15)\n"
     ]
    }
   ],
   "source": [
    "def date2(a):\n",
    "    return float(a[:2])/(12) + float(a[2:4])/(365.3333) + (float(a[4:])+2000)\n",
    "\n",
    "super_matched3 = super_df.copy()[super_df.DepthName != 'LAB']\n",
    "super_matched3['date_float'] = super_matched3.DateMMDDYY.apply(date2)\n",
    "\n",
    "# read in compiled probe data, parse time and date columns\n",
    "exo_file = \"../otu_data/WaterQualityData/PreheimProbeData/Compiled_EXO_Probe.xlsx\"\n",
    "probe_df = pd.read_excel(exo_file, \"Sheet1\", parse_dates=[0,1])\n",
    "\n",
    "# pull out column names containing data, so replicates can be combined\n",
    "data_cols = list(probe_df.columns[3:])\n",
    "\n",
    "# reformat date string and create date float to match both cols in microbiome data\n",
    "date_str = lambda d: f'{d.month:02}' + f'{d.day:02}' + str(d.year - 2000)\n",
    "probe_df['Date'] = probe_df.Date.apply(lambda x: date_str(pd.to_datetime(x).date()))\n",
    "probe_df['date_float'] = probe_df.Date.apply(date2)\n",
    "\n",
    "# pandas assumes time column corresponds to today's date, fix this in rows where time is defined\n",
    "gotthetime = probe_df.Time.notnull(); makeTime = lambda x: pd.to_datetime(x).time(); \n",
    "probe_df.loc[gotthetime, 'Time'] = probe_df.loc[gotthetime, 'Time'].apply(makeTime)\n",
    "print(\"Empty times and locations\\n{}\".format(probe_df.loc[:, ['Time', 'Latitude2']].isnull().sum()))\n",
    "\n",
    "# collect values needed to match probe & microbe data into a single column\n",
    "probe_df['SpaceTime'] = probe_df.loc[:, ['Assumed Depth (m)', 'date_float', 'Date']].apply(tuple, axis=1)\n",
    "print(\"Probe measurment rows: {}\".format(probe_df.shape[0]))\n",
    "print(\"Unique depth time pairs: {}\".format(probe_df.SpaceTime.unique().shape[0]))\n",
    "\n",
    "# find rows with identical dates and depths\n",
    "unq_dates, date_cnts = np.unique(probe_df.SpaceTime.values, return_counts=1)\n",
    "double_dates = unq_dates[date_cnts > 1]\n",
    "\n",
    "# assign an average of the data columns to the first replicate row \n",
    "dropped_doubles = []\n",
    "for dd in double_dates:\n",
    "    dup_ind = probe_df[probe_df.SpaceTime == dd].index\n",
    "    probe_df.loc[dup_ind[0], data_cols] = probe_df.loc[dup_ind, data_cols].mean()\n",
    "    dropped_doubles += list(dup_ind[1:])\n",
    "\n",
    "# drop all but the first replicate\n",
    "probe_df = probe_df.loc[~probe_df.index.isin(dropped_doubles), :]\n",
    "print(\"Measurment rows after replicates averaged: {}\".format(probe_df.shape[0]))\n",
    "\n",
    "# set space time as the index, verify integrity to double ensure no duplicates\n",
    "probe_df.set_index(['Assumed Depth (m)', 'date_float', 'Date'], inplace=True, verify_integrity=True)\n",
    "probe_df.drop('SpaceTime', axis=1, inplace=True)\n",
    "\n",
    "# remove transect rows \n",
    "super_33 = super_matched3[(super_matched3.StationName == 'CB33C') & \n",
    "                         (super_matched3.CollectionAgency == 'Preheim')].copy()\n",
    "\n",
    "# create a map between microbiome rows and environmental data rows\n",
    "spti_lookup = {}\n",
    "\n",
    "# iterate through environmental data rows\n",
    "for spti in probe_df.index:\n",
    "    # unpack space time attributes and make a copy of the microbiome data to cut up\n",
    "    depth_, datef_, date_ = spti; super_copy = super_33.copy();\n",
    "    # create a column of the difference between all date floats and this date\n",
    "    super_copy['date_diff'] = abs(super_copy.date_float - datef_)\n",
    "    # remove any rows not equal to the minimum\n",
    "    date_matched = super_copy[super_copy.date_diff == super_copy.date_diff.min()]\n",
    "    # make a column of the abs. value diff between depths (converted to integers, fractions rounded down)\n",
    "    date_matched['depthdiff'] = abs(date_matched.DepthName.astype(int) - int(depth_))\n",
    "    # remove anything not equal to the minimum\n",
    "    depth_matched = date_matched[date_matched.depthdiff == date_matched.depthdiff.min()]\n",
    "    # count the number of rows that pass both filters and the number of different depth deltas\n",
    "    n_matches, v_matches = depth_matched.depthdiff.unique().shape[0], list(depth_matched.depthdiff.unique())    \n",
    "    # if there is is only one matched depth and the match is perfect\n",
    "    if ( n_matches == 1) and v_matches == [0.]:\n",
    "        print(date_, \"matched to\" , date_matched['DateMMDDYY'].unique(), date_matched.shape)\n",
    "        print(depth_, \"matched to\", depth_matched['DepthName'].unique(), depth_matched.shape, \"\\n\")\n",
    "        for matched_i in  list(depth_matched.index):\n",
    "            spti_lookup[matched_i] = spti\n",
    "    else:\n",
    "        print(\"Measurement at {} - {} not matched to a sample\\n\".format(date_, depth_))\n",
    "\n",
    "\n",
    "probe_df2 = pd.DataFrame(index=spti_lookup.keys(), columns=['Time']+data_cols[1:])\n",
    "for k, v in spti_lookup.items():\n",
    "    probe_df2.loc[k, probe_df2.columns] = probe_df.loc[v, probe_df2.columns]\n",
    "\n",
    "print(\"{}/{} mbio rows matched from {} measurement rows\".format(probe_df2.shape[0], super_33.shape[0],\n",
    "                                                               probe_df.shape[0]))\n",
    "def correct_latlon(spdf):\n",
    "    better_coords = spdf.Latitude2.notnull() & spdf.Longitude2.notnull()\n",
    "    spdf.loc[better_coords, 'Latitude'] = spdf.loc[better_coords, 'Latitude2']\n",
    "    spdf.loc[better_coords, 'Longitude'] = spdf.loc[better_coords, 'Longitude2']\n",
    "    return spdf.drop(['Latitude2', 'Longitude2'], axis=1)\n",
    "\n",
    "super_matched2 = super_matched3.join(probe_df2)\n",
    "super_df_plus_garbage2 = super_df_plus_garbage.join(probe_df2)\n",
    "\n",
    "print(super_matched2[probe_df2.columns].isnull().sum())\n",
    "print(super_df_plus_garbage2[probe_df2.columns].isnull().sum())\n",
    "print(super_matched2.shape, super_df_plus_garbage2.shape, probe_df2.shape)\n",
    "\n",
    "super_matched = correct_latlon(super_matched2)\n",
    "super_df_plus_garbage3 = correct_latlon(super_df_plus_garbage2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_otu_cols = np.array([i for i in super_matched.columns if not i.startswith(\"OTU\")])\n",
    "needed_cols = ['StationName', 'DateMMDDYY', 'DepthName', 'qPCR ct', 'CollectionAgency', \n",
    "               'sequencing_ID', 'TrimCount']\n",
    "exported_data = super_matched.loc[super_matched.CollectionAgency != 'Preheim', needed_cols]\n",
    "exported_data.to_csv(\"/Volumes/KeithSSD/ChesapeakeMicrobiome/data/SampleProcessingData.txt\", sep=\"\\t\",\n",
    "                    header = True, index=True, index_label='SampleID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in water quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_file = \"../otu_data/WaterQualityData/melted_wq_07to17.tsv\"\n",
    "if not os.path.exists(melted_file):\n",
    "    db_file = \"../otu_data/WaterQualityData/WaterQualityData_07to17.tsv\"\n",
    "    #df_raw = pd.read_csv(db_file, sep=\"\\t\", parse_dates=[['SampleDate', 'SampleTime']], low_memory=False)\n",
    "    df_raw = pd.read_csv(db_file, sep=\"\\t\", low_memory=False)\n",
    "    df_nn = df_raw[df_raw.MeasureValue.notnull()]\n",
    "    df_nn['SampleDate_SampleTime'] = df_nn.SampleDate.apply(pd.to_datetime)\n",
    "    summer_idxs = set()\n",
    "    for yr_i, yr in enumerate(range(2012,2018)):\n",
    "        left_side = pd.to_datetime(str(yr)+'-03-15')\n",
    "        right_side = pd.to_datetime(str(yr)+'-10-15')\n",
    "        date_range_ = (df_nn.SampleDate_SampleTime >= left_side) & (df_nn.SampleDate_SampleTime < right_side)\n",
    "        summer_idxs.update(df_nn[date_range_].index)\n",
    "        print(\"Window {} is {} to {}, grabbed {} total\".format(yr_i, left_side, right_side, len(summer_idxs)))\n",
    "\n",
    "    df_nv = df_nn.loc[summer_idxs, :]\n",
    "    df_nv.loc[df_nv.Problem == 'NV', 'Problem'] = np.nan\n",
    "    df_nv.loc[df_nv.Problem == 'QQ', 'Problem'] = np.nan\n",
    "    df_nv.loc[df_nv.Problem == 'WW', 'Problem'] = np.nan\n",
    "    df_np = df_nv[df_nv.Problem.isnull()]\n",
    "\n",
    "    time_to_seconds = lambda x: int(x[:2])*3600 + int(x[3:5])*60 + int(x[6:])\n",
    "    df_np['SecondsElapsed'] = df_np.SampleTime.apply(time_to_seconds)\n",
    "    needed_cols = ['TotalDepth', 'UpperPycnocline', 'LowerPycnocline', 'Latitude', 'Longitude', 'SecondsElapsed']\n",
    "    idx_cols = [\"SampleDate_SampleTime\", \"Station\", \"Depth\"]\n",
    "    new_df = pd.pivot_table(data = df_np, index = idx_cols, columns = 'Parameter', \n",
    "                            values = 'MeasureValue', aggfunc=np.mean).sort_index()\n",
    "    \n",
    "    addl_cols = df_np.loc[:, idx_cols+needed_cols].groupby(idx_cols).agg(np.mean).sort_index()\n",
    "    wq_melted = new_df.join(addl_cols)\n",
    "    print(wq_melted.columns)\n",
    "    secondsToTime = lambda x: \"{:02}:{:02}:{:02}\".format(x//3600,x%3600//60, x%3600%60)\n",
    "    wq_melted['Time'] = wq_melted.SecondsElapsed.apply(secondsToTime)\n",
    "    wq_melted.drop('SecondsElapsed', axis=1, inplace=True)\n",
    "    wq_melted.to_csv(melted_file, sep=\"\\t\")\n",
    "else:\n",
    "    print(\"Reading melted water quality data from file\")\n",
    "    wq_melted_df = pd.read_csv(melted_file, sep=\"\\t\")\n",
    "    print(\"Df size {}\".format(wq_melted_df.shape))\n",
    "    print(\"First index: \", wq_melted_df.iloc[0, :3].values, \"\\nLast Index: \", wq_melted_df.iloc[-1, :3].values)\n",
    "    print(wq_melted_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq_melted_df[(wq_melted_df.Station == \"CB3.3C\") & (wq_melted_df.SampleDate_SampleTime == '2017-09-26')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match water quality data from Chesapeake Bay Foundation to Our Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that aren't our lab's collections\n",
    "super_transect = super_matched[super_matched.CollectionAgency != 'Preheim'].copy()\n",
    "\n",
    "# make a better date float for both \n",
    "wq_melted_df['date_float'] = pd.to_datetime(wq_melted_df['SampleDate_SampleTime']).astype(np.int64) // 10**9\n",
    "super_transect['date_float'] = pd.to_datetime(super_transect['DateMMDDYY'], format=\"%m%d%y\").astype(np.int64) // 10**9\n",
    "\n",
    "# count nulls\n",
    "wq_melted_df['null_count'] = wq_melted_df.isnull().sum(1)\n",
    "\n",
    "# create a space time variable with depths\n",
    "super_transect['SpaceTime'] = super_transect.loc[:, ['DateMMDDYY', 'date_float', 'StationName', 'DepthName']].apply(tuple, axis=1)\n",
    "\n",
    "# create an alternate water data for intergrating \n",
    "isint = lambda x: True if x%1.0 == 0 else False\n",
    "water_nzd = wq_melted_df[(wq_melted_df.Depth != 0) & wq_melted_df.Depth.apply(isint)]\n",
    "print(water_nzd.shape, wq_melted_df.shape, super_transect.shape, super_transect['SpaceTime'].unique().shape)\n",
    "\n",
    "# create a container for density profile data (excluding depths in tuple)\n",
    "idx_names = ['DateMMDDYY', 'date_float', 'StationName']\n",
    "wqm_idx = pd.MultiIndex.from_tuples([tuple(i[:3]) for i in super_transect.SpaceTime.unique()], names=idx_names)\n",
    "wqm_df = pd.DataFrame(index=wqm_idx, columns=['PycDepth', 'BVF', 'FoldChange']).sort_index()\n",
    "data_cols = wq_melted_df.columns[3:-8]\n",
    "print(data_cols)\n",
    "\n",
    "mbio_to_wdr = {}\n",
    "delta_limits = {'days':4.5, 'nulls':15, 'depth': 2.} \n",
    "match_num = super_transect['SpaceTime'].unique().shape[0]\n",
    "wqm_done = set()\n",
    "total_iterations = 0\n",
    "\n",
    "for sd_i, sd_x in enumerate(super_transect['SpaceTime'].unique()):\n",
    "    # unpack two dates and station\n",
    "    mdy_t, ux_t, stat_, depth_ = sd_x\n",
    "    # reformat station to CBF designation\n",
    "    stat_mod = stat_[:3] +\".\" + stat_[3:5]\n",
    "    # Subset by station because that requires perfect matches\n",
    "    stat_select_nzc = water_nzd[water_nzd.Station == stat_mod].copy()\n",
    "    stat_select_wmd = wq_melted_df[wq_melted_df.Station == stat_mod].copy()\n",
    "    print(\"{}/{}. Station {} selected rows: {} and {}\".format(sd_i, match_num, stat_mod,\n",
    "                                                              stat_select_nzc.shape[0], \n",
    "                                                              stat_select_wmd.shape[0]))\n",
    "    # search for the best row that matches the criteria\n",
    "    matched_yet = False\n",
    "    while not matched_yet:\n",
    "        # calculate time deltas\n",
    "        stat_select_wmd['date_diff'] = abs(stat_select_wmd.date_float - ux_t)/3600/24\n",
    "        stat_select_nzc['date_diff'] = abs(stat_select_nzc.date_float - ux_t)/3600/24\n",
    "\n",
    "        # calculate gradient\n",
    "        if not tuple(sd_x[:3]) in wqm_done:\n",
    "            time_select = stat_select_nzc[stat_select_nzc.date_diff == stat_select_nzc.date_diff.min()]\n",
    "            select_densities = time_select.sort_values(['Depth']).loc[:, ['Depth', 'SIGMA_T']]\n",
    "            depth_gradient = np.gradient(select_densities.SIGMA_T.values, select_densities.Depth.values)\n",
    "            select_densities['BVF'] = pd.Series(depth_gradient, index=select_densities.index)\n",
    "            select_densities = select_densities[select_densities.Depth > 2.0]\n",
    "            if select_densities.BVF.isnull().sum() <= 2:\n",
    "                s_x = (mdy_t, ux_t, stat_)\n",
    "                wqm_df.loc[s_x, 'PycDepth'] = select_densities.loc[select_densities['BVF'].idxmax(), 'Depth']\n",
    "                wqm_df.loc[s_x, 'BVF'] = select_densities.loc[select_densities['BVF'].idxmax(), 'BVF']\n",
    "                wqm_df.loc[s_x, 'FoldChange'] = wqm_df.loc[s_x, 'BVF']/np.median(select_densities['BVF'])\n",
    "            wqm_done.update(tuple(sd_x[:3]))\n",
    "            \n",
    "        # remove any rows not within the boundary\n",
    "        date_matched = stat_select_wmd[stat_select_wmd.date_diff == stat_select_wmd.date_diff.min()]\n",
    "        # make a column of the abs. value diff between depths (converted to integers, fractions rounded down)\n",
    "        if float(depth_) > date_matched.TotalDepth.max():\n",
    "            print(\"water column too shallow triggered {} > {}\".format(depth_, date_matched.Depth.max()))\n",
    "            depth_ = date_matched.Depth.max()\n",
    "\n",
    "        date_matched['depthdiff'] = abs(date_matched.Depth - float(depth_))\n",
    "\n",
    "        # remove anything to distant in the water column \n",
    "        depth_matched = date_matched[date_matched.depthdiff < delta_limits['depth']]\n",
    "        depth_matched['row_score'] = depth_matched.loc[:, ['depthdiff', 'null_count']].sum(1)\n",
    "        if not depth_matched.empty:\n",
    "            best_row = depth_matched[depth_matched.row_score == depth_matched.row_score.min()]\n",
    "            print(mdy_t, \"matched at a time delta of {} days\".format(best_row.date_diff.min()))\n",
    "            if best_row.shape[0] == 2:\n",
    "                best_row.drop(best_row.index[0], inplace=True)\n",
    "            else:\n",
    "                assert best_row.shape[0] == 1\n",
    "            print(\"{} rows matched at a depth delta of {} row = {} \\n\".format(depth_matched.shape[0], \n",
    "                                                                              best_row.depthdiff.min(), \n",
    "                                                                              best_row.index[0]))\n",
    "            mbio_to_wdr[sd_x] = best_row.index[0]\n",
    "            matched_yet = True\n",
    "        else:\n",
    "            print(\"No match found for {}\\n\".format(sd_x))\n",
    "            break\n",
    "\n",
    "# create row to row map\n",
    "mbioxTowqx = {}\n",
    "for sd_x, wq_ix in mbio_to_wdr.items():\n",
    "    tran_sub = super_transect[super_transect.SpaceTime == sd_x]\n",
    "    for mbiox in tran_sub.index:\n",
    "        mbioxTowqx[mbiox] = wq_ix\n",
    "        \n",
    "matched_wq = pd.DataFrame(index=mbioxTowqx.keys(), columns=wq_melted_df.columns)\n",
    "for mbiox_ in matched_wq.index:\n",
    "    matched_wq.loc[mbiox_, wq_melted_df.columns] = wq_melted_df.loc[mbioxTowqx[mbiox_], wq_melted_df.columns]\n",
    "\n",
    "print(matched_wq.loc[:, data_cols].isnull().sum())\n",
    "print(matched_wq.shape)\n",
    "print(super_transect.shape)\n",
    "\n",
    "null_fracts = (matched_wq.loc[:, data_cols].isnull().sum() / matched_wq.shape[0]).sort_values(ascending=False)\n",
    "obj_fxn, obj_fxn2 = [], []\n",
    "for col_idx in range(1,null_fracts.shape[0]-2):\n",
    "    qq_test = matched_wq.copy().loc[:, data_cols]\n",
    "    qq_test.drop(null_fracts.index[:col_idx], axis=1, inplace=True)\n",
    "    qq_test.dropna(axis=0, how='any', inplace=True)\n",
    "    obj_fxn.append(qq_test.shape[0]*qq_test.shape[1])\n",
    "    obj_fxn2.append(qq_test.shape[0])\n",
    "\n",
    "best_set = list(null_fracts.index[obj_fxn.index(max(obj_fxn))+1:])\n",
    "best_set2 = list(null_fracts.index[obj_fxn2.index(max(obj_fxn2))+1:])\n",
    "\n",
    "matched_wq['DateMMDDYY2'] = matched_wq.SampleDate_SampleTime.apply(lambda x: date_str(pd.to_datetime(x)))\n",
    "matched_wq['DepthName2'] = matched_wq.Depth.astype(int).apply(lambda x: \"{:02d}\".format(x))\n",
    "cols_to_rename = {}\n",
    "for a_col in matched_wq.columns:\n",
    "    if a_col in ['Latitude', 'Longitude', 'date_float', 'Time']:\n",
    "        cols_to_rename[a_col] = \"CBF_\"+a_col\n",
    "    else:\n",
    "        cols_to_rename[a_col] = a_col\n",
    "\n",
    "matched_wq.rename(cols_to_rename, axis=1, inplace=True)\n",
    "matched_wq = matched_wq.join(pd.DataFrame(index=matched_wq.index, columns=['PycDepth', 'BVF', 'GradFoldChange']))\n",
    "wq_to_join = matched_wq.drop(['SampleDate_SampleTime', 'Depth'], axis=1)\n",
    "super_matched4 = super_matched.join(wq_to_join)\n",
    "super_df_plus_garbage4 = super_df_plus_garbage3.join(wq_to_join)\n",
    "\n",
    "# maybe make date distance threshold harsher\n",
    "# get precipitation and windspeed data to confirm\n",
    "# should make options for upper/lower water column w/o stratification\n",
    "wqm_df.loc[wqm_df.FoldChange < 3, 'PycDepth'] = np.nan\n",
    "\n",
    "def add_strat_characteristics(smdf, wqdf):\n",
    "    for wq_ix in wqdf.index:\n",
    "        dmm, dflt, sta_ = wq_ix\n",
    "        row_selector = (smdf.StationName == sta_) & (smdf.DateMMDDYY == dmm)\n",
    "        smdf.loc[row_selector, 'PycDepth'] = wqdf.loc[wq_ix, 'PycDepth'].values[0]\n",
    "        smdf.loc[row_selector, 'BVF'] = wqdf.loc[wq_ix, 'BVF'].values[0]\n",
    "        smdf.loc[row_selector, 'GradFoldChange'] = wqdf.loc[wq_ix, 'FoldChange'].values[0]\n",
    "\n",
    "    better_coords2 = smdf.CBF_Latitude.notnull() & smdf.CBF_Longitude.notnull()\n",
    "    smdf.loc[better_coords2, 'Latitude'] = smdf.loc[better_coords2, 'CBF_Latitude']\n",
    "    smdf.loc[better_coords2, 'Longitude'] = smdf.loc[better_coords2, 'CBF_Longitude']\n",
    "    smdf.loc[better_coords2, 'Time'] = smdf.loc[better_coords2, 'CBF_Time']\n",
    "    smdf.loc[better_coords2, 'DateMMDDYY'] = smdf.loc[better_coords2, 'DateMMDDYY2']\n",
    "    smdf.loc[better_coords2, 'DepthName'] = smdf.loc[better_coords2, 'DepthName2']\n",
    "    to_drop_here = ['CBF_Latitude', 'CBF_Longitude', 'CBF_Time', 'date_float', \n",
    "                    'CBF_date_float', 'DepthName2', 'DateMMDDYY2']\n",
    "    return smdf.copy().drop(to_drop_here, axis=1)\n",
    "\n",
    "super_matched3 = add_strat_characteristics(super_matched4, wqm_df.copy())\n",
    "super_df_plus_garbage4['date_float'] = super_df_plus_garbage4.DateMMDDYY.apply(date2)\n",
    "super_df_plus_garbage5 = add_strat_characteristics(super_df_plus_garbage4, wqm_df.copy()) \n",
    "\n",
    "super_matched3.loc[:, ['CHLA', 'DepthName', 'DateMMDDYY', 'StationName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_set2)\n",
    "print(best_set)\n",
    "desired_columns = ['StationName', 'DateMMDDYY', 'DepthName', \"sequencing ID\", 'CollectionAgency', 'sequencing_ID', 'Month', 'Year', 'Month_Year', 'Salinity_Group', 'Time', 'Actual Depth (m)', 'Temp (°C)', 'Conductivity (μS/cm)', 'Specific Conductivity (μS/cm)', 'Total Dissolved Solids (mg/L)', 'Salinity (PSU)', 'ODO (% sat)', 'ODO (mg/L)', 'Pressure (psi)', 'pH', 'ORP (mV)', 'Turbidity (FNU)', 'Station', 'CHLA', 'DIN', 'DO', 'DOC', 'DON', 'DOP', 'FSS', 'KD', 'NH4F', 'NO23F', 'NO2F', 'NO3F', 'PC', 'PH', 'PHEO', 'PN', 'PO4F', 'PP', 'SALINITY', 'SECCHI', 'SIF', 'SIGMA_T', 'SPCOND', 'TDN', 'TDP', 'TN', 'TON', 'TP', 'TSS', 'VSS', 'WTEMP', 'TotalDepth']\n",
    "\n",
    "super_df_plus_garbage5[desired_columns].to_csv(\"../otu_data/environmental_and_mapping_data.txt\", sep=\"\\t\", \n",
    "                                               index_label='SampleID')\n",
    "\n",
    "otu_cols_ = [i for i in super_df_plus_garbage5.columns if i.startswith('OTU')]\n",
    "super_df_plus_garbage5[otu_cols_].to_csv(\"../otu_data/abundances_full.txt\", sep=\"\\t\", \n",
    "                                         index_label='SampleID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out a table of dates, depths, and stations for collaborators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_matched3.loc[\"SB072516TAWCSCB33CD22BR1TR1I123\", \"Time\"] = \"10:00:00\"\n",
    "super_matched3.loc[\"SB060117TAWCSCB33CD19BR2TR1I553\", \"Time\"] = \"13:00:00\"\n",
    "super_matched3.loc[\"SB072516TAWCSCB33CD20BR1TR1I121\", \"Time\"] = \"10:15:00\"\n",
    "super_matched3.loc[\"SB072516TAWCSCB33CD20BR2TR1I120\", \"Time\"] = \"10:25:00\"\n",
    "super_matched3.loc[\"SB061815TAWCSCB33CD0BR1TR1I3\", \"Time\"] = \"09:00:00\"\n",
    "super_matched3.loc[\"SB072516TAWCSCB33CD18BR1TR1I118\", \"Time\"] = \"10:20:00\"\n",
    "super_matched3.loc[\"SB072516TAWCSCB33CD16BR1TR1I115\", \"Time\"] = \"10:35:00\"\n",
    "super_matched3['Time'] = super_matched3['Time'].apply(str)\n",
    "\n",
    "# time of day, time of year, and linear time since sampling began predictors\n",
    "super_matched3['pd_date'] = pd.to_datetime(super_matched3.DateMMDDYY, format='%m%d%y')\n",
    "super_matched3['julian_day'] = (super_matched3['pd_date'] - pd.to_datetime('122014', format='%m%d%y')).dt.days\n",
    "super_matched3['day_length'] = super_matched3['julian_day'].apply(lambda d: -1*np.cos(2*np.pi*((d)/365.)))\n",
    "super_matched3['anti_day_length'] = super_matched3['julian_day'].apply(lambda d: -1*np.sin(2*np.pi*((d)/365.)))\n",
    "to_seconds = lambda x: (pd.to_datetime(x) - pd.to_datetime('00:00:00')).seconds\n",
    "super_matched3['julian_seconds'] = super_matched3['Time'].apply(to_seconds)\n",
    "\n",
    "# these steps convert it to hours, which shows that the earths orbit is not a circle :p\n",
    "#actual_day_len = lambda x: 12.166666666+(x*2.7666666666666666)\n",
    "#super_matched3['day_length_hr'] = super_matched3['day_length'].apply(actual_day_len)\n",
    "#super_matched3['anti_day_length_hr'] = super_matched3['anti_day_length'].apply(actual_day_len)\n",
    "\n",
    "some_columns = ['DepthName', 'DateMMDDYY', 'Time', 'StationName', 'CollectionAgency', 'Latitude', 'Longitude']\n",
    "to_remove = super_matched3.DepthName.isin(['LAB', 'lab'])\n",
    "subdf = super_matched3.loc[~to_remove, some_columns].drop_duplicates()\n",
    "subdf.to_csv(\"../otu_data/Preheim_CB_DepthStationYear_fixed.tab\", sep=\"\\t\")\n",
    "\n",
    "super_matched3.loc[super_matched3.Time.isnull(), some_columns+['pd_date', 'julian_day', 'day_length', 'julian_seconds']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at results of density gradient calculations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_meta_cols = meta_data_df.columns\n",
    "\n",
    "# This is station, depth, geoposition, collection agency, and time \n",
    "best_col_set = ['StationName', 'DateMMDDYY', 'DepthName', 'Latitude', 'Longitude', 'RawCount', \n",
    "                'TrimCount', 'RawCount_b', 'TrimCount_b', 'CollectionAgency', 'sequencing_ID', \n",
    "                'Month', 'Year', 'Month_Year', 'Salinity_Group', 'Time']\n",
    "\n",
    "metas_ls = {'encoded':{}, 'raw':{}, 'encoding':{}, 'rev_coding':{}}\n",
    "for sm in best_col_set:\n",
    "    metas_ls['raw'][sm] = super_matched3[sm].tolist()\n",
    "    if (sm == 'DateMMDDYY') or (sm == 'Time'):\n",
    "        presorted = sorted([(i, pd.to_datetime(i)) for i in set(metas_ls['raw'][sm])], key=lambda x: x[1])\n",
    "        metas_ls['encoding'][sm] = {raw:code for code, (raw, _) in enumerate(presorted)}\n",
    "    else:\n",
    "        metas_ls['encoding'][sm] = {raw:code for code, raw in enumerate(sorted(set(metas_ls['raw'][sm])))}\n",
    "    metas_ls['encoded'][sm] = [metas_ls['encoding'][sm][r] for r in metas_ls['raw'][sm]]\n",
    "    metas_ls['rev_coding'][sm] = {code:raw for raw, code in metas_ls['encoding'][sm].items()}\n",
    "\n",
    "wqm_df_transect = super_matched3.copy().reset_index()\n",
    "wqm_df_transect['GradFoldChange'] = wqm_df_transect['GradFoldChange'].astype(float)\n",
    "wqm_df_transect['StatEncoded'] = wqm_df_transect.loc[:, 'StationName'].map(metas_ls['encoding']['StationName'])\n",
    "wqm_df_transect['DatEncoded'] = wqm_df_transect.loc[:, 'DateMMDDYY'].map(metas_ls['encoding']['DateMMDDYY'])\n",
    "\n",
    "wqm_pivot = pd.pivot_table(wqm_df_transect, columns=['StationName'], values=['GradFoldChange'], \n",
    "                           index=['DatEncoded'], fill_value=-1.)\n",
    "print(wqm_pivot.shape)\n",
    "wqm_pivot.sort_index(inplace=True)\n",
    "wqm_pivot.rename(mapper=metas_ls['rev_coding']['DateMMDDYY'], axis=0, inplace=True)\n",
    "wqm_pivot.columns = [i[1] for i in wqm_pivot.columns]\n",
    "sns.set(rc={'xtick.labelsize': 10, 'ytick.labelsize': 6})\n",
    "mask = np.zeros_like(wqm_pivot.values)\n",
    "mask[wqm_pivot.values == -1.] = True\n",
    "fc_fig, fc_ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6), dpi=120)\n",
    "ax_fc = sns.heatmap(wqm_pivot, mask=mask, vmin=2, vmax=30, ax=fc_ax, cmap=\"YlGnBu\")\n",
    "ax_fc.set_title('Fold Change in Brunt–Väisälä Frequency at Pycnocline', fontsize=12)\n",
    "ax_fc.set(xlabel='Dates', ylabel='Stations')\n",
    "plt.show()\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add discharge to environmental data matrix\n",
    "\n",
    "We will also plot the raw discharge data to get a sense of the quantative contribution per river. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.close('all')\n",
    "expected_columns = {\"Discharge\": [\"146731_00060\", \"122065_00060\", \"69928_00060\", \"147046_00060\", \"69512_00060\"]}\n",
    "code2col = {val_i:key for key, val in expected_columns.items() for val_i in val}\n",
    "\n",
    "# load the files\n",
    "discharge_dir = \"../otu_data/WaterQualityData/USGS_Discharge\"\n",
    "discharge_files = [f for f in os.listdir(discharge_dir) if f.endswith(\".txt\")]\n",
    "dischargeNamePath = [(f.split(\"_\")[0], os.path.join(discharge_dir, f)) for f in discharge_files]\n",
    "\n",
    "# read in rappahanok\n",
    "df_list = []\n",
    "for riv_name, riv_file in dischargeNamePath:\n",
    "    print(riv_name)\n",
    "    df = pd.read_csv(riv_file, sep=\"\\t\", comment='#', header=0, low_memory=False).drop(0, axis=0)\n",
    "    # get data quality code columns\n",
    "    code_cols = [c for c in df.columns if c.endswith(\"_cd\") and not c.startswith(\"agency\") and not c.startswith(\"tz\")]\n",
    "    # remove any data not approved for publication\n",
    "    df2 = df[((df.loc[:, code_cols] != 'A') & (df.loc[:, code_cols].notnull())).sum(1) == 0]\n",
    "    df2.drop(code_cols, axis=1, inplace=True)\n",
    "    df2.rename(mapper=code2col, axis=1, inplace=True)\n",
    "\n",
    "    df2['Datetime'] = pd.to_datetime(df2['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df2.set_index('Datetime', inplace=True)\n",
    "    if df2.tz_cd.unique()[0] == 'EDT':\n",
    "        df2.index = df2.index.tz_localize('US/Eastern', ambiguous='NaT')\n",
    "        df2 = df2[df2.index.notnull()]\n",
    "        df2.index = df2.index.tz_convert('UTC').tz_localize(None)\n",
    "        df2.drop([\"agency_cd\", \"site_no\", \"datetime\", \"tz_cd\"], axis=1, inplace=True)\n",
    "        df2 = df2.astype(np.float64).loc[:, ['Discharge']]\n",
    "        df2 = df2.groupby(pd.Grouper(freq='D')).agg(np.nanmean).dropna(how='any', axis=0)\n",
    "        df2.columns= [i+\"_\"+riv_name for i in df2.columns]\n",
    "        df_list.append(df2.copy())\n",
    "    else:\n",
    "        raise ValueError(\"time zone not eastern\")\n",
    "    \n",
    "\n",
    "mega_df = pd.concat(df_list, axis=1).dropna(how='any', axis=0)\n",
    "mega_df['Discharge_Sum'] = mega_df.sum(axis=1)\n",
    "print(df_list[0].shape, df_list[1].shape, mega_df.shape[0])\n",
    "from sklearn.preprocessing import scale\n",
    "mega_scale = mega_df.div(mega_df.apply(np.median, axis=0), axis=1)\n",
    "\n",
    "datedf = pd.DataFrame(super_matched3.pd_date.unique(), columns=['date'])\n",
    "datedf['delta'] = pd.Series([14]*datedf.shape[0], index=datedf.index)\n",
    "datedf['before'] = datedf['date'] - pd.to_timedelta(datedf['delta'], unit='d')\n",
    "datedf['after'] = datedf['date'] + pd.to_timedelta(datedf['delta'], unit='d')\n",
    "for interval_ in [7, 14, 28]:\n",
    "    datedf[str(interval_)+'delta'] = pd.Series([interval_]*datedf.shape[0], \n",
    "                                               index=datedf.index)\n",
    "    datedf[str(interval_)] = datedf['date'] - pd.to_timedelta(datedf[str(interval_)+'delta'], unit='d')\n",
    "\n",
    "date_sets = {2017:set(),2015:set(),2016:set(),'all':set()}\n",
    "for d_ix in datedf.index:\n",
    "    left_side = datedf.loc[d_ix, 'before']\n",
    "    right_side = datedf.loc[d_ix, 'after']\n",
    "    this_year = right_side.year\n",
    "    print(this_year, left_side, right_side)\n",
    "    date_range_ = (mega_df.index >= left_side) & (mega_df.index <= right_side)\n",
    "    date_sets[this_year].update(mega_df[date_range_].index)\n",
    "    date_sets['all'].update(mega_df[date_range_].index)\n",
    "\n",
    "\n",
    "plt.clf(); plt.close('all')\n",
    "plt.rc('legend',fontsize=10)\n",
    "for yr in [2015, 2016, 2017]:\n",
    "    fig_d, ax_d = plt.subplots(nrows=1, ncols=1, figsize=(7.5,2.5), dpi=130, num=yr)\n",
    "    print(np.min(list(date_sets[yr])), np.max(list(date_sets[yr])))\n",
    "    temp2plot = mega_df.loc[date_sets[yr], :].copy().sort_index()\n",
    "    temp2plot2 = temp2plot.rename(index={xx:str(xx)[5:].split()[0] for xx in date_sets[yr]})\n",
    "    temp2plot3 = temp2plot2.rename(columns={xx:xx.replace(\"Discharge_\", \"\") for xx in temp2plot2.columns})\n",
    "    temp2plot3.plot(ax=ax_d)\n",
    "    ax_d.set_ylim((-5, 300000))\n",
    "    if yr != 2015:\n",
    "        ax_d.get_legend().remove()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    fig_d.savefig(\"../otu_data/WaterQualityData/figs/Mean_Daily_Discharge_{}.png\".format(yr), dpi=130)\n",
    "\n",
    "plt.show()\n",
    "mega_df_select = mega_df.loc[:, ['Discharge_James', 'Discharge_Susquehanna',  \n",
    "                                 'Discharge_Potomac', 'Discharge_Sum']]\n",
    "\n",
    "print((mega_df['Discharge_Susquehanna'] / mega_df['Discharge_Sum']).mean())\n",
    "# one week, two week, one month\n",
    "new_cols = [\"_\".join([i,str(j)]) for i in mega_df_select.columns for j in [7,14,28]]\n",
    "for nc in new_cols:\n",
    "    super_matched3[nc] = pd.Series([0]*super_matched3.shape[0], index=super_matched3.index)\n",
    "\n",
    "for d_ix in datedf.index:\n",
    "    for interval_ in [7, 14, 28]:\n",
    "        # early offset this time point\n",
    "        left_side = datedf.loc[d_ix, str(interval_)]\n",
    "        # the date itself \n",
    "        right_side = datedf.loc[d_ix, 'date']\n",
    "        # the indexes in the discharge matrix\n",
    "        m_date_range_ = (mega_df_select.index >= left_side) & (mega_df_select.index < right_side)\n",
    "        m_idxs = mega_df_select[m_date_range_].index\n",
    "        # the rows sampled on this date\n",
    "        s_date_range_ = super_matched3.index[super_matched3['pd_date'] == right_side]\n",
    "        print(\"date\", right_side, \"discharge idxs\", len(m_idxs), \"sample idxs\", s_date_range_.shape[0])\n",
    "        for riv in mega_df_select.columns:\n",
    "            this_date_riv = mega_df_select.loc[m_idxs, riv].sum()\n",
    "            print(\"\\t {}: {:02f}\". format(riv, this_date_riv))\n",
    "            dest_col = \"_\".join([riv,str(interval_)])\n",
    "            super_matched3.loc[s_date_range_, dest_col] = this_date_riv \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import taxa table and print some random rows to take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa_file = \"taxa_table.tsv\"\n",
    "data_path = \"../otu_data/dada2_outputs\"\n",
    "tax_f = os.path.join(data_path, taxa_file)\n",
    "taxa_df = pd.read_csv(tax_f, sep=\"\\t\")\n",
    "OTU_Seqs = {taxa_df.loc[idx, taxa_df.columns[0]]:idx for idx in taxa_df.index}\n",
    "OTU_Names = {idx:\"OTU{}\".format(idx+1) for idx in taxa_df.index }\n",
    "OTU_name2seq = {OTU_Names[num]:seq for seq, num in OTU_Seqs.items()}\n",
    "taxa_df.loc[:, taxa_df.columns[0]] = taxa_df.loc[:, taxa_df.columns[0]].apply(lambda x: OTU_Names[OTU_Seqs[x]])\n",
    "taxa_df = taxa_df.set_index(taxa_df.columns[0])\n",
    "taxa_df.loc[['OTU2880', 'OTU14561', 'OTU38271', \"OTU54666\", 'OTU15257', 'OTU30263', 'OTU2'], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where we rarefy to 3001 total counts and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio.stats import subsample_counts\n",
    "from skbio.diversity import beta_diversity\n",
    "import seaborn as sns\n",
    "from skbio.stats.distance import mantel\n",
    "\n",
    "def rarefy_table(abund_table, rare_level):\n",
    "    rare_abund = abund_table.copy() * 0.\n",
    "    total_abunds = abund_table.sum(1)\n",
    "    for samp in abund_table.index:\n",
    "        norm_factor = 1.0\n",
    "        if total_abunds[samp] < rare_level:\n",
    "            norm_factor = rare_level / total_abunds[samp]\n",
    "\n",
    "        select_vect = np.ceil(abund_table.loc[samp, :].values*norm_factor).astype(np.int64)\n",
    "        rare_vect = subsample_counts(select_vect, int(rare_level))\n",
    "        rare_abund.loc[samp, :] = rare_vect\n",
    "    return rare_abund\n",
    "\n",
    "disttabs = {}\n",
    "abundance_tables = {'Abundance Thresholded':abund_df_og_s1.copy()}\n",
    "for name_ab, ab_df in abundance_tables.items():\n",
    "    rare_pp = rarefy_table(ab_df, 3001)\n",
    "    sample_sums = ab_df.sum(1)\n",
    "    big_samples = ab_df[sample_sums > 300000].index\n",
    "    smaller_samples = ab_df[sample_sums < 50000].index\n",
    "    print(name_ab)\n",
    "    print(\"Big samples: {}, Small: {}\".format(len(big_samples), len(smaller_samples)))\n",
    "    plt.clf(); plt.close();\n",
    "    fixx, axx = plt.subplots(nrows=1, ncols=1, figsize=(6,6), dpi=130)\n",
    "    cols = ['gold', 'teal']\n",
    "    labs = ['full', 'rare']\n",
    "    for ixx, a_table in enumerate([ab_df, rare_pp]):\n",
    "        bc_dists = beta_diversity(\"braycurtis\", a_table.values, a_table.index)\n",
    "        disttabs[name_ab + \" \" + labs[ixx]] = bc_dists\n",
    "        bc_df = pd.DataFrame(index=a_table.index, columns=a_table.index, data=bc_dists._data)\n",
    "        print(np.median(bc_df.loc[big_samples, big_samples]), \"BvB\")\n",
    "        print(np.median(bc_df.loc[smaller_samples, smaller_samples]), \"SvS\")\n",
    "        print(np.median(bc_df.loc[big_samples, smaller_samples]), \"BvS\")\n",
    "        sns.distplot(bc_df.values.flatten(), axlabel='bray-curtis distance',\n",
    "                     kde_kws={\"label\":labs[ixx], \"lw\": 3}, ax=axx)\n",
    "    fixx.savefig(\"../otu_data/trim_stats/rarefaction_effect.png\")\n",
    "    plt.show()\n",
    "\n",
    "all_mat_names = list(disttabs.keys())\n",
    "for ix in range(len(all_mat_names)):\n",
    "    for jx in range(ix+1, len(all_mat_names)):\n",
    "        print(all_mat_names[ix], all_mat_names[jx])\n",
    "        r_pp, p_value_pp, n_pp = mantel(disttabs[all_mat_names[ix]], disttabs[all_mat_names[jx]], method='pearson')\n",
    "        print(r_pp, p_value_pp, n_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This creates a plot to show how many OTUs are shared across sequencing runs from the main station (CB33C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlycb33 = super_df['StationName'] == 'CB33C'\n",
    "adf = rare_pp.loc[onlycb33, :]\n",
    "mdf = super_df.loc[onlycb33, ~super_df.columns.isin(abund_df_og_s1.columns)]\n",
    "runs_to_keep = list(super_df['sequencing_ID'].unique())\n",
    "if 'controls' in runs_to_keep:\n",
    "    runs_to_keep.remove('controls')\n",
    "print(adf.shape, mdf.shape, runs_to_keep)\n",
    "shared_otus = pd.DataFrame(index=runs_to_keep, columns=runs_to_keep)\n",
    "shared_group = set(adf.columns)\n",
    "\n",
    "for run_grp1 in shared_otus.columns:\n",
    "    rg1_bool = mdf.sequencing_ID == run_grp1\n",
    "    print(\"{} has {} libraries\".format(run_grp1, rg1_bool.sum()))\n",
    "    for run_grp2 in shared_otus.index:\n",
    "        rg2_bool = mdf.sequencing_ID == run_grp2\n",
    "        otus_in_1 = adf.loc[rg1_bool, :].sum() / rg1_bool.sum()\n",
    "        otus_in_2 = adf.loc[rg2_bool, :].sum() / rg2_bool.sum()\n",
    "        foldchange = otus_in_1 / otus_in_2\n",
    "        num_shared = ((foldchange < 1000) & (foldchange > 0.001)).sum()\n",
    "        shared_otus.loc[run_grp2, run_grp1] = num_shared\n",
    "        # optional shared across all groups\n",
    "        shared_here = foldchange.index[(foldchange < 1000) & (foldchange > 0.001)]\n",
    "        shared_group = shared_group.intersection(set(shared_here))\n",
    "\n",
    "print(\"{} otus shared across all runs\".format(len(shared_group)))\n",
    "\n",
    "\n",
    "plt.clf(); plt.close();\n",
    "f, _axes_ = plt.subplots(nrows=1, ncols=1, figsize=(6, 6), dpi=125)\n",
    "sns.heatmap(shared_otus, annot=True, annot_kws={'fontsize':12}, fmt=\"d\", linewidths=.5, ax=_axes_, cbar=False)\n",
    "plt.savefig(\"../otu_data/OTU_Overlap_CB33_byRun_abthrsh.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf(); plt.close();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Alpha and Beta-Diversity Distances to Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio import TreeNode\n",
    "from io import StringIO\n",
    "from skbio.diversity import alpha_diversity\n",
    "import skbio.stats.composition as ssc \n",
    "from deicode.preprocessing import rclr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "rare_fn = \"../otu_data/final_rarefied_table.tsv\"\n",
    "meta_fn = \"../otu_data/final_metadata.tsv\"\n",
    "if os.path.exists(rare_fn) and os.path.exists(meta_fn):\n",
    "    meta_df_x = pd.read_csv(meta_fn, sep=\"\\t\", index_col=0, converters={'DateMMDDYY': lambda x: str(x)})\n",
    "    rare_abund = pd.read_csv(rare_fn, sep=\"\\t\", index_col=0)\n",
    "    print(rare_abund.shape, meta_df_x.shape, rare_abund.head().sum(1))\n",
    "else:\n",
    "    tree_file = \"../otu_data/tree_data/not_full_tree/RAxML_rootedTree.root.query_high_abund.ref.tre\"\n",
    "    with open(tree_file, 'r') as tfh:\n",
    "        tree_str = tfh.read().strip()\n",
    "\n",
    "    tree_obj = TreeNode.read(StringIO(tree_str))\n",
    "\n",
    "    rare_pp_nz = rare_pp.loc[:, rare_pp.columns[rare_pp.sum() > 0]]\n",
    "    \n",
    "\n",
    "    print(\"Dropped {} empty columns\".format(rare_pp.shape[1] - rare_pp_nz.shape[1]))\n",
    "    print(\"Calculating Beta Diversity\")\n",
    "    wu_dm = beta_diversity(\"weighted_unifrac\", rare_pp_nz.values, \n",
    "                           list(rare_pp_nz.index), tree=tree_obj, \n",
    "                           otu_ids=list(rare_pp_nz.columns))._data\n",
    "    print(\"Finished unifrac\", wu_dm.shape)\n",
    "\n",
    "    bc_dm = beta_diversity(\"braycurtis\", rare_pp_nz.values, rare_pp_nz.index)._data\n",
    "    print(\"Finished bray curtis\", bc_dm.shape)\n",
    "\n",
    "    # clr transform\n",
    "    close_mat = ssc.closure(rare_pp_nz.values)\n",
    "    nz_mat = ssc.multiplicative_replacement(close_mat)\n",
    "    rclr_mat = rclr().fit_transform(nz_mat)\n",
    "    clr_dm = squareform(pdist(rclr_mat, 'euclidean'))\n",
    "    print(\"Finished clr+euclidean\", clr_dm.shape)\n",
    "\n",
    "    # put it all together \n",
    "    beta_mats = []\n",
    "    for suff, mat in zip(['_wu', '_bc', '_clr'], [wu_dm, bc_dm, clr_dm]):\n",
    "        new_cols = [i+suff for i in list(rare_pp_nz.index)]\n",
    "        dm_df = pd.DataFrame(mat, index=rare_pp_nz.index, columns=new_cols)\n",
    "        beta_mats.append(dm_df.copy())\n",
    "\n",
    "    super_beta = beta_mats[0].join(beta_mats[1]).join(beta_mats[2])\n",
    "    print(super_beta.shape)\n",
    "    \n",
    "    print(\"Calculating alpha diversity metrics and rarefaction table\")\n",
    "    alpha_cols = ['enspie', 'enspie_25', 'enspie_975', \n",
    "                  'observed_otus', 'observed_otus_25', 'observed_otus_975',\n",
    "                  'faith_pd', 'faith_pd_25', 'faith_pd_975']\n",
    "    \n",
    "    total_abunds = abund_df_og_s1.sum(1)\n",
    "    rare_level = 3001\n",
    "    bstrap_cnt = 100\n",
    "    enspies_bsts = pd.DataFrame(index=abund_df_og_s1.index, \n",
    "                                columns=range(bstrap_cnt)).astype(float)\n",
    "    obs_otuses_bsts = pd.DataFrame(index=abund_df_og_s1.index, \n",
    "                                   columns=range(bstrap_cnt)).astype(float)\n",
    "    faith_pd_bsts = pd.DataFrame(index=abund_df_og_s1.index, \n",
    "                                 columns=range(bstrap_cnt)).astype(float)\n",
    "    for r_ix in range(bstrap_cnt):\n",
    "        rare_table = rarefy_table(abund_df_og_s1.copy(), rare_level)\n",
    "        faith_pd_bsts.loc[:, r_ix] = alpha_diversity('faith_pd', rare_table.values, ids=list(rare_table.index), \n",
    "                                                     otu_ids=list(rare_table.columns), tree=tree_obj)\n",
    "        obs_otuses_bsts.loc[:, r_ix] = alpha_diversity('observed_otus', \n",
    "                                                       rare_table.values, \n",
    "                                                       list(rare_table.index))\n",
    "        enspies_bsts.loc[:, r_ix] = alpha_diversity('enspie', rare_table.values, list(rare_table.index))\n",
    "        print(\"Alpha calcs completion percent {:.2%}\".format((r_ix+1)/bstrap_cnt), sep=' ', end=\"\\r\", flush=True)\n",
    "    \n",
    "    alpha_metrics = pd.DataFrame(index=abund_df_og_s1.index, columns=alpha_cols).astype(float)\n",
    "    alpha_iterator = zip(['enspie', \"observed_otus\", 'faith_pd'], [enspies_bsts, obs_otuses_bsts, faith_pd_bsts])\n",
    "    pct_fxn = lambda x: np.percentile(x, [2.5, 50, 97.5])\n",
    "    for met_name, met_df in alpha_iterator:\n",
    "        met_srs = met_df.apply(pct_fxn, axis=1)\n",
    "        alpha_metrics.loc[met_srs.index, met_name] = met_srs.apply(lambda x: x[1])\n",
    "        alpha_metrics.loc[met_srs.index, met_name+'_25'] = met_srs.apply(lambda x: x[1] - x[0]) \n",
    "        alpha_metrics.loc[met_srs.index, met_name+'_975'] = met_srs.apply(lambda x: x[2] - x[1])\n",
    "    \n",
    "    m_data_cols_ = set(super_df.columns) - set(abund_df_og_s1.columns)\n",
    "    print(alpha_metrics.shape, super_beta.shape, )\n",
    "    meta_df_x = super_df.loc[:, m_data_cols_].join(alpha_metrics).join(super_beta)\n",
    "    rare_abund = rare_pp_nz.copy()\n",
    "    meta_df_x.to_csv(meta_fn, sep=\"\\t\")\n",
    "    rare_abund.to_csv(rare_fn, sep=\"\\t\")\n",
    "    print(\"Writing rarefied counts ({1}) and metadata ({0}) to file\".format(rare_abund.shape, meta_df_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rarefaction curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rarefaction_curve(abun, boots, nsamples):\n",
    "    idxs_to_choose = np.array(list(range(abun.shape[0])))\n",
    "    rarefaction_ = pd.DataFrame(index=range(1,boots+1), columns=range(1, nsamples+1))\n",
    "    for stepnum in rarefaction_.columns:\n",
    "        for btsrp in rarefaction_.index:\n",
    "            np.random.seed(stepnum*btsrp)\n",
    "            print(\"Step {} Strap {}  \".format(stepnum,  btsrp), sep=' ', end=\"\\r\", flush=True)\n",
    "            sample_set = abun.index[np.random.choice(idxs_to_choose, size=(stepnum,), replace=False)]\n",
    "            sub_table = abun.loc[sample_set, :]\n",
    "            detected_pops = sub_table.columns[sub_table.sum() > 0].shape[0]\n",
    "            rarefaction_.loc[btsrp, stepnum] = detected_pops\n",
    "    return rarefaction_\n",
    "\n",
    "def rarefaction_curve2(abun, boots=300, nsamples=-1):\n",
    "    abun_pa = abun > 0\n",
    "    if nsamples == -1:\n",
    "        nsamples = abun.shape[0]\n",
    "    idxs_to_choose = np.array(list(range(abun.shape[0])))\n",
    "    rarefaction_ = pd.DataFrame(index=range(1,boots+1), columns=range(1, nsamples+1))\n",
    "    # rows are the numbers for each round\n",
    "    for btsrp in rarefaction_.index:\n",
    "        np.random.seed(btsrp)\n",
    "        samp_order = abun.index[np.random.choice(idxs_to_choose, size=(nsamples,), replace=False)]\n",
    "        accumulated_otus = set()\n",
    "        # columns are the OTUS accumulated per round\n",
    "        for stepnum in rarefaction_.columns:\n",
    "            accumulated_otus.update(abun_pa.columns[abun_pa.loc[samp_order[stepnum-1], :]])\n",
    "            rarefaction_.loc[btsrp, stepnum] = len(accumulated_otus)\n",
    "            print(\"Step {} Strap {}  \".format(stepnum,  btsrp), sep=' ', end=\"\\r\", flush=True)\n",
    "    return rarefaction_\n",
    "\n",
    "def plot_rarefaction_quantiles(fig_file_name, rarefaction_, data_label):\n",
    "    rare_qs = np.quantile(rarefaction_.values, [0.025, 0.5, 0.975], axis=0, keepdims=True)\n",
    "    rare_qs = rare_qs.reshape((3, rarefaction_.shape[1])).T\n",
    "    plt.close(); plt.clf();\n",
    "    figrar, axrar = plt.subplots(nrows=1, ncols=1, figsize=(8,6), dpi=120)\n",
    "    x_intervals = rarefaction_.columns.astype(int)\n",
    "    axrar.plot(x_intervals, rare_qs[:, 1], lw = 1, color = '#539caf', alpha = 1, label = data_label)\n",
    "    axrar.fill_between(x_intervals, rare_qs[:, 0], rare_qs[:, 2], \n",
    "                       color = '#539caf', alpha = 0.4, label = '95% CI')\n",
    "    axrar.set_title(\"Rarefaction Curve\")\n",
    "    axrar.set_xlabel(\"Samples Drawn\"); axrar.set_ylabel(\"Novel OTUS observed\")\n",
    "    axrar.tick_params(axis='both', labelsize=8); axrar.legend(loc = 'best');\n",
    "    plt.show()\n",
    "    figrar.savefig(fig_file_name, dpi=120)\n",
    "    return \n",
    "\n",
    "def plot_rarefaction_quantiles2(fig_file_name, rarefactions_, data_labels):\n",
    "    colors = ['#E69F00', '#0072B2']\n",
    "    plt.close(); plt.clf();\n",
    "    figrar, axrar = plt.subplots(nrows=1, ncols=1, figsize=(8,6), dpi=120)\n",
    "    for rix, rarefaction_ in rarefactions_:\n",
    "        rare_qs = np.quantile(rarefaction_.values, [0.025, 0.5, 0.975], axis=0, keepdims=True)\n",
    "        rare_qs = rare_qs.reshape((3, rarefaction_.shape[1])).T\n",
    "        x_intervals = rarefaction_.columns.astype(int)\n",
    "        axrar.plot(x_intervals, rare_qs[:, 1], lw = 1, color = colors[rix],\n",
    "                   alpha = 1, label = data_labels[rix])\n",
    "        axrar.fill_between(x_intervals, rare_qs[:, 0], rare_qs[:, 2], color = colors[rix],\n",
    "                           alpha = 0.4, label = data_labels[rix]+' 95% CI')\n",
    "    axrar.set_title(\"Rarefaction Curve\")\n",
    "    axrar.set_xlabel(\"Samples Drawn\"); axrar.set_ylabel(\"Novel OTUS observed\")\n",
    "    axrar.tick_params(axis='both', labelsize=8); axrar.legend(loc = 'best');\n",
    "    plt.show()\n",
    "    figrar.savefig(fig_file_name, dpi=120)\n",
    "    return\n",
    "\n",
    "\n",
    "abund_df_tr_nz = abund_df_tr.loc[:, abund_df_tr.columns[abund_df_tr.sum() > 0]]\n",
    "abund_df_tr_nz.to_csv(\"../otu_data/clean_abundance.txt\", sep=\"\\t\", index_label='Sample')\n",
    "print(abund_df_tr_nz.shape, abund_df_tr.shape)\n",
    "if os.path.exists(\"../otu_data/trim_stats/rarefaction_curve_data_full.tsv\"):\n",
    "    rarefaction_pts = pd.read_csv(\"../otu_data/trim_stats/rarefaction_curve_data_full.tsv\", sep=\"\\t\", index_col=0)\n",
    "    fig_name_f = \"../otu_data/trim_stats/rarefaction_curve_figure.png\"\n",
    "    plot_rarefaction_quantiles(fig_name_f, rarefaction_pts.iloc[:, :301], 'full data set')\n",
    "    \n",
    "    if os.path.exists(\"../otu_data/trim_stats/rarefaction_curve_data_rare.tsv\"):\n",
    "        rarefaction_df2 = pd.read_csv(\"../otu_data/trim_stats/rarefaction_curve_data_rare.tsv\", sep=\"\\t\", index_col=0)\n",
    "        fig_name_f2 = \"../otu_data/trim_stats/rarefaction_curve_on_rarified_data.png\"\n",
    "        plot_rarefaction_quantiles(fig_name_f2, rarefaction_df2, 'rarified data set')\n",
    "else:\n",
    "    rarefaction_pts = rarefaction_curve(abund_df_tr_nz, 100, 349)\n",
    "    rarefaction_pts.to_csv(\"../otu_data/trim_stats/rarefaction_curve_data_full.tsv\", sep=\"\\t\")\n",
    "    rarefaction_df2 = rarefaction_curve(rare_abund, 100, 300)\n",
    "    rarefaction_df2.to_csv(\"../otu_data/trim_stats/rarefaction_curve_data_rare.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "# this is to see where the curve slows down \n",
    "#first_diff = rare_quants[1:, 1] - rare_quants[:-1, 1]\n",
    "#decelration = first_diff[first_diff <= 0]\n",
    "# this is to see how OTUS are added each year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifteen_idxs = [i for i in abund_df_tr_nz.index if \"15TAWCSCB\" in i]\n",
    "sixteen_idxs = [i for i in abund_df_tr_nz.index if \"16TAWCSCB\" in i]\n",
    "seventeen_idxs = [i for i in abund_df_tr_nz.index if \"17TAWCSCB\" in i]\n",
    "print(len(fifteen_idxs), len(sixteen_idxs), len(seventeen_idxs))\n",
    "firstyear = set(abund_df_tr.columns[abund_df_tr.loc[fifteen_idxs, :].sum() > 0])\n",
    "secondyear = set(abund_df_tr.columns[abund_df_tr.loc[sixteen_idxs, :].sum() > 0])\n",
    "thirdyear = set(abund_df_tr.columns[abund_df_tr.loc[seventeen_idxs, :].sum() > 0])\n",
    "allyears = thirdyear.union(firstyear).union(secondyear)\n",
    "\n",
    "print(len(firstyear))\n",
    "print(len((firstyear) - (secondyear | thirdyear)))\n",
    "print(len(firstyear) /len(allyears))\n",
    "\n",
    "print(len(secondyear))\n",
    "print(len(secondyear - (firstyear | thirdyear)))\n",
    "print(len(secondyear - firstyear))\n",
    "print(len(secondyear - firstyear)/len(allyears))\n",
    "\n",
    "print(len(thirdyear))\n",
    "print(len(thirdyear - (firstyear | secondyear)))\n",
    "print(len(thirdyear - (firstyear | secondyear))/len(allyears))\n",
    "\n",
    "\n",
    "print(len(firstyear & secondyear))\n",
    "print(len(firstyear & thirdyear))\n",
    "print(len(secondyear & thirdyear))\n",
    "print(len(firstyear & secondyear & thirdyear))\n",
    "print(len(allyears))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rarefaction_quantiles2(fig_file_names, rarefactions_, data_labels):\n",
    "    colors = ['#E69F00', '#0072B2']\n",
    "    sizes_ = [(6,6), (3, 3)]\n",
    "    plt.close(); plt.clf();\n",
    "    for rix, rarefaction_ in enumerate(rarefactions_):\n",
    "        figrar, axrar = plt.subplots(nrows=1, ncols=1, figsize=sizes_[rix], dpi=120)\n",
    "        rare_qs = np.quantile(rarefaction_.values, [0.025, 0.5, 0.975], axis=0, keepdims=True)\n",
    "        rare_qs = rare_qs.reshape((3, rarefaction_.shape[1])).T\n",
    "        x_intervals = rarefaction_.columns.astype(int)\n",
    "        axrar.plot(x_intervals, rare_qs[:, 1], lw = 1, color = colors[rix],\n",
    "                   alpha = 1, label = data_labels[rix])\n",
    "        axrar.fill_between(x_intervals, rare_qs[:, 0], rare_qs[:, 2], color = colors[rix],\n",
    "                           alpha = 0.4, label = data_labels[rix]+' 95% CI')\n",
    "        if rix == 0:\n",
    "            axrar.set_xlabel(\"Samples Drawn\", fontsize=12); \n",
    "            axrar.set_ylabel(\"Novel OTUS observed\", fontsize=12)\n",
    "            axrar.legend(loc = 4);\n",
    "        else:\n",
    "            axrar.set_title(\"On rarefied table\", fontsize=12)\n",
    "        \n",
    "        axrar.tick_params(axis='both', labelsize=12); \n",
    "        figrar.tight_layout()\n",
    "        plt.show()\n",
    "        figrar.savefig(fig_file_names[rix], dpi=120)\n",
    "    return\n",
    "\n",
    "#if not os.path.exists(\"../otu_data/trim_stats/rarefaction_curve2_both.tsv\"):\n",
    "#rarefaction_fl = rarefaction_curve2(abund_df_tr_nz)\n",
    "#rarefaction_rr = rarefaction_curve2(rare_abund)\n",
    "#plot_rarefaction_quantiles2([\"../otu_data/trim_stats/rarefaction_curve2_full.png\",\n",
    "#                             \"../otu_data/trim_stats/rarefaction_curve2_rarefied.png\"], \n",
    "#                             [rarefaction_fl, rarefaction_rr], ['Full', 'Rarefied'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rare_abund.shape, meta_df_x.shape, abund_df_og_s1.shape, super_matched3.shape)\n",
    "for _df_ in [rare_abund, meta_df_x, abund_df_og_s1, super_matched3]:\n",
    "    if '95_FiltCtrl_Surface_R1' in list(_df_.index):\n",
    "        _df_.drop('95_FiltCtrl_Surface_R1', axis=0, inplace=True)\n",
    "\n",
    "print('95_FiltCtrl_Surface_R1' in list(rare_abund.index))\n",
    "print('95_FiltCtrl_Surface_R1' in list(meta_df_x.index))\n",
    "print('95_FiltCtrl_Surface_R1' in list(abund_df_og_s1.index))\n",
    "print('95_FiltCtrl_Surface_R1' in list(super_matched3.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abund_df_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add replicate column\n",
    "super_matched3['replicate'] = pd.Series(pd.Categorical([1]*super_matched3.shape[0], categories=[1,2,3,4]), \n",
    "                                     index=super_matched3.index)\n",
    "\n",
    "st_cols = ['DateMMDDYY', 'DepthName', 'StationName']\n",
    "super_matched3.loc[:, 'SpaceTime'] = super_matched3.loc[:, st_cols].apply(tuple, axis=1)\n",
    "    \n",
    "unq_tds = super_matched3.SpaceTime.unique()\n",
    "print(\"{} potential replicates\".format(super_matched3.shape[0] - unq_tds.shape[0]))\n",
    "\n",
    "dist_col_pairs = {\"wu\":[], \"bc\":[], 'clr':[]}\n",
    "for (t_, d_, s_) in super_matched3.SpaceTime.unique():\n",
    "    mxO = super_matched3[super_matched3['SpaceTime'] == (t_, d_, s_)].index\n",
    "    for num_r, mx_i in enumerate(mxO):\n",
    "        print(t_, d_, s_, num_r)\n",
    "        super_matched3.loc[mx_i, 'replicate'] = num_r + 1\n",
    "    if len(mxO) != 1:\n",
    "        # sort out all the columns in distance matrix that contain the name of a replicate index\n",
    "        dist_cols = [i for i in meta_df_x.columns if i.split(\"_\")[0] in list(mxO)]\n",
    "        print(dist_cols, mxO)\n",
    "        assert len(dist_cols) == len(mxO)*3\n",
    "        for ix_ in mxO:\n",
    "            for pc in dist_cols:\n",
    "                if not ix_ in pc:\n",
    "                    if \"_bc\" in pc:\n",
    "                        dist_col_pairs['bc'].append((ix_, pc))\n",
    "                    elif \"_wu\" in pc:\n",
    "                        dist_col_pairs['wu'].append((ix_, pc))\n",
    "                    elif \"_clr\" in pc:\n",
    "                        dist_col_pairs['clr'].append((ix_, pc))\n",
    "\n",
    "bc_rep_vals = np.array([meta_df_x.loc[i, j] for i, j in dist_col_pairs['bc']], dtype=float)\n",
    "wu_rep_vals = np.array([meta_df_x.loc[i, j] for i, j in dist_col_pairs['wu']], dtype=float)\n",
    "clr_rep_vals = np.array([meta_df_x.loc[i, j] for i, j in dist_col_pairs['clr']], dtype=float)\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "rep_dists = np.hstack((minmax_scale(bc_rep_vals), \n",
    "                       minmax_scale(wu_rep_vals), \n",
    "                       minmax_scale(clr_rep_vals)))\n",
    "dist_row_names = ['bray-curtis']*bc_rep_vals.shape[0]\n",
    "dist_row_names += ['weighted-unifrac']*wu_rep_vals.shape[0]\n",
    "dist_row_names += ['clr+euclidean']*clr_rep_vals.shape[0]\n",
    "rep_names = np.array(dist_row_names)\n",
    "rep_dist_df = pd.DataFrame(np.vstack((rep_names, rep_dists)).T, columns=['metric', 'dist'])\n",
    "rep_dist_df.loc[:, 'metric'] = rep_dist_df.loc[:, 'metric'].astype('category')\n",
    "rep_dist_df.loc[:, 'dist'] = rep_dist_df.loc[:, 'dist'].astype('float')\n",
    "\n",
    "plt.clf(); plt.close();\n",
    "figrep, axrep = plt.subplots(nrows=1, ncols=1, figsize=(6,6), dpi=120)\n",
    "ax = sns.violinplot(y=\"dist\", x=\"metric\", data=rep_dist_df, ax=axrep)\n",
    "ax.yaxis.label.set_size(0)\n",
    "ax.xaxis.label.set_size(0)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "figrep.savefig(\"../otu_data/trim_stats/replicate_distances.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "super_matched3_derep = super_matched3[(super_matched3.replicate == 1) & (super_matched3.DepthName != 'LAB')]\n",
    "print(super_matched3_derep.shape, super_matched3.shape, rep_dist_df.shape)\n",
    "rep_dist_df.groupby('metric').agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(super_matched3_derep.shape)\n",
    "print(meta_df_x.shape)\n",
    "to_add_cols = set(meta_df_x.columns) - set(super_matched3_derep.columns)\n",
    "print(len(to_add_cols))\n",
    "abund_md_df_derep = super_matched3_derep.join(meta_df_x[to_add_cols])\n",
    "print(abund_md_df_derep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in super_matched3_derep.columns if not i.startswith(\"OTU\") and not i.startswith(\"SB\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kicking the tires on environmental data\n",
    "\n",
    "We want to see: \n",
    "If there are outliers in each data set seperately\n",
    "The concordance between shared variables in both data sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# these are the factors that are common to all sample sets\n",
    "# This is station, depth, geoposition, collection agency, and time \n",
    "super_matched3_derep['depth_float'] = super_matched3_derep['DepthName'].astype(float)\n",
    "super_matched3_derep['TotalDepth'] = super_matched3_derep['TotalDepth'].astype(float)\n",
    "\n",
    "# total_depth_percentage\n",
    "total_depth_srs = super_matched3_derep.loc[:, ['StationName', 'TotalDepth']].groupby('StationName').agg(np.nanmean)\n",
    "depth_mapper = lambda x: x[1]/total_depth_srs.loc[x[0]]\n",
    "super_matched3_derep['Depth_Percentage'] = super_matched3_derep.loc[:, ['StationName', 'depth_float']].apply(depth_mapper, axis=1)\n",
    "\n",
    "best_col_set = ['StationName', 'depth_float', 'Latitude', 'Longitude', 'RawCount', 'TrimCount', \n",
    "                'CollectionAgency', 'sequencing_ID', 'Month', 'Year', 'Month_Year',\n",
    "                'julian_day', 'day_length', 'anti_day_length', 'julian_seconds', 'Depth_Percentage']\n",
    "\n",
    "# these are most interesting discharge cols (also common to any sample)\n",
    "discharge_cols = ['Discharge_James_14', 'Discharge_Susquehanna_14']\n",
    "best_col_set += discharge_cols\n",
    "\n",
    "# these are the uncorrelated alpha diversity columns\n",
    "alpha_cols = ['enspie', 'faith_pd']\n",
    "assert set(alpha_cols).issubset(set(meta_df_x.columns))\n",
    "assert set(super_matched3_derep.index).issubset(meta_df_x.index)\n",
    "if not set(alpha_cols).issubset(set(super_matched3_derep.columns)):\n",
    "    super_matched3_derep = super_matched3_derep.join(meta_df_x.loc[super_matched3_derep.index, alpha_cols])\n",
    "best_col_set += alpha_cols\n",
    "\n",
    "assert super_matched3_derep.loc[:, best_col_set].isnull().sum().sum() == 0\n",
    "for bcs in best_col_set:\n",
    "    bcs_levels = super_matched3_derep[bcs].unique()\n",
    "    if len(bcs_levels) < 10:\n",
    "        print(\"{}: {}\".format(bcs, list(bcs_levels)))\n",
    "    else:\n",
    "        print(\"{}: {} levels (eg: {})\".format(bcs, len(bcs_levels), repr(list(bcs_levels)[0])))\n",
    "\n",
    "## these are the three data matrices\n",
    "# these are Preheim lab samples with data measured by our probe\n",
    "cb33_indexes = super_matched3_derep[super_matched3_derep.CollectionAgency == 'Preheim'].index\n",
    "print(\"Preheim indexes: {}\".format(len(cb33_indexes)))\n",
    "probe_cols = ['Temp (°C)','Salinity (PSU)', 'ODO (mg/L)', 'pH']\n",
    "probe_data = super_matched3_derep.loc[cb33_indexes, best_col_set+probe_cols].dropna(axis=0, how='any')\n",
    "probe_data_tp = super_matched3_derep.loc[cb33_indexes, ['pd_date', 'depth_float']+probe_cols].dropna(axis=0, how='any')\n",
    "assert probe_data_tp.shape[0] == probe_data_tp.shape[0]\n",
    "print(\"{} microbial samples matched with {} (measurements, columns)\".format(len(cb33_indexes), probe_data.shape))\n",
    "if not os.path.exists(\"../otu_data/WaterQualityData/figs/CB33probe_data_pairplot.png\"):\n",
    "    plt.close('all')\n",
    "    g = sns.pairplot(probe_data_tp, hue=\"pd_date\", palette=\"husl\")\n",
    "    for i, j in zip(*np.triu_indices_from(g.axes, 1)):\n",
    "        g.axes[i, j].set_visible(False)\n",
    "    g.axes[-1,0].set_xlim((-2, 25))\n",
    "    g.axes[-1,1].set_xlim((12, 32))\n",
    "    g.axes[-1,2].set_xlim((0, 22))\n",
    "    g.axes[-1,3].set_xlim((-2, 12))\n",
    "    g.savefig(\"../otu_data/WaterQualityData/figs/CB33probe_data_pairplot.png\", dpi=130)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# these are a more complete set similar predictors and the rows which have them \n",
    "cb_data_colset_2 = ['WTEMP', 'SALINITY', 'DO', 'PH']\n",
    "both_colsets = best_col_set+probe_cols+cb_data_colset_2\n",
    "cb_plus_transect = super_matched3_derep.loc[:, both_colsets]\n",
    "col_pairs = zip(probe_cols, cb_data_colset_2)\n",
    "# we are going to copy data from our columns into their corresponding columns (note the matched order for zip)\n",
    "for ourcol, theircol in col_pairs:\n",
    "    cb_plus_transect.loc[cb33_indexes, theircol] = cb_plus_transect.loc[cb33_indexes, ourcol]\n",
    "    cb_plus_transect.drop(ourcol, axis=1, inplace=True)\n",
    "\n",
    "cbPlusT_df = cb_plus_transect.dropna(axis=0, how='any')\n",
    "cbPlusT_df_toplot = cbPlusT_df.loc[:,['day_length', \"CollectionAgency\", 'depth_float']+cb_data_colset_2]\n",
    "print(cbPlusT_df_toplot.columns)\n",
    "if not os.path.exists(\"../otu_data/WaterQualityData/figs/all_probedata_pairs.png\"):\n",
    "    g2 = sns.pairplot(cbPlusT_df_toplot, hue=\"CollectionAgency\", palette=\"husl\")\n",
    "    for i, j in zip(*np.triu_indices_from(g2.axes, 1)):\n",
    "        g2.axes[i, j].set_visible(False)\n",
    "    g2.savefig(\"../otu_data/WaterQualityData/figs/all_probedata_pairs.png\", dpi=130)\n",
    "    plt.show()\n",
    "## \n",
    "\n",
    "# these are the environmental data cols\n",
    "high_corr = ['TDN', 'PO4F', 'TDP', 'TON', 'PN', \"PP\", 'DIN', 'SIGMA_T', 'PH',  \n",
    "             'SALINITY', 'DON']\n",
    "cb_data_colset_1 = ['TON', 'TP', 'TN', 'PN', 'PP', 'PC', 'TSS', 'NO2F', 'DON', 'DIN', 'NH4F',\n",
    "                    'NO23F', 'DOP', 'CHLA', 'NO3F', 'PHEO', 'PO4F', 'TDN', 'TDP', 'SALINITY', \n",
    "                    'SIGMA_T', 'SPCOND', 'DO', 'PH', 'WTEMP']\n",
    "\n",
    "big_colset = best_col_set+cb_data_colset_1\n",
    "jt_df = super_matched3_derep.loc[:, big_colset].dropna(axis=0, how='any')\n",
    "jt_df_toplot = jt_df.loc[:,['anti_day_length','day_length', 'Depth_Percentage', 'Latitude']+cb_data_colset_1+alpha_cols+discharge_cols].astype(float)\n",
    "\n",
    "plt.clf(); plt.close('all');\n",
    "corrplot_fn3 = \"../otu_data/WaterQualityData/figs/wq_corrplot_transect_pearson_summeronly_full.png\"\n",
    "if not os.path.exists(corrplot_fn3):\n",
    "    summeronly = jt_df_toplot[jt_df.Month.isin(['06', '07', '08'])]\n",
    "    corrplot3 = sns.clustermap(summeronly.corr('pearson'), metric='correlation', \n",
    "                              figsize=(18,18), cmap=\"coolwarm\", annot=True)\n",
    "    corrplot3.savefig(corrplot_fn3, dpi=100); plt.show()\n",
    "    \n",
    "corrplot_fn = \"../otu_data/WaterQualityData/figs/wq_corrplot_transect_pearson_full.png\"\n",
    "if not os.path.exists(corrplot_fn):\n",
    "    corrplot = sns.clustermap(jt_df_toplot.corr('pearson'), metric='correlation', \n",
    "                              figsize=(16,16), cmap=\"coolwarm\", annot=True)\n",
    "    corrplot.savefig(corrplot_fn, dpi=125); plt.show()\n",
    "\n",
    "    \n",
    "corrplot_fn4 = \"../otu_data/WaterQualityData/figs/wq_corrplot_transect_pearson_full_diff.png\"\n",
    "if not os.path.exists(corrplot_fn4):\n",
    "    corrplot = sns.clustermap(jt_df_toplot.corr('pearson') - summeronly.corr('pearson'), metric='correlation', \n",
    "                              figsize=(16,16), cmap=\"coolwarm\", annot=True)\n",
    "    corrplot.savefig(corrplot_fn4, dpi=125); plt.show()\n",
    "    \n",
    "    \n",
    "jt_df_toplot_r = jt_df_toplot.drop(high_corr, axis=1)\n",
    "jt_df_toplot_r = jt_df_toplot_r.rename(columns={i:i.replace(\"Discharge_\", \"Q_\").replace(\"uehanna\", \"\") for i in jt_df_toplot_r.columns})\n",
    "#jt_df.drop(high_corr, axis=1, inplace=True)\n",
    "sns.set(font_scale=1.4)\n",
    "corrplot_fn2 = \"../otu_data/WaterQualityData/figs/wq_corrplot_transect_pearson_reduced_1.png\"\n",
    "if not os.path.exists(corrplot_fn2):\n",
    "    corrplot2 = sns.clustermap(jt_df_toplot_r.corr('pearson'), metric='correlation', \n",
    "                              figsize=(12,12), cmap=\"coolwarm\", annot=False)\n",
    "    corrplot2.savefig(corrplot_fn2, dpi=125); plt.show();\n",
    "sns.set(font_scale=1.)\n",
    "\n",
    "print(probe_data.shape, set(probe_data.columns) - set(best_col_set))\n",
    "print(cbPlusT_df.shape, set(cbPlusT_df.columns) - set(best_col_set))\n",
    "print(jt_df.shape, set(jt_df.columns) - set(best_col_set))\n",
    "\n",
    "data_dir = \"../otu_data/WaterQualityData/matched_cleaned_data\"\n",
    "jt_df.to_csv(data_dir+\"/transect_mdata_colset_1.tsv\", sep=\"\\t\")\n",
    "cbPlusT_df.to_csv(data_dir+\"/all_mdata_colset_2.tsv\", sep=\"\\t\")\n",
    "probe_data.to_csv(data_dir+\"/cb33_mdata_probecols.tsv\", sep=\"\\t\")\n",
    "\n",
    "\"\"\"\n",
    "if not os.path.exists(\"../otu_data/WaterQualityData/figs/transect_envdata_pairs.png\"):\n",
    "    g3 = sns.pairplot(jt_df_toplot, hue=\"CollectionAgency\", palette=\"husl\")\n",
    "    for i, j in zip(*np.triu_indices_from(g3.axes, 1)):\n",
    "        g3.axes[i, j].set_visible(False)\n",
    "    g3.savefig(\"../otu_data/WaterQualityData/figs/transect_envdata_pairs.png\", dpi=130)\n",
    "    plt.show()\n",
    "# lets add the ODU and DNR water data from CB3.3C\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# export the distance matrix, community abundances and enviornmental data for CB3.3C (our lab only)\n",
    "assert set(probe_data.index).issubset(set(rare_abund.index))\n",
    "cb33_comm = rare_abund.loc[probe_data.index, :]\n",
    "cb33_comm = cb33_comm.loc[:, cb33_comm.columns[cb33_comm.sum() > 0]]\n",
    "print(cb33_comm.shape)\n",
    "unifrac_cols = [i+\"_wu\" for i in probe_data.index]\n",
    "assert set(unifrac_cols).issubset(set(meta_df_x.columns))\n",
    "cb33_distmat = meta_df_x.loc[probe_data.index, unifrac_cols]\n",
    "print(cb33_distmat.shape)\n",
    "\n",
    "\n",
    "cb33_md = cb33_all_df.loc[probe_data.index, :].join(alpha_33)\n",
    "print(cb33_md.shape)\n",
    "cb33_env_dir = \"../otu_data/WaterQualityData/cb33_envmodel\"\n",
    "cb33_md_fn = os.path.join(cb33_env_dir, 'cb33_mdata.tsv')\n",
    "cb33_md.to_csv(cb33_md_fn, sep=\"\\t\")\n",
    "\n",
    "cb33_distmat_fn = os.path.join(cb33_env_dir, 'cb33distmat.tsv')\n",
    "cb33_distmat.to_csv(cb33_distmat_fn, sep=\"\\t\")\n",
    "\n",
    "cb33_comm_fn = os.path.join(cb33_env_dir, 'cb33comm.tsv')\n",
    "cb33_comm.to_csv(cb33_comm_fn, sep=\"\\t\")\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jt_df_toplot_r.describe().iloc[1:3,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(jt_df.shape, cbPlusT_df.shape)\n",
    "\n",
    "print(cbPlusT_df.StationName.unique().shape)\n",
    "cbPlusT_df['Days since Jan 1st'] = cbPlusT_df.julian_day % 365.\n",
    "cbPlusT_df['Latitude_'] = cbPlusT_df.Latitude.astype(float).round(2)\n",
    "cbPlusT_df_down = cbPlusT_df[cbPlusT_df.Depth_Percentage > 0.8]\n",
    "cbPlusT_df_up = cbPlusT_df[cbPlusT_df.depth_float < 2.0 ]\n",
    "\n",
    "print(spearmanr(cbPlusT_df.loc[:, 'Days since Jan 1st'].values, cbPlusT_df.loc[:, 'WTEMP'].values), cbPlusT_df.shape )\n",
    "print(spearmanr(cbPlusT_df.loc[:, 'DO'].values, cbPlusT_df.loc[:, 'PH'].values), cbPlusT_df.shape )\n",
    "print(spearmanr(probe_data.loc[:, 'ODO (mg/L)'].values, probe_data.loc[:, 'pH'].values), probe_data.shape)\n",
    "print(((cbPlusT_df_down.DO < 2.0).sum() - 2) / cbPlusT_df_down.shape[0], cbPlusT_df_down.shape[0], (cbPlusT_df_down.DO < 2.0).sum() - 2)\n",
    "print(((cbPlusT_df_down['Days since Jan 1st'] < 5) | (cbPlusT_df_down['Days since Jan 1st'] > 9) ).sum())\n",
    "print((41/(138-41)))\n",
    "\n",
    "filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X','o', 'v', '^', '<', '>')\n",
    "plt.clf(); plt.close('all');\n",
    "time_series_rep1_f = \"../otu_data/WaterQualityData/figs/DO_WTEMP_PH_bySurface_Bottom.png\"\n",
    "scatfig, scatax = plt.subplots(3,2,figsize=(9,9), dpi=120)\n",
    "ax1 = sns.scatterplot(x='Days since Jan 1st', y=\"WTEMP\", hue=\"Latitude_\", style=\"Year\", \n",
    "                     data=cbPlusT_df_down, markers=filled_markers[1:], ax=scatax[0,0], legend=False)\n",
    "ax2 = sns.scatterplot(x='Days since Jan 1st', y=\"DO\", hue=\"Latitude_\", style=\"Year\", \n",
    "                      data=cbPlusT_df_down, markers=filled_markers[1:], ax=scatax[1,0], legend=False)\n",
    "ax3 = sns.scatterplot(x='Days since Jan 1st', y=\"PH\", hue=\"Latitude_\", style=\"Year\", \n",
    "                      data=cbPlusT_df_down, markers=filled_markers[1:], ax=scatax[2,0], legend=False)\n",
    "ax4 = sns.scatterplot(x='Days since Jan 1st', y=\"WTEMP\", hue=\"Latitude_\", style=\"Year\", \n",
    "                     data=cbPlusT_df_up, markers=filled_markers, ax=scatax[0,1], legend=False)\n",
    "ax5 = sns.scatterplot(x='Days since Jan 1st', y=\"DO\", hue=\"Latitude_\", style=\"Year\", \n",
    "                      data=cbPlusT_df_up, markers=filled_markers, ax=scatax[1,1], legend=False)\n",
    "ax6 = sns.scatterplot(x='Days since Jan 1st', y=\"PH\", hue=\"Latitude_\", style=\"Year\", \n",
    "                      data=cbPlusT_df_up, markers=filled_markers, ax=scatax[2,1], legend=False)\n",
    "ax5.set_xlabel(\"\"); ax4.set_xlabel(\"\"); ax1.set_xlabel(\"\"); ax2.set_xlabel(\"\");\n",
    "ax4.set_title(\"Surface Water\", fontsize=14)\n",
    "ax1.set_title(\"Bottom Water\", fontsize=14)\n",
    "scatfig.savefig(time_series_rep1_f)\n",
    "\n",
    "legend_rep1_f = \"../otu_data/WaterQualityData/figs/DO_WTEMP_PH_bySurface_Bottom_legend.png\"\n",
    "scatfig_l, scatax = plt.subplots(3,2,figsize=(9,9), dpi=120)\n",
    "scatax[1,0].set_axis_off();scatax[1,1].set_axis_off(); \n",
    "scatax[0,0].set_axis_off();scatax[0,1].set_axis_off();\n",
    "scatax[2,0].set_axis_off();scatax[2,1].set_axis_off();\n",
    "ax2 = sns.scatterplot(x='Days since Jan 1st', y=\"WTEMP\", hue=\"Latitude_\", style=\"Year\", \n",
    "                     data=cbPlusT_df_up, markers=filled_markers, ax=scatax[0,1], legend='full')\n",
    "ax2.set_xlim((300, 589.45))\n",
    "scatfig_l.savefig(legend_rep1_f)\n",
    "plt.show()\n",
    "cbPlusT_df_down.loc[:, ['StationName', 'Latitude_']].groupby('StationName').agg(np.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_vars = ['enspie', 'faith_pd', 'TSS', 'CHLA', 'DO', 'TP', 'TN', 'WTEMP', \n",
    "               'NO2F', 'NO3F', 'NO23F', 'NH4F', 'DOP', 'PHEO', 'PC', 'Discharge_Susquehanna_14', \n",
    "               \"Discharge_James_14\"]\n",
    "\n",
    "select_vars = ['CHLA', 'TN', 'NO2F', 'NO3F', 'NH4F',\n",
    "               'DOP', 'TP', 'PHEO', 'PC', 'Discharge_Susquehanna_14']\n",
    "\n",
    "print(jt_df.StationName.unique().shape)\n",
    "jt_df['Days Since Jan 1st'] = jt_df.julian_day % 365.\n",
    "jt_df['Latitude_'] = jt_df.Latitude.astype(float).apply(lambda x: float(str(x)[:5]))\n",
    "jt_df_down = jt_df[jt_df.Depth_Percentage > 0.8]\n",
    "jt_df_up = jt_df[jt_df.depth_float < 2.0 ]\n",
    "print(jt_df_up.loc[:, ['DO', 'PH', 'CHLA', 'WTEMP']].astype(float).corr())\n",
    "print((jt_df.PN / jt_df.TN).mean(), (jt_df.PN / jt_df.TN).std(), np.median((jt_df.PN / jt_df.TN)))\n",
    "print((jt_df.TDN / jt_df.TN).mean(), (jt_df.TDN / jt_df.TN).std(), np.median((jt_df.TDN / jt_df.TN)))\n",
    "print((jt_df.DON / jt_df.TN).mean(), (jt_df.DON / jt_df.TN).std(), np.median((jt_df.DON / jt_df.TN)))\n",
    "print((jt_df.DIN / jt_df.TN).mean(), (jt_df.DIN / jt_df.TN).std(), np.median((jt_df.DIN / jt_df.TN)))\n",
    "print((jt_df.NH4F / jt_df.TN).mean(), (jt_df.NH4F / jt_df.TN).std(), np.median((jt_df.NH4F / jt_df.TN)))\n",
    "print((jt_df.NO3F / jt_df.TN).mean(), (jt_df.NO3F / jt_df.TN).std(), np.median((jt_df.NO3F / jt_df.TN)))\n",
    "\n",
    "plt.clf(); plt.close('all');\n",
    "scatfig2, scatax = plt.subplots(len(select_vars), 2, figsize=(9,len(select_vars)*4), dpi=120)\n",
    "for sv_x, sv in enumerate(select_vars):\n",
    "    ax1 = sns.scatterplot(x='Days Since Jan 1st', y=sv, hue=\"Latitude_\", style=\"Year\", \n",
    "                         data=jt_df_down, markers=filled_markers, ax=scatax[sv_x,0], legend=False)\n",
    "    if sv_x == 0:\n",
    "        ax4 = sns.scatterplot(x='Days Since Jan 1st', y=sv, hue=\"Latitude_\", style=\"Year\", \n",
    "                             data=jt_df_down, markers=filled_markers[1:], ax=scatax[sv_x,1], legend='full')\n",
    "        ax4.legend(loc=2)\n",
    "        ax4.set_xlim((300, 500))\n",
    "    if not sv_x in [5, 9]:\n",
    "        scatax[sv_x,0].set_xlabel(\"\")\n",
    "    \n",
    "    scatax[sv_x,1].set_axis_off();\n",
    "        \n",
    "plt.subplots_adjust(wspace=.2, hspace=0.2)\n",
    "plt.show()\n",
    "timesrs2_rep1_f = \"../otu_data/WaterQualityData/figs/Chem_Discharge_Bottom_preformatted.png\"\n",
    "scatfig2.savefig(timesrs2_rep1_f)\n",
    "\n",
    "print(probe_data.shape, probe_data.columns)\n",
    "print(jt_df.shape, jt_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biom.table import Table\n",
    "from biom.util import biom_open\n",
    "\n",
    "name_checks = {'SAR11_clade': 'SAR11 clade',\n",
    "               'SAR86_clade': 'SAR86 clade'}\n",
    "name_corrector = lambda x: name_checks[x] if x in name_checks.keys() else x\n",
    "\n",
    "sample_ids = []\n",
    "for i in list(abund_md_df_derep.index):\n",
    "    sample_ids.append(i)\n",
    "\n",
    "for __df__, _outname_ in zip([abund_md_df_derep, abund_df_tr], ['_partial', '_full']):\n",
    "    observ_ids, observ_metadata = [], []\n",
    "    for i in list(__df__.columns):\n",
    "        if i.startswith(\"OTU\") and i in list(taxa_df.index):\n",
    "            observ_ids.append(i)\n",
    "            observ_metadata.append({'taxonomy': [name_corrector(j) for j in taxa_df.loc[i, :].dropna().values]})\n",
    "\n",
    "    _data_ = __df__.loc[sample_ids, observ_ids].values.T\n",
    "    print(_outname_, _data_.shape, __df__.shape)\n",
    "    table = Table(_data_, observ_ids, sample_ids, observ_metadata, None)\n",
    "    with biom_open('../otu_data/FAPROTAX_out/otu_taxa{}.biom'.format(_outname_), 'w') as f:  \n",
    "        table.to_hdf5(f, \"faith and trust\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_fxn_df = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/report_full.txt\", sep=\"\\t\",\n",
    "                 index_col=0, comment='#')\n",
    "fxn_rare_df = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/functional_taxa_partial.txt\", \n",
    "                          sep=\"\\t\", index_col=0)\n",
    "\n",
    "fxn_full_df = pd.read_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/FAPROTAX_out/functional_taxa_full.txt\", \n",
    "                          sep=\"\\t\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel 1 of Alpha Diversity Figure (1): CB33C Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_limit_cols  = ['enspie_25', 'enspie_975', 'faith_pd_25', 'faith_pd_975']\n",
    "assert set(probe_data.index).issubset(meta_df_x.index)\n",
    "alpha_CB33 = probe_data.join(meta_df_x.loc[probe_data.index, ci_limit_cols])\n",
    "alpha_CB33 = alpha_CB33.join(super_matched3_derep.loc[probe_data.index, 'pd_date'])\n",
    "xmax1 = (alpha_CB33.enspie+alpha_CB33.enspie_975).max()*1.1\n",
    "xmax2 = (alpha_CB33.faith_pd + alpha_CB33.faith_pd_975).max()*1.1\n",
    "\n",
    "label_maker = lambda x: \"{:02}/{:02} {}m\".format(x[0].month, x[0].day, int(x[1]))\n",
    "\n",
    "plt.clf()\n",
    "fig, ax_arr = plt.subplots(nrows=1, ncols=3, figsize=(8.5,11), dpi=250)\n",
    "height_ = 0.45\n",
    "fig.text(0.5, 0.07, 'Effective Evenness (ENSpie)', ha='center')\n",
    "fig.text(0.5, 0.94, \"Phylogenetic Diversity (Faith)\", ha='center')\n",
    "for ar_, yr_ in enumerate(alpha_CB33.Year.unique()):\n",
    "    sub_df_pre = alpha_CB33[alpha_CB33.Year == yr_].copy().sort_values(['pd_date', 'depth_float'], ascending=False)\n",
    "    sub_df = sub_df_pre.reset_index()\n",
    "    y_reverse = sub_df.index.values #(sub_df.index.values - max(sub_df.index.values))*-1\n",
    "    labs =  sub_df.loc[:, ['pd_date', 'depth_float']].apply(label_maker, axis=1)\n",
    "    ax_arr[ar_].set_title(\"20\"+yr_)\n",
    "    ax_arr[ar_].grid(b=True, linestyle='-', axis='x')\n",
    "    ax_arr[ar_].grid(b=False, axis='y')#, color=\"#000000\")\n",
    "    ax_arr[ar_].barh(y=y_reverse-height_, width=sub_df.loc[y_reverse, 'enspie'].values, height=height_,\n",
    "                     xerr=sub_df.loc[:, ['enspie_25', 'enspie_975']].values.T,  color=\"#FF1493\",\n",
    "                     tick_label=labs.values, label=\"ENSPIE\")\n",
    "    ax_arr[ar_].set_ylim([-1, len(y_reverse)-0.5])\n",
    "    ax_arr[ar_].set_xlim([0., xmax1])\n",
    "    ax_arr[ar_].set_facecolor('#DCDCDC')\n",
    "    ar_2 = ax_arr[ar_].twiny()\n",
    "    ar_2.grid(False, 'both')# , linestyle='--', axis='x', color=\"#000000\")\n",
    "    ar_2.barh(y=y_reverse, width=sub_df.loc[y_reverse, 'faith_pd'].values,\n",
    "             xerr=sub_df.loc[:, ['faith_pd_25', 'faith_pd_975']].values.T, \n",
    "             color=\"#00BFFF\", height=height_, label=\"Faith's PD\")\n",
    "    ar_2.set_xlim([0., xmax2])\n",
    "    ar_2.tick_params(which='both', labelsize=8)\n",
    "    ax_arr[ar_].tick_params(which='both', labelsize=8)\n",
    "    if ar_ == 0:\n",
    "        fig.legend(loc='lower center')\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4)\n",
    "fig.savefig(\"../otu_data/alpha_scatter_cb33.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel 2 of Alpha Diveristy Figure (1): Transect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "face_colors = {'Mesohaline':'#DCDCDC',\n",
    "               'Oligohaline':'#EAEDED',\n",
    "               'Polyhaline':'#A9A9A9'}\n",
    "t_col_1 = ['CB22', 'CB31', 'CB32', 'CB41C', 'CB42C', 'CB43C', 'CB44']\n",
    "t_col_2 = ['CB51', 'CB52', 'CB53', 'CB54', 'CB61', 'CB62']\n",
    "t_col_3 = ['CB63', 'CB64', 'CB71', 'CB72', 'CB73', 'CB74']\n",
    "\n",
    "assert set(jt_df.index).issubset(meta_df_x.index)\n",
    "transect_data = jt_df.join(meta_df_x.loc[jt_df.index, ci_limit_cols])\n",
    "transect_data = transect_data.join(super_matched3_derep.loc[jt_df.index, ['pd_date', 'Salinity_Group']])\n",
    "\n",
    "label_maker2 = lambda x: x[0]+\"/\"+x[2]+\" \"+str(int(x[1]))+\"m\"\n",
    "\n",
    "height_2 = 0.3\n",
    "xmaxt1 = (transect_data.enspie + transect_data.enspie_975).max()*1.1\n",
    "xmaxt2 = (transect_data.faith_pd + transect_data.faith_pd_975).max()*1.1\n",
    "\n",
    "for col_nstr, stat_grp in enumerate([t_col_1, t_col_2, t_col_3]):\n",
    "    plt.clf()\n",
    "    subdf = transect_data[transect_data.StationName.isin(stat_grp)]\n",
    "    hrs = [subdf[subdf.StationName == i].shape[0] for i in stat_grp]\n",
    "    if col_nstr == 2:\n",
    "        hrs.append(hrs[-1])\n",
    "    \n",
    "    fig2 = plt.figure(constrained_layout=True, figsize=(2.83,11), dpi=250)\n",
    "    gs = gridspec.GridSpec(len(hrs), 1, height_ratios=hrs, figure=fig2)\n",
    "    \n",
    "    for facet in range(gs._nrows):\n",
    "        if (col_nstr == 2) and (facet == (gs._nrows-1)):\n",
    "            facax = plt.subplot(gs[facet])\n",
    "            facax.axis('off')\n",
    "        else:\n",
    "            this_stat = stat_grp[facet]\n",
    "            subdubdf_pre = subdf.loc[subdf.StationName == this_stat].copy()\n",
    "            subdubdf_pre.sort_values(['depth_float', 'pd_date'], ascending=False, inplace=True)\n",
    "            subdubdf = subdubdf_pre.reset_index()\n",
    "            y_pos = subdubdf.index.values\n",
    "            \n",
    "            labs =  subdubdf.loc[:, ['Month', 'depth_float', 'Year']].apply(label_maker2, axis=1)\n",
    "            facax = plt.subplot(gs[facet])\n",
    "            facax.set_facecolor(face_colors[subdubdf.Salinity_Group.values[0]])\n",
    "            facax.barh(y=y_pos-height_2, width=subdubdf.loc[:, 'enspie'].values, height=height_2,\n",
    "                       xerr=subdubdf.loc[:, ['enspie_25', 'enspie_975']].values.T,  color=\"#FF1493\",\n",
    "                       tick_label=labs.values, label=\"ENSPIE\")\n",
    "            facax.set_xlim([0., xmaxt1])\n",
    "            facax_2 = facax.twiny()\n",
    "            facax_2.barh(y=y_pos, width=subdubdf.loc[:, 'faith_pd'].values,\n",
    "                         xerr=subdubdf.loc[:, ['faith_pd_25', 'faith_pd_975']].values.T, \n",
    "                         color=\"#00BFFF\", height=height_2, label=\"Faith's PD\")\n",
    "            facax_2.set_xlim([0., xmax2-1.5])\n",
    "            \n",
    "            facax_2.grid(b=False)#, which='minor', axis='x', color=\"#000000\")\n",
    "            facax_2.add_artist(AnchoredText(this_stat, borderpad=0., prop=dict(size=12), \n",
    "                                            frameon=False, loc='center right'))\n",
    "            \n",
    "            if facet != (gs._nrows - 1) and (col_nstr != 2):\n",
    "                # the bottom one only retains the enspie labels\n",
    "                facax.set_xticklabels([])\n",
    "                facax.tick_params(axis=u'x', which=u'both', length=0)\n",
    "            elif facet != (gs._nrows - 2) and (col_nstr == 2):\n",
    "                facax.set_xticklabels([])\n",
    "                facax.tick_params(axis=u'x', which=u'both',length=0)\n",
    "            elif facet == (gs._nrows - 1) and (col_nstr == 1):\n",
    "                facax.set_xlabel('Effective # of Sp. (ENSPIE)')\n",
    "            \n",
    "            if facet != 0:\n",
    "                # the top one only has pd labels\n",
    "                facax_2.set_xticklabels([])\n",
    "                facax_2.tick_params(axis=u'x', which=u'both',length=0)\n",
    "            elif (facet == 0) and (col_nstr == 1):\n",
    "                facax_2.set_xlabel(\"Faith's PD\")\n",
    "            \n",
    "            if this_stat == 'CB63':\n",
    "                fig2.legend(loc='lower center')\n",
    "    \n",
    "    fig2.savefig(\"../otu_data/alpha_scatter_transect_{}.png\".format(col_nstr), dpi=250)\n",
    "\n",
    "plt.clf(); plt.close();\n",
    "\n",
    "fig3 = plt.figure(figsize=(8.5, 11), dpi=250)\n",
    "for i in range(3):\n",
    "    img=mpimg.imread(\"../otu_data/alpha_scatter_transect_{}.png\".format(i))\n",
    "    sub = fig3.add_subplot(1, 3, i + 1)\n",
    "    sub.axis('off')\n",
    "    sub.imshow(img)\n",
    "\n",
    "fig3.tight_layout()\n",
    "fig3.subplots_adjust(wspace=0)\n",
    "fig3.savefig(\"../otu_data/alpha_scatter_transect.png\", dpi=250)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reformat distmat produced by raxml\n",
    "dist_file = \"../otu_data/tree_data/not_full_tree/RAxML_distances.query_high_abund.distmat\"\n",
    "melted_dists = pd.read_csv(dist_file, sep=\"\\t\", usecols=range(2), header=None)\n",
    "melted_dists.columns = ['OTUS', 'Distance']\n",
    "\n",
    "# split first column\n",
    "temp = melted_dists[\"OTUS\"].str.split(\" \", n = 1, expand = True) \n",
    "melted_dists[\"OTUONE\"]= temp[0] \n",
    "melted_dists[\"OTUTWO\"]= temp[1] \n",
    "melted_dists.drop(['OTUS'], inplace=True, axis=1)\n",
    "melted_srs = melted_dists.set_index(['OTUONE', 'OTUTWO']).Distance\n",
    "dist_df = melted_srs.unstack(level=-1)\n",
    "dist_df.columns = [i.strip() for i in dist_df.columns]\n",
    "\n",
    "# Make into a hollow symmetrical matrix\n",
    "dist_df.insert(0, 'OTU1', pd.Series(index=dist_df.index))\n",
    "dist_t = dist_df.T\n",
    "dist_df2 = dist_t.join(pd.Series(index=dist_t.index, name='OTU9982')).T\n",
    "distmat = dist_df2.values\n",
    "np.fill_diagonal(distmat, 0)\n",
    "dist_df3 = pd.DataFrame(distmat, index=dist_df2.index, columns=dist_df2.columns)\n",
    "upper_triangle = dist_df3.fillna(0).values\n",
    "symmetric_ = upper_triangle + upper_triangle.T\n",
    "\n",
    "# write out\n",
    "done_dist = pd.DataFrame(symmetric_, index=dist_df3.index, columns=dist_df3.columns)\n",
    "done_dist.to_csv(\"../otu_data/dispersal_selection_data/not_full_tree_distances.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out sequences to make a tree. This is commended out because it was done and should not be redone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_path = \"../otu_data/tree_data\"\n",
    "#fnames = ['query_high_abund002.fasta', 'query_high_abund0005.fasta']\n",
    "#abund_df_ogs = [abund_df_og_s1 ]#, abund_df_og_p1]\n",
    "#for fname, abund_df_og in zip(fnames, abund_df_ogs):\n",
    "#    heads = sorted(list(abund_df_og.columns))\n",
    "#    tails = [OTU_name2seq[i] for i in heads]\n",
    "#    with open(os.path.join(out_path, fname), \"w\") as wofh:\n",
    "#        print(wofh.write(\"\".join([\">{}\\n{}\\n\".format(i, j) for i, j in zip(heads, tails)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This calculates the inter-run distances, plots their pdfs, and performs Mann-Whitney U test on each pairing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "groups = list(super_df['sequencing_ID'].unique())\n",
    "\n",
    "def calculate_group_distance(a_label, a_table):\n",
    "    # subset according to \"in\" label \n",
    "    in_indexes = super_df[super_df['sequencing_ID'] == a_label].index\n",
    "    \n",
    "    # subset according to \"out\" label\n",
    "    out_group = set(groups) - set([a_label]) \n",
    "    out_indexes = super_df[super_df['sequencing_ID'].isin(out_group)].index\n",
    "    assert len(out_indexes) + len(in_indexes) == len(a_table.index)\n",
    "    \n",
    "    # calculate bray-curtis for all groups\n",
    "    bc_dists = beta_diversity(\"braycurtis\", a_table.values, a_table.index)\n",
    "    bc_df = pd.DataFrame(bc_dists._data, index=a_table.index, columns=a_table.index)\n",
    "    \n",
    "    # subset rows by \"in\" and columns by \"out\" \n",
    "    sub_bcdf = bc_df.loc[in_indexes, out_indexes]\n",
    "    if sub_bcdf.shape[0] == 1:\n",
    "        flat_df = sub_bcdf.T\n",
    "        flat_df.columns = [a_label]\n",
    "    else:\n",
    "        flat_df = pd.DataFrame(sub_bcdf.values.flatten(), columns=[a_label])\n",
    "        \n",
    "    print(a_label, flat_df.shape, sub_bcdf.shape)\n",
    "    return (flat_df, flat_df[a_label].mean())\n",
    "\n",
    "out_data = {}\n",
    "for a_label in groups:\n",
    "    out_data[a_label] = calculate_group_distance(a_label, rare_pp.copy())\n",
    "\n",
    "_ = out_data.pop('controls'); groups.remove('controls'); \n",
    "\n",
    "flat_dfs = pd.DataFrame({l:fdf[l].sample(4000).values for l, (fdf, m) in out_data.items()})\n",
    "\n",
    "# calculate average distance \n",
    "\n",
    "color_choices = [\"sky blue\", \"olive\", \"gold\", \"teal\", \"rich blue\", \"wisteria\", \"lipstick red\"]\n",
    "\n",
    "plt.clf()\n",
    "f, _axes_ = plt.subplots(nrows=1, ncols=1, figsize=(12, 12), dpi=150)\n",
    "f.suptitle(\"Samples of all dist hists\")\n",
    "for a_label, cc in zip(groups, color_choices):\n",
    "    sns.distplot(flat_dfs[a_label], hist=False, axlabel='bray-curtis distance',\n",
    "                 kde_kws={\"label\":a_label, \n",
    "                          \"lw\": 3, \n",
    "                          \"color\": sns.xkcd_rgb[cc]}, \n",
    "                 ax=_axes_)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"../otu_data/pca_plots/cross_dists_abthrsh.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "pair_tests = pd.DataFrame(index=groups, columns=groups)\n",
    "for ix in pair_tests.index:\n",
    "    for cx in pair_tests.columns:\n",
    "        result = mannwhitneyu(flat_dfs.loc[:, cx].values, flat_dfs.loc[:, ix].values)\n",
    "        pair_tests.loc[ix, cx] = result[1]*2\n",
    "\n",
    "sig_level = 0.05/(7*6)\n",
    "print(\"Significance level is {}\".format(sig_level))\n",
    "pair_tests < sig_level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 2: Hierarchical Clustering of Principal Components w/ Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in super_matched3_derep.columns if '_wu' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabaz_score, silhouette_samples\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# There are three ways of doing this: using rclr+euclidean, bray-curtis, or unifrac\n",
    "\n",
    "#score_types = ['CalinskiHarabaz', \"SilhouetteScore\"]\n",
    "#suffix_types = [ \"_clr\", \"_bc\", \"_wu\"]\n",
    "\n",
    "score_types = [\"SilhouetteScore\"]\n",
    "suffix_types = [ \"_wu\"]\n",
    "linkage_types = [\"complete\", \"average\", 'single']\n",
    "\n",
    "cluster_params = [(i, j, k, j+i+\"_\"+k) for i in suffix_types for j in score_types for k in linkage_types]\n",
    "print(list(zip(*cluster_params))[3][0])\n",
    "clust_num_range = list(range(2,30))\n",
    "clust_scores = pd.DataFrame(index=clust_num_range, columns=list(zip(*cluster_params))[3]).astype(float)\n",
    "met_dict = {'CalinskiHarabaz':calinski_harabaz_score, \"SilhouetteScore\":silhouette_samples}\n",
    "\n",
    "dist_dict = {}\n",
    "for st, sct, lkt, colname_ in cluster_params:\n",
    "    print(st, sct, lkt, colname_)\n",
    "    suff_cols = [i for i in abund_md_df_derep.columns if st in i and i[:(-1*len(st))] in abund_md_df_derep.index]\n",
    "    assert len(suff_cols) == len(abund_md_df_derep.index)\n",
    "    normval = 1\n",
    "    for i, j in zip(abund_md_df_derep.index, suff_cols):\n",
    "        print(st, i, k, i[:(-1*len(st))] in abund_md_df_derep.index)\n",
    "        assert i == j[:(-1*len(st))]\n",
    "    precomputed_dists = abund_md_df_derep.loc[:, suff_cols]\n",
    "    dist_dict[st] = precomputed_dists.copy()\n",
    "    if sct == 'SilhouetteScore':\n",
    "        normval = 1\n",
    "    for n_clusters_ in clust_num_range:\n",
    "        cluster_mod = AgglomerativeClustering(n_clusters=n_clusters_, affinity='precomputed', linkage=lkt) \n",
    "        cluster_labels = cluster_mod.fit_predict(precomputed_dists.values)\n",
    "        with_misclassified = met_dict[sct](precomputed_dists.values, cluster_labels)\n",
    "        clust_scores.loc[n_clusters_, colname_] = with_misclassified[with_misclassified > 0].mean()\n",
    "\n",
    "plt.close(); plt.clf();\n",
    "fig_cscore, ax_cscore = plt.subplots(nrows=1, ncols=1, figsize=(6, 6), dpi=120)\n",
    "clust_scores.SilhouetteScore_wu_average.plot(ax=ax_cscore)\n",
    "plt.grid()\n",
    "fig_cscore.savefig(\"../otu_data/pca_plots/SilhouetteScore_WeightedUnifrac.png\")\n",
    "plt.show()\n",
    "plt.close(); plt.clf();\n",
    "for sc in clust_scores.columns:\n",
    "    print(sc, clust_scores[sc].idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# clean clusters\n",
    "for n_clusters in [5, 9, 12]:\n",
    "    best_mod = AgglomerativeClustering(n_clusters=n_clusters, affinity='precomputed', linkage='average') \n",
    "    wu_dists = dist_dict['_wu']\n",
    "    cluster_labels = best_mod.fit_predict(wu_dists.values)\n",
    "    sample_silhouette_values = silhouette_samples(wu_dists.values, cluster_labels)\n",
    "    positive_idxs = wu_dists.index[sample_silhouette_values > -1]\n",
    "    positive_silhouettes = sample_silhouette_values[sample_silhouette_values > -1]\n",
    "    print(positive_silhouettes.mean())\n",
    "    positive_labels = cluster_labels[sample_silhouette_values > -1]\n",
    "    label_counts = np.unique(positive_labels, return_counts=1)\n",
    "\n",
    "    good_clusts = label_counts[0][label_counts[1] > 0]\n",
    "    print(\"all clusters\", set(cluster_labels))\n",
    "    print(\"good clusters\", good_clusts)\n",
    "    print(\"total clustered\", label_counts[1][label_counts[1] > 0].sum())\n",
    "\n",
    "    fig, ax1 = plt.subplots(nrows=1, ncols=1, dpi=120, figsize=(6,6))\n",
    "    ax1.set_xlim([-.45, .8])\n",
    "    ax1.set_ylim([0, len(wu_dists.values) + ((1+len(good_clusts))* 10) ])\n",
    "    y_lower = 10\n",
    "\n",
    "    clean_clusts = {}\n",
    "    for i in good_clusts:\n",
    "        print(i)\n",
    "        ith_cluster_silhouette_values = positive_silhouettes[positive_labels == i]\n",
    "        clean_clusts[i] = positive_idxs[positive_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        color = cm.nipy_spectral((float(i)*2) / (2*n_clusters))\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.35, y_lower + 0.5 * size_cluster_i, \"Cluster {} : ({})\".format(i, size_cluster_i), fontsize=14)\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for cleaned clusters.\", fontsize=14)\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize=14)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"../otu_data/pca_plots/sample_silhouettes_for_{}_clusters.png\".format(n_clusters))\n",
    "    plt.clf(); plt.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.spatial.distance as ssd\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "clean_labs = []\n",
    "for c_i, (ccn, ccv) in enumerate(clean_clusts.items()):\n",
    "    print(c_i, \"Clust {} has {} members: {}\".format(ccn, len(ccv), ccv[0]))\n",
    "    clean_labs += list(ccv)\n",
    "    \n",
    "ordered_index = [i for i in wu_dists.index if i in clean_labs]\n",
    "ordered_columns = [i for i in wu_dists.columns if i[:-3] in clean_labs]\n",
    "cleaned_df = wu_dists.loc[ordered_index, ordered_columns]\n",
    "sqmat = ssd.squareform(cleaned_df.values)\n",
    "toplim, botlim = sqmat.max(), sqmat[sqmat > 0.0].min()\n",
    "\n",
    "# fix abundance and metadata tables\n",
    "abund_md_df_clust = abund_md_df.loc[ordered_index, :]\n",
    "\n",
    "opt_clusts = len(clean_clusts.values())\n",
    "plt.clf()\n",
    "# Override the default linewidth.\n",
    "matplotlib.rcParams['lines.linewidth'] = 0.5\n",
    "Z = hierarchy.linkage(sqmat, 'average', optimal_ordering=True)\n",
    "fig_ = plt.figure(figsize=(7, 6), dpi=250)\n",
    "gs = gridspec.GridSpec(5, 4, figure=fig_, wspace=0.01, hspace=0.01, \n",
    "                       height_ratios=[75,5,5,5,5], width_ratios=[18,.4,1.3,1.3])\n",
    "ax1 = plt.subplot(gs[0,0])\n",
    "\n",
    "\n",
    "for c_height in np.arange(botlim*0.1, toplim*1.1, botlim*.01):\n",
    "    cuttree = hierarchy.cut_tree(Z, height=[c_height])\n",
    "    if len(set(list(cuttree[:, 0]))) <= opt_clusts:\n",
    "        break\n",
    "\n",
    "#c_height = 0.55\n",
    "\n",
    "print(\"optimal minimum cophenetic distance is {}\".format(c_height))\n",
    "hierarchy.set_link_color_palette(['tab:blue', 'tab:orange', 'tab:green', 'tab:red', \n",
    "                                  'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', \n",
    "                                  'tab:olive', 'tab:cyan'])\n",
    "\n",
    "res = hierarchy.dendrogram(Z, color_threshold=c_height*1.01, get_leaves=True, \n",
    "                           ax=ax1, labels=cleaned_df.index, no_labels=True, \n",
    "                           above_threshold_color='k')\n",
    "ax1.axhline(y=c_height*1.01)\n",
    "ax1.axis('off')\n",
    "matplotlib.rcParams['lines.linewidth'] = 1.5\n",
    "\n",
    "# get axis order\n",
    "new_idx = cleaned_df.index[res['leaves']]\n",
    "# metadata columns to plot\n",
    "sel_md_cols = ['Month_Year', 'StationName', #'Pycnocline', \n",
    "               'Salinity_Group', 'sequencing ID', 'CollectionAgency']\n",
    "# resort metadata\n",
    "clustered_metadata = abund_md_df_derep.loc[new_idx, sel_md_cols].copy()\n",
    "# encode colors and reverse\n",
    "color_codings = [{1:'06 15', 2:'07 15', 3:'08 15', 7:'06 16', 8:'07 16', 9:'08 16', \n",
    "                  13:'04 17', 14:'05 17', 15:'06 17', 16:'07 17', 17:'08 17', 20:'09 17'},\n",
    "                 {4:'CB33C', 1:'CB22', 5:'CB41C', 7:'CB43C', 8:'CB44', 9:'CB51', 10:'CB52', \n",
    "                  11:'CB53', 12:'CB54', 17:'CB71', 14:'CB62', 15:'CB63', 18:'CB72', 16:'CB64', \n",
    "                  20:'CB74', 2:'CB31', 3:'CB32', 6:'CB42C', 13:'CB61', 19:'CB73'},\n",
    "                 #{1:'Above', 20:'Below', 10:np.nan}, \n",
    "                 {10:'Mesohaline', 1:'Oligohaline', 20:'Polyhaline'},\n",
    "                 {1:'sprehei1_123382', 4:'Miseq_data_SarahPreheim_Sept2016', 7:'esakows1_132789', \n",
    "                  10:'Keith_Maeve1_138650', 13:'sprehei1_149186', 17:'esakows1_152133_plate_1', \n",
    "                  20:'esakows1_152133_plate_2'}, {1:\"Preheim\", 10:\"ODU\", 20:\"DNR\"}]\n",
    "\n",
    "rev_col_codes = []\n",
    "for x in color_codings:\n",
    "    rev_col_codes.append({j:i for i, j in x.items()})\n",
    "\n",
    "# map encodings\n",
    "for sdc, cmapping in zip(sel_md_cols, rev_col_codes):\n",
    "    clustered_metadata[sdc] = clustered_metadata[sdc].map(cmapping)\n",
    "\n",
    "# select palette with max # of categories\n",
    "#all_col_pal = sns.color_palette(\"cubehelix_r\", clustered_metadata.max().max())\n",
    "all_col_pal = sns.cubehelix_palette(n_colors=clustered_metadata.max().max(), start=.3, rot=1.5, gamma=1.0,\n",
    "                                    hue=0.7, light=0.95, dark=0.25, reverse=0)\n",
    "cmap_ = LinearSegmentedColormap.from_list('Custom', tuple(all_col_pal), len(tuple(all_col_pal)))\n",
    "\n",
    "mdata_nrow = clustered_metadata.shape[0]\n",
    "# plot first row and bar\n",
    "ax2 = plt.subplot(gs[1,0])\n",
    "ax2l = plt.subplot(gs[1,1:])\n",
    "ax2_vals = clustered_metadata['Month_Year'].values.reshape(mdata_nrow,1).T\n",
    "sns.heatmap(ax2_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax2)\n",
    "ax2l.text(0.,0.4, \"Sampling Date\", fontdict={'fontsize':8})\n",
    "ax2.axis('off'); ax2l.axis('off');\n",
    "\n",
    "ax4 = plt.subplot(gs[2,0])\n",
    "ax3 = plt.subplot(gs[0,1])\n",
    "ax4_vals = clustered_metadata['StationName'].values.reshape(mdata_nrow,1).T\n",
    "stat_ax = sns.heatmap(ax4_vals, linewidths=0.0, cmap=cmap_, cbar=True, cbar_ax=ax3, xticklabels=False, yticklabels=False, ax=ax4)\n",
    "cb = stat_ax.collections[0].colorbar\n",
    "cb.ax.tick_params(labelsize=6);\n",
    "cb.set_ticks([1, 20]); cb.set_ticklabels(['Min', 'Max']); \n",
    "ax4l = plt.subplot(gs[2,1:])\n",
    "ax4l.text(0.,0.4, \"Station\", fontdict={'fontsize':8})\n",
    "ax4.axis('off'); ax4l.axis('off');\n",
    "\n",
    "ax6 = plt.subplot(gs[3,0])\n",
    "ax6l = plt.subplot(gs[3,1:])\n",
    "ax6_vals = clustered_metadata['Salinity_Group'].values.reshape(mdata_nrow,1).T\n",
    "sns.heatmap(ax6_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax6)\n",
    "ax6l.text(0.,0.4, \"Salinity Region\", fontdict={'fontsize':8})\n",
    "ax6.axis('off'); ax6l.axis('off');\n",
    "\n",
    "#ax7 = plt.subplot(gs[4,0])\n",
    "#ax7l = plt.subplot(gs[4,1:])\n",
    "#ax7_vals = clustered_metadata[\"Pycnocline\"].values.reshape(mdata_nrow,1).T\n",
    "#sns.heatmap(ax7_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax7)\n",
    "#ax7l.text(0.,0.4, \"Pycnocline\", fontdict={'fontsize':6})\n",
    "#ax7.axis('off'); ax7l.axis('off');\n",
    "\n",
    "ax8 = plt.subplot(gs[4,0])\n",
    "ax8l = plt.subplot(gs[4,1:])\n",
    "ax8_vals = clustered_metadata[\"CollectionAgency\"].values.reshape(mdata_nrow,1).T\n",
    "sns.heatmap(ax8_vals, linewidths=0.0, cmap=cmap_, cbar=False, xticklabels=False, yticklabels=False, ax=ax8)\n",
    "ax8l.text(0.,0.4, \"Agency\", fontdict={'fontsize':8})\n",
    "ax8.axis('off'); ax8l.axis('off');\n",
    "\n",
    "fig_.savefig('../otu_data/pca_plots/pca_of_wu_avlinkage_clusters_clean.png', dpi=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets make PCA plots colored by metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deicode.optspace import OptSpace\n",
    "\n",
    "rename_cols = {i - 1: 'PC' + str(i) for i in range(1, 3)}\n",
    "\n",
    "print(\"Decomposing {} dist table {}\".format('Weighted Unifrac', cleaned_df.shape))\n",
    "opt_i = OptSpace(rank=2).fit(cleaned_df.values)\n",
    "sl_i = pd.DataFrame(opt_i.sample_weights, index=cleaned_df.index)\n",
    "sample_loadings = sl_i.rename(columns=rename_cols)\n",
    "\n",
    "rev_cc_mapping = {k:\"Clust\"+str(i) for i,j in clean_clusts.items() for k in j}\n",
    "\n",
    "clustcolors = [cm.nipy_spectral((float(i)*2) / 24) for i in range(12)]\n",
    "keithcm = LinearSegmentedColormap.from_list('keithspec', clustcolors, N=len(clustcolors))\n",
    "\n",
    "mdf = pd.DataFrame(index=cleaned_df.index, columns=['ClustAssgns'])\n",
    "mdf['ClustAssgns'] = cleaned_df.reset_index()['Samples'].map(rev_cc_mapping).values\n",
    "\n",
    "from skbio import DistanceMatrix\n",
    "from skbio.stats.ordination import pcoa\n",
    "plt.clf(); plt.close();\n",
    "dm = DistanceMatrix(cleaned_df.values, cleaned_df.index)\n",
    "pcoa_results = pcoa(dm)\n",
    "fig = pcoa_results.plot(df=mdf, column='ClustAssgns',\n",
    "                        title='', cmap=keithcm, s=25)\n",
    "\n",
    "fig.set_size_inches(w=8, h=6, forward=True)\n",
    "fig.suptitle(\"PCoA of Weighted Unifrac\", x=0.34, y=0.95, fontsize=14)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('PC1', fontsize=12)\n",
    "ax.set_ylabel('PC2', fontsize=12)\n",
    "ax.set_zlabel('PC3', fontsize=12)\n",
    "fig.set_dpi(120)\n",
    "fig.savefig(\"../otu_data/pca_plots/3d_pca_wu_coloredbyclust.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "clean_join = lambda x: tuple([x[0].strip(), x[1].strip()])\n",
    "melted_dists['pair'] = melted_dists.loc[:, ['OTUONE', 'OTUTWO']].apply(clean_join, axis=1)\n",
    "\"\"\"\n",
    "# define interval \n",
    "short_part_upper = np.arange(0.01,0.06, 0.01)\n",
    "short_part_lower = short_part_upper - 0.01\n",
    "longer_part_upper = np.arange(0.05,0.45, 0.05)\n",
    "longer_part_lower = longer_part_upper - 0.05\n",
    "upper_part = np.hstack((short_part_upper, longer_part_upper))\n",
    "lower_part = np.hstack((short_part_lower, longer_part_lower))\n",
    "\n",
    "for up_lim, low_lim in zip(upper_part, lower_part):\n",
    "    top_chop = melted_dists.Distance < up_lim\n",
    "    low_rows = melted_dists.Distance >= low_lim\n",
    "    dist_bin = melted_dists[top_chop & low_rows]\n",
    "    print(\"OTUS separated by between {} and {} units in phylogenetic space: {}\".format(low_lim, up_lim, \n",
    "                                                                                       dist_bin.shape))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-talk')\n",
    "\n",
    "mdata_clean = meta_df_x.loc[]\n",
    "print(select_metadata)\n",
    "select_mdata = select_metadata + ['Latitude', 'Longitude', 'RawCount_b', 'TrimCount_b', 'Cluster']\n",
    "\n",
    "metas_ls = {'encoded':{}, 'raw':{}, 'encoding':{}, 'rev_coding':{}}\n",
    "for sm in select_mdata:\n",
    "    nct = meta_df_x[sm].isnull().sum()\n",
    "    metas_ls['raw'][sm] = meta_df_x[sm].tolist()\n",
    "    metas_ls['encoding'][sm] = {raw:code for code, raw in enumerate(sorted(set(metas_ls['raw'][sm])))}\n",
    "    metas_ls['encoded'][sm] = [metas_ls['encoding'][sm][r] for r in metas_ls['raw'][sm]]\n",
    "    metas_ls['rev_coding'][sm] = {code:raw for raw, code in metas_ls['encoding'][sm].items()}\n",
    "    uct = len(metas_ls['encoding'][sm])\n",
    "\n",
    "title_i = 'WeightedUnifracDists'\n",
    "\n",
    "plt.clf()\n",
    "fig, ax_i = plt.subplots(nrows=1, ncols=1, figsize=(12,12), dpi=250)\n",
    "ax_i.set_title(\"{}, colored by sequencing run\".format(title_i))\n",
    "ticks_, labels_ = zip(*metas_ls['rev_coding']['sequencing_ID'].items())\n",
    "cmap_i = plt.cm.get_cmap('Spectral', len(labels_))\n",
    "im = ax_i.scatter(sample_loadings.iloc[:, 0], \n",
    "                  sample_loadings.iloc[:, 1], \n",
    "                  c=metas_ls['encoded']['sequencing_ID'], \n",
    "                  edgecolor='k', alpha=0.8, cmap=cmap_i)\n",
    "cbar = fig.colorbar(im, ticks=ticks_)\n",
    "cbar.ax.set_yticklabels(labels_)     \n",
    "ax_i.set_xlabel(sample_loadings.columns[0])\n",
    "ax_i.set_ylabel(sample_loadings.columns[1])\n",
    "ax_i.set_facecolor('0.6')\n",
    "ax_i.set_axisbelow(True)\n",
    "ax_i.minorticks_on()\n",
    "ax_i.grid(which='major', linestyle='-', linewidth='0.5', color='gray')\n",
    "ax_i.grid(which='minor', linestyle='-', linewidth='0.25', color='black')\n",
    "#    texts = []\n",
    "#    from adjustText import adjust_text\n",
    "#    for ctrl_ in control_libs:\n",
    "#        x = sample_loadings[title_i].loc[ctrl_, 'PC1']\n",
    "#        y = sample_loadings[title_i].loc[ctrl_, 'PC2']\n",
    "#        s = meta_data_df2.loc[ctrl_, 'Short sample name'] + \"_\" + sid_map[meta_data_df2.loc[ctrl_, 'sequencing ID']]\n",
    "#        texts.append(plt.text(x, y, s))       \n",
    "#    adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\n",
    "fig.subplots_adjust(right=0.8)\n",
    "figname = \"PCA_{}_abthresh_seq_run_colors.png\".format(title_i)\n",
    "figpath = os.path.join(\"../otu_data/pca_plots\", figname)\n",
    "print(\"Saving {}\".format(figname))\n",
    "plt.savefig(figpath)\n",
    "plt.clf()\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "# calculate cluster sums \n",
    "\n",
    "temp_amdf = abund_md_df_clust.reset_index()\n",
    "temp_amdf['Cluster Label'] = temp_amdf['Samples'].map(rev_cc_mapping)\n",
    "ab_md_df_dr_cl = temp_amdf.set_index('Samples')\n",
    "cols_to_group = list(rare_abund.columns)+['Cluster Label']\n",
    "ab_df_dr_cl = ab_md_df_dr_cl.loc[:, cols_to_group]\n",
    "cluster_sums = ab_df_dr_cl.groupby('Cluster Label').agg(np.sum).sort_index()\n",
    "print(cluster_sums.sum(1))\n",
    "\n",
    "def taxa_breakdown(abunds_, taxas_, level_, weighted=True, flatten_val=0.0):\n",
    "    # 'Kingdom', 'Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species'\n",
    "    # remove non-existant features\n",
    "    flip_abunds = abunds_.loc[:, abunds_.sum(0) > 0].T\n",
    "    # create presence or absence table if need be\n",
    "    if not weighted:\n",
    "        flip_abunds = (flip_abunds > 0).astype(int)\n",
    "    # add level column\n",
    "    otu_fetch = lambda x: taxas_.loc[x, level_]\n",
    "    flip_abunds['otu_name'] = flip_abunds.index\n",
    "    flip_abunds['taxa_name'] = flip_abunds['otu_name'].apply(otu_fetch)\n",
    "    flip_abunds.drop('otu_name', axis=1, inplace=True)\n",
    "    ttable_raw = flip_abunds.groupby('taxa_name').agg(np.sum)\n",
    "    ttable = ttable_raw.div(ttable_raw.sum(0))\n",
    "    if flatten_val:\n",
    "        flat_ttv = ttable.values\n",
    "        flat_ttv[flat_ttv < flatten_val] = 0.0\n",
    "        ttable = pd.DataFrame(flat_ttv, index=ttable.index, columns=ttable.columns)\n",
    "    return ttable.T\n",
    "\n",
    "abunds_1 = cluster_sums.copy()\n",
    "taxas_1 = taxa_df.copy().astype(str)\n",
    "for level_1 in ['Class', 'Order']:\n",
    "    ttable_1 = taxa_breakdown(abunds_1, taxas_1, level_1, weighted=True, flatten_val=0.01)\n",
    "    print(ttable_1.shape)\n",
    "    ttable_1 = ttable_1.loc[:, ttable_1.columns[ttable_1.sum() > 0]]\n",
    "    print(\"The collapsed taxa table is {}\".format(ttable_1.shape))\n",
    "\n",
    "    col_order = ttable_1.max().sort_values(ascending=False).index\n",
    "    ttable_1 = ttable_1.loc[:, col_order]\n",
    "\n",
    "    fignamet = \"TaxaClusters_{}.png\".format(level_1)\n",
    "    figpatht = os.path.join(\"../otu_data/pca_plots\", fignamet)\n",
    "\n",
    "    plt.clf(); plt.close();\n",
    "    fig_width = 8\n",
    "    fig_t = plt.figure(figsize=(fig_width,10), dpi=140)\n",
    "    gs = gridspec.GridSpec(2, 1, figure=fig_t, height_ratios=[7,4], hspace=.2,\n",
    "                           bottom=0.075, top=0.925, right=0.925, left=0.075)\n",
    "    ax_arr = [plt.subplot(gs[0,0])]\n",
    "\n",
    "    possible_colors = [j for i, j in sns.xkcd_rgb.items() if not 'white' in i]\n",
    "    np.random.seed(2)\n",
    "    colors_needed = np.random.choice(possible_colors, size=ttable_1.columns.shape)\n",
    "    print(\"{} colors grabbed\".format(len(colors_needed)))\n",
    "\n",
    "    # loop over each table to plot\n",
    "    axis_titles = ['Aggregate Cluster Taxonomy']\n",
    "    for ax_ix, table_x in enumerate([ttable_1]):\n",
    "        # pick an axis\n",
    "        ax_i = ax_arr[ax_ix]\n",
    "        ax_i.set_title(axis_titles[ax_ix])\n",
    "        # set the width of each bar to the number of samples\n",
    "        adjusted_width = (fig_width / table_x.shape[0])*(.8)\n",
    "        # set the left bottom anchor of each bar\n",
    "        bar_locs = np.arange(table_x.shape[0])*(fig_width / table_x.shape[0])\n",
    "        # set the bar labels \n",
    "        bar_names = table_x.index\n",
    "        # loop over each taxon name\n",
    "        for bar_n, bar_col in enumerate(table_x.columns):\n",
    "            # subset those fractions across samples\n",
    "            bar_x = table_x.loc[:, bar_col]\n",
    "            # set the y-axis location for each bar\n",
    "            if bar_n == 0:\n",
    "                running_base = bar_x*0.0\n",
    "            # Create an individual bar\n",
    "            ax_i.bar(bar_locs, bar_x.values, bottom=running_base.values, \n",
    "                     color=colors_needed[bar_n], edgecolor='white', \n",
    "                     width=adjusted_width, tick_label=bar_names)\n",
    "            for tick in ax_i.get_xticklabels():\n",
    "                tick.set_rotation(45)\n",
    "            # increment the bottoms\n",
    "            running_base = running_base + bar_x\n",
    "\n",
    "    ax2 = plt.subplot(gs[1,0])\n",
    "    patches = [mpatches.Patch(color=color, label=label) for label, color in zip(list(ttable_1.columns), colors_needed)]\n",
    "    ax2.legend(patches, list(ttable_1.columns), loc='best', bbox_to_anchor=(0., 0., 1., 1.),\n",
    "               mode='expand', fontsize='x-small', ncol=3)\n",
    "\n",
    "    ax2.axis('off')\n",
    "    # Show graphic\n",
    "    plt.show()\n",
    "    print(\"Saving {}\".format(fignamet))\n",
    "    fig_t.savefig(figpatht, dpi=140)\n",
    "    plt.show();\n",
    "    plt.clf(); plt.close();\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where we write out the rarefied matrix and calculate (if not done) and write out alpha diversity stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abund_md_df_clust['Cluster Label'] = ab_md_df_dr_cl.loc[abund_md_df_clust.index, 'Cluster Label'].copy()\n",
    "\n",
    "cols_to_check = ['StationName', 'Month_Year', 'DepthName', 'Month', \n",
    "                 'Year', 'Salinity_Group', 'sequencing_ID']\n",
    "for clust in abund_md_df_clust['Cluster Label'].unique():\n",
    "    subdf = abund_md_df_clust[abund_md_df_clust['Cluster Label'] == clust]\n",
    "    subdf_alpha = subdf.loc[:, ['faith_pd', 'observed_otus', 'enspie']]\n",
    "    pct_fxn = lambda x: np.percentile(x, [0, 2.5, 50, 97.5, 100])\n",
    "    alpha_stats = subdf_alpha.apply(pct_fxn)\n",
    "    num_samps = subdf.shape[0]\n",
    "    print(clust, num_samps)\n",
    "    print(alpha_stats)\n",
    "    for col_s in cols_to_check:\n",
    "        n_x, x_x = np.unique(subdf[col_s].values, return_counts=1)\n",
    "        print(col_s, len(n_x))\n",
    "        for n, x in zip(n_x, x_x):\n",
    "            print(\"\\t{}: {:.2%}\".format(n, x/num_samps))        \n",
    "        input()\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPARCC Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparcc_dir = \"../otu_data/sparcc_data\"\n",
    "sparcc_file = os.path.join(sparcc_dir, \"filtered_otu_table.txt\")\n",
    "if os.path.exists(sparcc_file):\n",
    "    just_abunds = pd.read_csv(sparcc_file, sep=\"\\t\", index_col=0).T\n",
    "else:\n",
    "    just_abunds = abund_md_df_derep.loc[:, abund_df_og_s1.columns].T\n",
    "    just_abunds.to_csv(sparcc_file, sep=\"\\t\", index_label='OTU_id')\n",
    "\n",
    "print(just_abunds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPARCC In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "corrs_file = \"/Volumes/KeithSSD/CB_V4/otu_data/sparcc_data/sparcc_corr.out\"\n",
    "df = pd.read_csv(corrs_file, sep=\"\\t\", index_col=0)\n",
    "pval_file = \"/Volumes/KeithSSD/CB_V4/otu_data/sparcc_data/test_pvals.two_sided.txt\"\n",
    "p_df = pd.read_csv(pval_file, sep=\"\\t\", index_col=0)\n",
    "\n",
    "assert df.index.equals(p_df.index)\n",
    "assert df.columns.equals(p_df.columns)\n",
    "\n",
    "def melt_upper_triangle(df_, val_str):\n",
    "    dfnan = df_.where(np.triu(np.ones(df_.shape)).astype(np.bool))\n",
    "    melted_df = dfnan.stack().reset_index()\n",
    "    melted_df.columns = ['OTU_1','OTU_2', val_str]\n",
    "    melted_df2 = melted_df[melted_df['OTU_1'] != melted_df['OTU_2']]\n",
    "    return melted_df2.set_index(['OTU_1', 'OTU_2'])\n",
    "\n",
    "mpdf = melt_upper_triangle(p_df, 'p-value')\n",
    "mdf = melt_upper_triangle(df, 'correlation')\n",
    "\n",
    "fulldf = mdf.join(mpdf)\n",
    "\n",
    "# pull total abundances\n",
    "# pull taxonomy (order?)\n",
    "\n",
    "reject, pvals_corrected = multipletests(fulldf['p-value'].values, alpha=0.05, method='fdr_bh')[:2]\n",
    "\n",
    "thresholded = fulldf.loc[fulldf.index[reject], ['correlation']].reset_index()\n",
    "\"\"\"\n",
    "corr_cutoff = abs(thresholded.correlation) > 0.3\n",
    "thresholded_cutoff = thresholded[corr_cutoff]\n",
    "print(thresholded_cutoff.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# conver to relative abundance \n",
    "ja_ra = just_abunds.div(just_abunds.sum(1), axis=0)\n",
    "\n",
    "thresh_plus_scores = thresholded_cutoff.copy()\n",
    "# pool by cluster i.e. sum rows and divide by number of them\n",
    "clust_ra = pd.DataFrame(index=clean_clusts.keys(), columns=ja_ra.columns)\n",
    "for clust, membs in clean_clusts.items():\n",
    "    n_mems = len(membs)\n",
    "    clust_row = ja_ra.loc[membs, :].sum() / n_mems\n",
    "    clust_ra.loc[clust, :] = clust_row\n",
    "    pool_clust = lambda x: clust_ra.loc[clust, [x[0], x[1]]].sum()\n",
    "    colname = \"clust_{}_score\".format(clust)\n",
    "    thresh_plus_scores[colname] = thresh_plus_scores.loc[:, ['OTU_1', 'OTU_2']].apply(pool_clust, axis=1)\n",
    "    print(colname, 'done')\n",
    "    \n",
    "order_otu_one = thresh_plus_scores.OTU_1.apply(lambda x: taxa_df.loc[x, 'Phylum'])\n",
    "order_otu_two = thresh_plus_scores.OTU_2.apply(lambda x: taxa_df.loc[x, 'Phylum'])\n",
    "order_otu_two.name = \"OTU_2_Taxonomy\"\n",
    "order_otu_one.name = \"OTU_1_Taxonomy\"\n",
    "thresh_cut_taxa = thresh_plus_scores.join(order_otu_one).join(order_otu_two)\n",
    "thresh_cut_taxa.to_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/sparcc_data/corr_net.txt\", sep=\"\\t\", index=False)\n",
    "thresh_cut_taxa.head()\n",
    "\"\"\"\n",
    "ca_colnorm = clust_ra.div(clust_ra.sum(0), axis=1)\n",
    "\n",
    "print(\"Top {} OTU Intersections\")\n",
    "for clust1 in clean_clusts.keys():\n",
    "    c1_row = ca_colnorm.loc[clust1, :].sort_values(ascending=False)\n",
    "    c1_anchors = set(c1_row[c1_row == 1].index)\n",
    "    in_first_col = thresholded_cutoff.OTU_1.isin(c1_anchors)\n",
    "    in_second_col = thresholded_cutoff.OTU_2.isin(c1_anchors)\n",
    "    retained_rows = thresholded_cutoff[in_first_col | in_second_col]\n",
    "    retained_pct = retained_rows.shape[0] / thresh_cut_taxa.shape[0]\n",
    "    print(\"{} has {} anchors which retain {}/{:.2%} of edges in graph\".format(clust1, len(c1_anchors),\n",
    "                                                                              retained_rows.shape[0], retained_pct))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This reformats the guppy distance matrix into something usable and symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_file = \"../otu_data/tree_data/full_tree/query_cmsearched.hug_tol.clean.align.dist.tab\"\n",
    "\n",
    "triang_arr = [[0]]\n",
    "rec_n = 1\n",
    "with open(dist_file, \"r\") as dih:\n",
    "    for ix, l in enumerate(dih):\n",
    "        rec_n += 1\n",
    "        triang_arr.append(l.replace(\"S\", \"\").replace(\"P\", \"\").split()+[0.])\n",
    "        \n",
    "\n",
    "full_arr = np.array([x+[0.0]*(rec_n-len(x)) for x in triang_arr], dtype=float)\n",
    "dist_df = pd.DataFrame(full_arr + full_arr.T)\n",
    "dist_df.index = list(dist_df.index)\n",
    "dist_df.columns = list(dist_df.columns)\n",
    "print(dist_df.shape)\n",
    "print(dist_df.index[:5])\n",
    "print(dist_df.columns[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will read in pplacer file and collapse the abundance table according to edge placements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in the pplacer placements data\n",
    "pplacements = '../otu_data/tree_data/full_tree/query_cmsearched.hug_tol.clean.align.csv'\n",
    "pp_df = pd.read_csv(pplacements, index_col=1)\n",
    "\n",
    "# calculate likelihood statistics per edge\n",
    "binned_lwr = pp_df.loc[:, ['edge_num', 'like_weight_ratio']].groupby('edge_num').agg(['mean', 'std'])\n",
    "\n",
    "# remove really shitty edges (high std, low likelihood)\n",
    "rough_edges = set()\n",
    "rough_edges.update(binned_lwr[binned_lwr.loc[:, binned_lwr.columns[1]] > 0.3].index)\n",
    "rough_edges.update(binned_lwr[binned_lwr.loc[:, binned_lwr.columns[0]] < 0.2].index)\n",
    "\n",
    "# create a map and dataframe to for pooling placements by edge\n",
    "idx_bools = {idx:pp_df[pp_df.edge_num.isin([idx])].index for idx in sorted(pp_df.edge_num.unique())}\n",
    "pooled_pp_df = pd.DataFrame(index=abund_df_jm.index, columns=sorted(pp_df.edge_num.unique())).fillna(0.)\n",
    "for idx_, otus_ in idx_bools.items():\n",
    "    pooled_pp_df.loc[:, idx_] = abund_df_jm.loc[:, otus_].sum(1)\n",
    "\n",
    "# ensure the sumes of the original and pooled tables are identical\n",
    "assert pooled_pp_df.sum().sum() - abund_df_jm.sum().sum() == 0\n",
    "print(\"After clustering {} features reduced to {}\".format(abund_df_jm.shape[1], pooled_pp_df.shape[1]))\n",
    "\n",
    "# remove controls and low abundance edges and low yield samples\n",
    "pooled_pp_ns = decrease_sparsity(pooled_pp_df.copy(), control_libs, addl_keys=['Zymo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check how tight our pools are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count)\n",
    "\n",
    "# filter out edges in rough_edges and those without relatively fine scale taxonomic classification \n",
    "good_edges = {}\n",
    "for edge in pooled_pp_ns.columns:\n",
    "    members = idx_bools[edge]\n",
    "    if not edge in rough_edges:\n",
    "        taxa_rows = taxa_df.loc[members, :]\n",
    "        if taxa_rows.shape[0] > 2:\n",
    "            classified_ = taxa_rows.isnull().sum() / taxa_rows.shape[0] < 0.5\n",
    "            if classified_[classified_].shape[0] > 0:\n",
    "                lowest_level = classified_[classified_].index[-1]\n",
    "                if not lowest_level in ['Kingdom', 'Phylum',]:\n",
    "                    good_edges[edge] = [most_frequent(taxa_rows.loc[:, i].tolist()) for i in taxa_df.columns]\n",
    "        else:\n",
    "            good_edges[edge] = [most_frequent(taxa_rows.loc[:, i].tolist()) for i in taxa_df.columns]\n",
    "\n",
    "# this is some code to check intra- and inter- distances between taxonomic classes\n",
    "edge_taxa = pd.DataFrame(good_edges, index=taxa_df.columns).T\n",
    "for level in taxa_df.columns:\n",
    "    class_names, class_counts = np.unique(edge_taxa[level].dropna().values, return_counts=1)\n",
    "    common_classes = list(class_names[class_counts > 4])\n",
    "    max_combos = int((len(common_classes)*(len(common_classes)-1))/2)\n",
    "    print(\"{} has {} common classes (comparisons = {})\".format(level, len(common_classes), max_combos))\n",
    "    check_counter = 0 \n",
    "    for mc in range(max_combos):\n",
    "        np.random.shuffle(common_classes)\n",
    "        c_1 = common_classes[0]\n",
    "        c_2 = common_classes[1]\n",
    "        print(\"\\tChecking {} and {}\".format(c_1, c_2))\n",
    "        check_counter += 1\n",
    "        for iter_cnt in range(10):\n",
    "            np.random.seed(iter_cnt*50)\n",
    "            edges_c1 = edge_taxa[edge_taxa[level] == c_1]\n",
    "            edges_c2 = edge_taxa[edge_taxa[level] == c_2]\n",
    "            edge_nums1 = np.random.choice(edges_c1.index, size=(4,), replace=False)\n",
    "            edge_nums2 = np.random.choice(edges_c2.index, size=(4,), replace=False)\n",
    "            cross_dist = dist_df.loc[edge_nums1, edge_nums2].mean().mean()\n",
    "            intra_c1_dist = dist_df.loc[edge_nums1, edge_nums1].mean().mean()\n",
    "            intra_c2_dist = dist_df.loc[edge_nums2, edge_nums2].mean().mean()\n",
    "            assert cross_dist > intra_c1_dist\n",
    "            assert cross_dist > intra_c2_dist\n",
    "        if check_counter > 5:\n",
    "            break\n",
    "\n",
    "# these are what we need to calculate effect sizes on \n",
    "sub_dists = dist_df.loc[edge_taxa.index, edge_taxa.index]\n",
    "sub_pooled_pp_ns = pooled_pp_ns.loc[:, edge_taxa.index]\n",
    "\n",
    "edge_names = {x:\"Edge{}\".format(x) for x in list(edge_taxa.index)}\n",
    "\n",
    "to_write_dists = sub_dists.rename(index=edge_names, columns=edge_names)\n",
    "to_write_abunds = rare_pp.rename(columns=edge_names)\n",
    "\n",
    "to_write_dists.to_csv(\"../otu_data/clustered_sequences/fixed_pplacer_distmat.tsv\", sep=\"\\t\")\n",
    "to_write_abunds.to_csv(\"../otu_data/clustered_sequences/pplacer_abundances.tsv\", sep=\"\\t\")\n",
    "\n",
    "meta_data_df.loc[sub_pooled_pp_ns.index, ['CollectionAgency']].to_csv(\"../otu_data/clustered_sequences/strata.tsv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "from skbio.stats.distance import permanova\n",
    "from skbio.stats.distance import permdisp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata and filter out OTUs w/ >50% total abundance in blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check effect of rarefaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_df = pd.concat([alpha_df, meta_df_nocodes.loc[alpha_df.index, :]], axis=1)\n",
    "super_exp = super_df[(super_df.station != 'LAB') & (super_df.depth != 'Control') & (super_df.depth != 'nan')]\n",
    "super_exp.loc[super_exp.depth == 'Surface', 'depth'] = '01'\n",
    "super_exp.depth = super_exp.depth.apply(lambda x: \"0\"+x if len(x) == 1 else x)\n",
    "super_sorted = super_exp.sort_values(['year', 'month', 'lat', 'depth' ], ascending=[True, True, False, True])\n",
    "super_sorted['month_year'] = super_sorted.loc[:, ['month', 'year']].apply(lambda x: \" \".join(x), axis=1)\n",
    "super_sorted['salinity_group'] = pd.Series([\"\"]*super_sorted.index.shape[0], index=super_sorted.index)\n",
    "\n",
    "oos_before = abund_df.apply(observed_otus, axis=1) \n",
    "oos_after = rare_abund.apply(observed_otus, axis=1)\n",
    "print(\"Spearman correleations between trimmed read count and observed otus before rarefaction\")\n",
    "print(spearmanr(oos_before.loc[super_sorted.index].values, super_sorted.TrimCount.values))\n",
    "print(\"Spearman correleations between trimmed read count and observed otus after rarefaction\")\n",
    "print(spearmanr(oos_after.loc[super_sorted.index].values, super_sorted.TrimCount.values))\n",
    "print(\"Spearman correleations between trimmed read count and ENSPIE after rarefaction\")\n",
    "print(spearmanr(super_sorted.enspie.values, super_sorted.TrimCount.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract interesting subclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_ = hierarchy.fclusterdata(projected, c_height, criterion='distance', method='ward', metric='euclidean')\n",
    "clust_cols = ['nMonths', 'nYears', 'nStations', 'nDates', 'nPyc']\n",
    "clust_cols += ['nMonths_noCB33', 'nYears_noCB33', 'nStations_noCB33', 'nDates_noCB33', 'nPyc_noCB33']\n",
    "\n",
    "cluster_stats = pd.DataFrame(index=list(set(list(clustered_))), columns = clust_cols)\n",
    "\n",
    "bottom_only = ['CB22', 'CB31', 'CB32', 'CB52', 'CB51', 'CB43C', 'CB42C', 'CB41C']\n",
    "\n",
    "for x in set(list(clustered_)):\n",
    "    mixed_clust = super_sorted.index[clustered_ == x]\n",
    "    subdf = super_sorted.loc[mixed_clust, :]\n",
    "    num_samps = mixed_clust.shape[0]\n",
    "    print(\"Cluster {} with {} samps\".format(x, num_samps))\n",
    "    print(subdf.enspie.mean(), subdf.enspie.std())\n",
    "    print(subdf.loc[subdf.station.isin(['CB71','CB72', 'CB63', 'CB61', 'CB62', 'CB22']), ['station', 'month_year']])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for significant covariates that obey homogeniety of variance assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_abund2 = rare_abund.loc[super_sorted.index, rare_abund.columns[rare_abund.sum(0) > 0]]\n",
    "exp_abunds2 = exp_abunds.loc[:, exp_abunds.columns[exp_abunds.sum(0) > 0]]\n",
    "print(rare_abund2.shape, exp_abunds2.shape)\n",
    "\n",
    "stat_cols = ['subset_var', 'effect_type', 'nclasses', 'nsamples', 'permanovaF', 'permanovaP', 'dispF', \"dispP\"]\n",
    "results_df = pd.DataFrame(index=range(1000), columns=stat_cols)\n",
    "    \n",
    "for a_df, a_label in zip([exp_abunds2], ['no_rare']):\n",
    "    rclr_mat = rclr().fit_transform(a_df.values)\n",
    "    U, s, V = OptSpace().fit_transform(rclr_mat)\n",
    "    dist_df = pd.DataFrame(index=a_df.index, columns=a_df.index, data=cdist(U,U))\n",
    "    counter = 0\n",
    "    for stat in super_sorted.station.unique():\n",
    "        sub_setter = super_sorted.station == stat\n",
    "        sub_dists = dist_df.loc[sub_setter, sub_setter]\n",
    "        skb_sub_dists = DistanceMatrix(sub_dists.values)\n",
    "        for m_type in ['month_year', 'month', 'year', 'pycnocline']:\n",
    "            mdata = list(super_sorted.loc[sub_setter, m_type].values)\n",
    "            print(\"Checking for {} effects within {}: {} unique classes\".format(stat, m_type, len(set(mdata))))\n",
    "            if len(set(mdata)) > 1:\n",
    "                anova_test = permanova(skb_sub_dists, mdata, permutations=999)\n",
    "                disp_test = permdisp(skb_sub_dists, mdata, permutations=999)\n",
    "                results_df.loc[counter, 'subset_var'] = stat\n",
    "                results_df.loc[counter, 'effect_type'] = m_type\n",
    "                results_df.loc[counter, 'nclasses'] = disp_test['number of groups']\n",
    "                results_df.loc[counter, 'nsamples'] = disp_test['sample size']\n",
    "                results_df.loc[counter, 'permanovaF'] = anova_test['test statistic']\n",
    "                results_df.loc[counter, 'permanovaP'] = anova_test['p-value']\n",
    "                results_df.loc[counter, 'dispF'] = disp_test['test statistic']\n",
    "                results_df.loc[counter, 'dispP'] = disp_test['p-value']\n",
    "                counter += 1\n",
    "                print(\"Counter moving to {}\".format(counter))\n",
    "            else:\n",
    "                print(\"\\t..... skipping\")\n",
    "    \n",
    "    bool1 = super_sorted.station != 'CB33C'\n",
    "    for b2_val in [['2016'], ['2017'], ['2016', '2017']]:\n",
    "        bool2 = super_sorted.year.isin(b2_val)\n",
    "        yearly_transects = DistanceMatrix(dist_df.loc[(bool1 & bool2), (bool1 & bool2)].values)\n",
    "        mdata = list(super_sorted.loc[(bool1 & bool2), 'station'].values)\n",
    "        anova_test = permanova(yearly_transects, mdata, permutations=999)\n",
    "        disp_test = permdisp(yearly_transects, mdata, permutations=999)\n",
    "        results_df.loc[counter, 'subset_var'] = '{}_transect'.format(\"_\".join(b2_val))\n",
    "        results_df.loc[counter, 'effect_type'] = 'Station'\n",
    "        results_df.loc[counter, 'nclasses'] = disp_test['number of groups']\n",
    "        results_df.loc[counter, 'nsamples'] = disp_test['sample size']\n",
    "        results_df.loc[counter, 'permanovaF'] = anova_test['test statistic']\n",
    "        results_df.loc[counter, 'permanovaP'] = anova_test['p-value']\n",
    "        results_df.loc[counter, 'dispF'] = disp_test['test statistic']\n",
    "        results_df.loc[counter, 'dispP'] = disp_test['p-value']\n",
    "        counter += 1\n",
    "        print(\"Counter moving to {}\".format(counter))\n",
    "\n",
    "    for a_stat in super_sorted.station.unique():\n",
    "        bool1 = super_sorted.station == a_stat\n",
    "        for b2_val in ['Above', 'Below']:\n",
    "            bool2 = super_sorted.pycnocline == b2_val\n",
    "            for effect_var in ['month_year', 'month', 'year']:\n",
    "                mdata = list(super_sorted.loc[(bool1 & bool2), effect_var].values)\n",
    "                print(\"Checking for {} effects within {}: {} unique classes\".format(stat, m_type, len(set(mdata))))\n",
    "                if len(set(mdata)) > 1:\n",
    "                    skb_sub_dists = DistanceMatrix(dist_df.loc[(bool1 & bool2), (bool1 & bool2)].values)\n",
    "                    anova_test = permanova(skb_sub_dists, mdata, permutations=999)\n",
    "                    disp_test = permdisp(skb_sub_dists, mdata, permutations=999)\n",
    "                    results_df.loc[counter, 'subset_var'] = a_stat+'_'+b2_val\n",
    "                    results_df.loc[counter, 'effect_type'] = effect_var\n",
    "                    results_df.loc[counter, 'nclasses'] = disp_test['number of groups']\n",
    "                    results_df.loc[counter, 'nsamples'] = disp_test['sample size']\n",
    "                    results_df.loc[counter, 'permanovaF'] = anova_test['test statistic']\n",
    "                    results_df.loc[counter, 'permanovaP'] = anova_test['p-value']\n",
    "                    results_df.loc[counter, 'dispF'] = disp_test['test statistic']\n",
    "                    results_df.loc[counter, 'dispP'] = disp_test['p-value']\n",
    "                    print(\"Counter moving to {}\".format(counter))\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    print(\"\\t..... skipping\")\n",
    "                        \n",
    "\n",
    "    results_df['Real Effect'] = (results_df.permanovaP < 0.05) & (results_df.dispP > 0.05)\n",
    "    results_df['Confounded Effects'] = (results_df.permanovaP < 0.05) & (results_df.dispP < 0.05)\n",
    "    results_df['No Effects'] = (results_df.permanovaP > 0.05) & (results_df.dispP > 0.05)\n",
    "    results_df.sort_values(by=['Real Effect', 'Confounded Effects', 'No Effects'], ascending=False, inplace=True)\n",
    "    results_df[results_df.subset_var.notnull()].to_csv('../data/otu_data_pca/hypothesis_testing_{}.tsv'.format(a_label), sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_f = \"../data/TrimOTUsData/taxa_table.tsv\"\n",
    "taxa_df = pd.read_csv(tax_f, sep=\"\\t\")\n",
    "OTU_Seqs = {taxa_df.loc[idx, taxa_df.columns[0]]:idx for idx in taxa_df.index}\n",
    "OTU_Names = {idx:\"OTU{}\".format(idx+1) for idx in taxa_df.index }\n",
    "OTU_name2seq = {OTU_Names[num]:seq for seq, num in OTU_Seqs.items()}\n",
    "taxa_df.loc[:, taxa_df.columns[0]] = taxa_df.loc[:, taxa_df.columns[0]].apply(lambda x: OTU_Names[OTU_Seqs[x]])\n",
    "taxa_df = taxa_df.set_index(taxa_df.columns[0])\n",
    "\n",
    "assert str(taxa_df_light.iloc[0, -1]) == 'nan'\n",
    "taxa_df_light = taxa_df.loc[abund_df.columns, :]\n",
    "taxa_df_ln = taxa_df_light.replace(taxa_df_light.iloc[0, -1], \"\")\n",
    "for c in taxa_df_ln.columns:\n",
    "    taxa_df_ln[c] = taxa_df_ln[c].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_genera = {}\n",
    "with open(\"../data/Problem_Taxa.txt\", 'r') as pt_fh:\n",
    "    for l in pt_fh:\n",
    "        pbg = l.split()[0].lower()\n",
    "        if pbg in problem_genera.keys():\n",
    "            pass\n",
    "        else:\n",
    "            flagged_idxs = set()\n",
    "            if pbg != 'candida':\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Family.str.contains(pbg)].index)\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Genus.str.contains(pbg)].index)\n",
    "            else:\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Family.str.contains(pbg) & ~taxa_df_ln.Family.str.contains('candidatus')].index)\n",
    "                flagged_idxs.update(taxa_df_ln[taxa_df_ln.Genus.str.contains(pbg) & ~taxa_df_ln.Genus.str.contains('candidatus')].index)\n",
    "            \n",
    "            if len(flagged_idxs) > 0:\n",
    "                problem_genera[pbg] = list(flagged_idxs)\n",
    "                print(\"{}: {} unique types\".format(len(flagged_idxs), pbg))\n",
    "                print(taxa_df_ln.loc[problem_genera[pbg], :].drop_duplicates())\n",
    "                input()\n",
    "\n",
    "\n",
    "taxa_df_fgs = taxa_df_ln.loc[:, ['Family', \"Genus\", \"Species\"]].apply(tuple, axis=1)\n",
    "\n",
    "med_detected = [(\"neisseriaceae\", 'neisseria', \"\"),\n",
    "                (\"pseudomonadaceae\", 'pseudomonas', \"\"), \n",
    "                (\"spirochaetaceae\", \"treponema\", \"\"), \n",
    "                (\"spirochaetaceae\", \"treponema_2\", \"\"),\n",
    "                (\"rickettsiaceae\", \"rickettsia\", \"\"), \n",
    "                (\"rickettsiaceae\", \"rickettsia\", \"typhi\"), \n",
    "                (\"leptospiraceae\", \"leptospira\", \"\"),\n",
    "                (\"legionellaceae\", \"legionella\", \"\"),\n",
    "                (\"legionellaceae\", \"legionella\", \"steelei\"),\n",
    "                (\"francisellaceae\", \"francisella\", \"\"),\n",
    "                (\"pasteurellaceae\", \"haemophilus\", \"\"),\n",
    "                (\"parachlamydiaceae\", \"parachlamydia\", \"acanthamoebae\"),\n",
    "                (\"aeromonadaceae\", \"aeromonas\", \"\"),\n",
    "                (\"coxiellaceae\", \"coxiella\", \"cheraxi\"),\n",
    "                (\"coxiellaceae\", \"coxiella\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"yersinia\", \"\"),\n",
    "                (\"vibrionaceae\", \"vibrio\", \"\"),\n",
    "                (\"peptostreptococcaceae\", \"peptoclostridium\", \"\"),\n",
    "                (\"peptostreptococcaceae\", \"paeniclostridium\", \"\"),\n",
    "                (\"clostridiaceae_2\", \"clostridium_sensu_stricto\", \"\"),\n",
    "                (\"clostridiaceae_1\", \"clostridium_sensu_stricto_3\", \"intestinale\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"vulgatus\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"uniformis\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"coprosuis\"),\n",
    "                (\"bacteroidaceae\", \"bacteroides\", \"massiliensis\"),\n",
    "                (\"campylobacteraceae\", \"campylobacter\", \"\"),\n",
    "                (\"listeriaceae\", \"listeria\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"escherichia/shigella\", \"\"),\n",
    "                (\"mycobacteriaceae\", \"mycobacteriummycobacteriaceae\", \"\"),\n",
    "                (\"moraxellaceae\", \"acinetobacter\", \"\"),\n",
    "                (\"streptococcaceae\", \"streptococcus\", \"mutans\"),\n",
    "                (\"streptococcaceae\", \"streptococcus\", \"\"),\n",
    "                (\"peptostreptococcaceae\", \"peptostreptococcus\", \"\"),\n",
    "                (\"enterococcaceae\", \"enterococcus\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"serratia\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"klebsiella\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"salmonella\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"citrobacter\", \"\"),\n",
    "                (\"enterobacteriaceae\", \"pantoea\", \"\"),\n",
    "                (\"lactobacillaceae\", \"lactobacillus\", \"delbrueckii\"),\n",
    "                (\"lactobacillaceae\", \"lactobacillus\", \"kitasatonis\"),\n",
    "                (\"staphylococcaceae\", \"staphylococcus\", \"haemolyticus\"),\n",
    "                (\"staphylococcaceae\", \"staphylococcus\", \"\"),\n",
    "                (\"bifidobacteriaceae\",  \"bifidobacterium\", \"bifidum\"),\n",
    "                (\"bifidobacteriaceae\",  \"bifidobacterium\", \"\")]\n",
    "\n",
    "hab_detected = [(\"nostocaceae\", \"aphanizomenon_mdt14a\", \"\"),\n",
    "                (\"cyanobiaceae\", \"cyanobium_pcc-6307\", \"\"),\n",
    "                (\"nostocaceae\", \"cylindrospermum_pcc-7417\", \"\"),\n",
    "                (\"nostocaceae\", \"dolichospermum_nies41\", \"\"),\n",
    "                (\"limnotrichaceae\", \"limnothrix\", \"\"),\n",
    "                (\"microcystaceae\", \"microcystis_pcc-7914\", \"\"),\n",
    "                (\"nostocaceae\", \"nodularia_pcc-9350\", \"\"),\n",
    "                (\"nostocales_incertae_sedis\", \"phormidium_sag_81.79\", \"uncinatum\"),\n",
    "                (\"phormidiaceae\", \"phormidium_iam_m-71\", \"\"),\n",
    "                (\"phormidiaceae\", \"planktothrix_niva-cya_15\", \"\"),\n",
    "                (\"microcystaceae\", \"snowella_0tu37s04\", \"\"),\n",
    "                (\"microcystaceae\",  \"microcystis_pcc-7914\", \"\"),\n",
    "                (\"microcystaceae\", \"snowella_0tu37s04\", \"litoralis\")]\n",
    "\n",
    "for i in range(1,19):\n",
    "    med_detected.append((\"clostridiaceae_1\", \"clostridium_sensu_stricto_\"+str(i), \"\"))\n",
    "\n",
    "med_idxs = taxa_df_fgs[taxa_df_fgs.isin(med_detected)].index\n",
    "hab_idxs = taxa_df_fgs[taxa_df_fgs.isin(hab_detected)].index\n",
    "\n",
    "\n",
    "print(med_idxs.shape, hab_idxs.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df1 = abund_df_c2.loc[ordered_axis, med_idxs]\n",
    "plot_df2 = abund_df_c2.loc[ordered_axis, hab_idxs]\n",
    "#sns.set(font_scale=.5)\n",
    "plt.clf(); plt.close();\n",
    "plt.style.use('seaborn-paper')\n",
    "fig_, (ax_1, ax_2) = plt.subplots(nrows=1, ncols=2, sharey='col', figsize=(60,60), dpi=180, gridspec_kw = {'width_ratios':[5.7, 1]})\n",
    "sns.heatmap(plot_df1, cmap=sns.light_palette('red', as_cmap=True), robust=True, linewidths=.5, ax=ax_1, cbar=False)\n",
    "sns.heatmap(plot_df2, cmap=sns.light_palette('green', as_cmap=True), robust=True, linewidths=.5, ax=ax_2, cbar=False)\n",
    "matplot_fn = \"../data/WaterQualityData/figures/probTaxa.png\"\n",
    "fig_.savefig(matplot_fn, dpi=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the usearch data\n",
    "c90_file = \"../otu_data/clustered_sequences/abundances.c90.tsv\"\n",
    "if not os.path.exists(c90_file):\n",
    "    cluster_mem_file = \"../otu_data/clustered_sequences/cluster_members.90.txt\"\n",
    "    clust90 = pd.read_csv(cluster_mem_file, sep=\"\\t\", header=None)\n",
    "    \n",
    "    def cluster_table(clustxx, abund_df_i):\n",
    "        c_labels = ['Cluster'+str(c_i) for c_i in clustxx[1].unique()]\n",
    "        clust_xx_df = pd.DataFrame(index=abund_df_i.index, columns=c_labels)\n",
    "        clust_dict = {}\n",
    "        for c_labs in c_labels:\n",
    "            c_int = int(c_labs[7:])\n",
    "            clust_dict[c_int] = list(set(clustxx[clustxx[1].isin([c_int])][8].values))\n",
    "            clust_xx_df.loc[:, c_labs] = abund_df_i.loc[:, abund_df_i.columns.isin(clust_dict[c_int])].sum(1)\n",
    "        \n",
    "        return (clust_xx_df, clust_dict)\n",
    "    \n",
    "    clust_90_df, clust_90_dict = cluster_table(clust90, abund_df_jm.copy())\n",
    "    print(\"After clustering {} features reduced to {}\".format(abund_df_jm.shape[1], clust_90_df.shape[1]))\n",
    "    assert clust_90_df.sum().sum() - abund_df_jm.sum().sum() == 0\n",
    "    clust_90_df.to_csv(c90_file, sep=\"\\t\")\n",
    "    print(\"Clustered file written\")\n",
    "else:\n",
    "    print(\"Reading stored UCLUST abundances\")\n",
    "    clust_90_df = pd.read_csv(c90_file, sep=\"\\t\", index_col=0)\n",
    "    print(\"Decreasing sparsity of clustered abundances\")    \n",
    "\n",
    "clust_90_ns = decrease_sparsity(clust_90_df.copy(), control_libs, addl_keys=['Zymo'])\n",
    "print(\"After clustering {} features reduced to {}\".format(abund_df_jm.shape[1], clust_90_ns.shape[1]))\n",
    "\n",
    "from skbio.stats.distance import mantel\n",
    "\n",
    "assert clust_90_ns.index.equals(abund_df_og.index)\n",
    "assert pooled_pp_ns.index.equals(abund_df_og.index)\n",
    "\n",
    "bc_dm_pp = beta_diversity(\"braycurtis\", pooled_pp_ns.values, pooled_pp_ns.index)\n",
    "bc_dm_c90 = beta_diversity(\"braycurtis\", clust_90_ns.values, clust_90_ns.index)\n",
    "bc_dm_og = beta_diversity(\"braycurtis\", abund_df_og.values, abund_df_og.index)\n",
    "\n",
    "r_pp, p_value_pp, n_pp = mantel(bc_dm_pp, bc_dm_og, method='pearson')\n",
    "r_c90, p_value_c90, n_c90 = mantel(bc_dm_pp, bc_dm_c90, method='pearson')\n",
    "print(\"Correlation of UCLUST 90% to original {} ({})\".format(r_c90, p_value_c90))\n",
    "print(\"Correlation of Pplacer edges to original {} ({})\".format(r_pp, p_value_pp))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
