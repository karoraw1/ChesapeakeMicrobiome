{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StationName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CB22</th>\n",
       "      <td>39.349</td>\n",
       "      <td>-76.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB31</th>\n",
       "      <td>39.250</td>\n",
       "      <td>-76.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB32</th>\n",
       "      <td>39.164</td>\n",
       "      <td>-76.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB33C</th>\n",
       "      <td>38.998</td>\n",
       "      <td>-76.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CB41C</th>\n",
       "      <td>38.826</td>\n",
       "      <td>-76.399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Latitude  Longitude\n",
       "StationName                     \n",
       "CB22           39.349    -76.176\n",
       "CB31           39.250    -76.240\n",
       "CB32           39.164    -76.306\n",
       "CB33C          38.998    -76.359\n",
       "CB41C          38.826    -76.399"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.ticker as mticker\n",
    "import geopy.distance\n",
    "\n",
    "xl_file = \"/Volumes/KeithSSD/CB_V4/otu_data/mixing_data/Particle_Data.xlsx\"\n",
    "\n",
    "assert os.path.exists(xl_file)\n",
    "unformatted_df1 = pd.read_excel(xl_file, sheet_name='2015')\n",
    "unformatted_df2 = pd.read_excel(xl_file, sheet_name='2016')\n",
    "\n",
    "env_data_file = \"/Volumes/KeithSSD/CB_V4/otu_data/WaterQualityData/matched_cleaned_data/all_mdata_with_habitat.txt\"\n",
    "env_data = pd.read_csv(env_data_file, sep=\"\\t\")\n",
    "stat_latlon = env_data[['StationName', 'Latitude', 'Longitude']].groupby('StationName').agg('mean').round(3)\n",
    "print(stat_latlon.shape)\n",
    "stat_latlon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep df by add columns of station lat, lon, and depth*-1 to individual particle data\n",
    "def prep_particle_chunk(sample_pt_i, sample_pts, stat_latlon, particle_data, index_range_i):\n",
    "    one_sample = sample_pts.loc[sample_pt_i, :].to_dict()\n",
    "    one_sample['StatLat'] = stat_latlon.loc[one_sample['StationName'], 'Latitude']\n",
    "    one_sample['StatLon'] = stat_latlon.loc[one_sample['StationName'], 'Longitude']\n",
    "    one_sample['DepthName'] *= -1\n",
    "    origin_cols = pd.DataFrame({p_i: one_sample for p_i in index_range_i}).T\n",
    "    one_section = particle_data.loc[index_range_i, :].copy()\n",
    "    particle_sub = pd.concat((one_section, origin_cols), axis=1, sort=1, verify_integrity=1)\n",
    "    return particle_sub.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_km</th>\n",
       "      <th>x_km</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>depth</th>\n",
       "      <th>CollectionAgency</th>\n",
       "      <th>DateMMDDYY</th>\n",
       "      <th>DepthName</th>\n",
       "      <th>Samples</th>\n",
       "      <th>StatLat</th>\n",
       "      <th>StatLon</th>\n",
       "      <th>StationName</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>907451.69</td>\n",
       "      <td>4334940.5</td>\n",
       "      <td>39.071159</td>\n",
       "      <td>-76.29114</td>\n",
       "      <td>-9.697164</td>\n",
       "      <td>Preheim</td>\n",
       "      <td>62716</td>\n",
       "      <td>-1</td>\n",
       "      <td>SB062716TAWCSCB33CD1BR2TR1I80</td>\n",
       "      <td>38.998</td>\n",
       "      <td>-76.359</td>\n",
       "      <td>CB33C</td>\n",
       "      <td>09:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>911187.13</td>\n",
       "      <td>4331585.0</td>\n",
       "      <td>39.039276</td>\n",
       "      <td>-76.25012</td>\n",
       "      <td>-0.102223</td>\n",
       "      <td>Preheim</td>\n",
       "      <td>62716</td>\n",
       "      <td>-1</td>\n",
       "      <td>SB062716TAWCSCB33CD1BR2TR1I80</td>\n",
       "      <td>38.998</td>\n",
       "      <td>-76.359</td>\n",
       "      <td>CB33C</td>\n",
       "      <td>09:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>896647.69</td>\n",
       "      <td>4324071.0</td>\n",
       "      <td>38.978493</td>\n",
       "      <td>-76.42194</td>\n",
       "      <td>-2.098320</td>\n",
       "      <td>Preheim</td>\n",
       "      <td>62716</td>\n",
       "      <td>-1</td>\n",
       "      <td>SB062716TAWCSCB33CD1BR2TR1I80</td>\n",
       "      <td>38.998</td>\n",
       "      <td>-76.359</td>\n",
       "      <td>CB33C</td>\n",
       "      <td>09:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>909035.63</td>\n",
       "      <td>4327793.5</td>\n",
       "      <td>39.006233</td>\n",
       "      <td>-76.27716</td>\n",
       "      <td>-4.127914</td>\n",
       "      <td>Preheim</td>\n",
       "      <td>62716</td>\n",
       "      <td>-1</td>\n",
       "      <td>SB062716TAWCSCB33CD1BR2TR1I80</td>\n",
       "      <td>38.998</td>\n",
       "      <td>-76.359</td>\n",
       "      <td>CB33C</td>\n",
       "      <td>09:13:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897784.50</td>\n",
       "      <td>4315252.0</td>\n",
       "      <td>38.898766</td>\n",
       "      <td>-76.41397</td>\n",
       "      <td>-6.030720</td>\n",
       "      <td>Preheim</td>\n",
       "      <td>62716</td>\n",
       "      <td>-1</td>\n",
       "      <td>SB062716TAWCSCB33CD1BR2TR1I80</td>\n",
       "      <td>38.998</td>\n",
       "      <td>-76.359</td>\n",
       "      <td>CB33C</td>\n",
       "      <td>09:13:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        y_km       x_km        lat       lon     depth CollectionAgency  \\\n",
       "1  907451.69  4334940.5  39.071159 -76.29114 -9.697164          Preheim   \n",
       "2  911187.13  4331585.0  39.039276 -76.25012 -0.102223          Preheim   \n",
       "3  896647.69  4324071.0  38.978493 -76.42194 -2.098320          Preheim   \n",
       "4  909035.63  4327793.5  39.006233 -76.27716 -4.127914          Preheim   \n",
       "5  897784.50  4315252.0  38.898766 -76.41397 -6.030720          Preheim   \n",
       "\n",
       "  DateMMDDYY DepthName                        Samples StatLat StatLon  \\\n",
       "1      62716        -1  SB062716TAWCSCB33CD1BR2TR1I80  38.998 -76.359   \n",
       "2      62716        -1  SB062716TAWCSCB33CD1BR2TR1I80  38.998 -76.359   \n",
       "3      62716        -1  SB062716TAWCSCB33CD1BR2TR1I80  38.998 -76.359   \n",
       "4      62716        -1  SB062716TAWCSCB33CD1BR2TR1I80  38.998 -76.359   \n",
       "5      62716        -1  SB062716TAWCSCB33CD1BR2TR1I80  38.998 -76.359   \n",
       "\n",
       "  StationName      Time  \n",
       "1       CB33C  09:13:00  \n",
       "2       CB33C  09:13:00  \n",
       "3       CB33C  09:13:00  \n",
       "4       CB33C  09:13:00  \n",
       "5       CB33C  09:13:00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unformatted_df = unformatted_df2.copy()\n",
    "\n",
    "# these are the station location data\n",
    "sample_pts = unformatted_df.iloc[: , :6].dropna()\n",
    "\n",
    "# pull out this data and add the right column names \n",
    "particle_data = unformatted_df.iloc[1:, 6:].dropna()\n",
    "particle_data.columns = ['y_km', 'x_km', 'lat', 'lon', 'depth']\n",
    "particle_data = particle_data.apply(pd.to_numeric, axis=1)\n",
    "\n",
    "# longitude is wrong\n",
    "particle_data['lon'] = particle_data['lon'] - 360\n",
    "\n",
    "lowest_points, subset_to_use = {}, set()\n",
    "for s in sample_pts.StationName.unique():\n",
    "    sample_sub = sample_pts.loc[sample_pts.StationName == s, :]\n",
    "    depth_sub = sample_sub.loc[sample_sub.DepthName == sample_sub.DepthName.max(), :]\n",
    "    lowest_points[s] = (list(depth_sub.index), len(depth_sub.DateMMDDYY.unique()))\n",
    "    subset_to_use.update(list(depth_sub.index))\n",
    "\n",
    "# the following 300 rows are the point locations \n",
    "index_ranges = {i:list(range(i+1,i+301)) for i in sample_pts.index}\n",
    "\n",
    "assert set([j for i in index_ranges.values() for j in i]) == set(particle_data.index)\n",
    "\n",
    "particle_chunks = {}\n",
    "for s_p, ir_i in list(index_ranges.items()):\n",
    "    particle_chunks[s_p] = prep_particle_chunk(s_p, sample_pts, stat_latlon, particle_data, ir_i)\n",
    "\n",
    "print(len(index_ranges), len(subset_to_use))\n",
    "list(particle_chunks.values())[0].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CB33C CB33C 16.60\n",
      "CB33C CB22 41.55\n",
      "CB33C CB43C 54.71\n",
      "CB33C CB44 84.60\n",
      "CB33C CB51 78.38\n",
      "CB33C CB52 123.88\n",
      "CB33C CB53 147.56\n",
      "CB33C CB54 153.12\n",
      "CB33C CB71 161.92\n",
      "CB33C CB62 181.91\n",
      "CB33C CB63 182.55\n",
      "CB33C CB72 196.63\n",
      "CB33C CB64 209.63\n",
      "CB33C CB74 215.27\n",
      "CB33C CB31 31.54\n",
      "CB33C CB32 13.33\n",
      "CB33C CB41C 38.21\n",
      "CB33C CB42C 64.74\n",
      "CB33C CB61 170.47\n",
      "CB33C CB73 220.11\n",
      "CB22 CB33C 56.42\n",
      "CB22 CB22 4.66\n",
      "CB22 CB43C 95.11\n",
      "CB22 CB44 123.65\n",
      "CB22 CB51 117.36\n",
      "CB22 CB52 161.92\n",
      "CB22 CB53 185.23\n",
      "CB22 CB54 190.65\n",
      "CB22 CB71 198.76\n",
      "CB22 CB62 219.95\n",
      "CB22 CB63 220.78\n",
      "CB22 CB72 234.23\n",
      "CB22 CB64 247.86\n",
      "CB22 CB74 251.33\n",
      "CB22 CB31 10.98\n",
      "CB22 CB32 29.19\n",
      "CB22 CB41C 79.77\n",
      "CB22 CB42C 105.05\n",
      "CB22 CB61 208.51\n",
      "CB22 CB73 257.49\n",
      "CB43C CB33C 35.55\n",
      "CB43C CB22 90.29\n",
      "CB43C CB43C 11.36\n",
      "CB43C CB44 37.55\n",
      "CB43C CB51 32.13\n",
      "CB43C CB52 76.80\n",
      "CB43C CB53 100.47\n",
      "CB43C CB54 106.11\n",
      "CB43C CB71 115.54\n",
      "CB43C CB62 134.08\n",
      "CB43C CB63 134.55\n",
      "CB43C CB72 149.11\n",
      "CB43C CB64 161.48\n",
      "CB43C CB74 169.11\n",
      "CB43C CB31 80.61\n",
      "CB43C CB32 62.43\n",
      "CB43C CB41C 12.88\n",
      "CB43C CB42C 18.49\n",
      "CB43C CB61 122.73\n",
      "CB43C CB73 172.65\n",
      "CB44 CB33C 51.33\n",
      "CB44 CB22 104.21\n",
      "CB44 CB43C 13.56\n",
      "CB44 CB44 21.82\n",
      "CB44 CB51 17.33\n",
      "CB44 CB52 59.96\n",
      "CB44 CB53 83.68\n",
      "CB44 CB54 89.31\n",
      "CB44 CB71 98.60\n",
      "CB44 CB62 117.57\n",
      "CB44 CB63 118.12\n",
      "CB44 CB72 132.49\n",
      "CB44 CB64 145.14\n",
      "CB44 CB74 152.21\n",
      "CB44 CB31 94.92\n",
      "CB44 CB32 77.19\n",
      "CB44 CB41C 28.58\n",
      "CB44 CB42C 10.97\n",
      "CB44 CB61 106.17\n",
      "CB44 CB73 156.05\n",
      "CB51 CB33C 62.50\n",
      "CB51 CB22 114.27\n",
      "CB51 CB43C 22.61\n",
      "CB51 CB44 14.12\n",
      "CB51 CB51 12.34\n",
      "CB51 CB52 48.56\n",
      "CB51 CB53 72.29\n",
      "CB51 CB54 77.90\n",
      "CB51 CB71 87.08\n",
      "CB51 CB62 106.38\n",
      "CB51 CB63 107.00\n",
      "CB51 CB72 121.21\n",
      "CB51 CB64 134.06\n",
      "CB51 CB74 140.70\n",
      "CB51 CB31 105.22\n",
      "CB51 CB32 87.80\n",
      "CB51 CB41C 40.08\n",
      "CB51 CB42C 14.60\n",
      "CB51 CB61 94.95\n",
      "CB51 CB73 144.77\n",
      "CB52 CB33C 83.34\n",
      "CB52 CB22 134.11\n",
      "CB52 CB43C 43.03\n",
      "CB52 CB44 13.85\n",
      "CB52 CB51 20.14\n",
      "CB52 CB52 27.87\n",
      "CB52 CB53 51.48\n",
      "CB52 CB54 57.07\n",
      "CB52 CB71 66.15\n",
      "CB52 CB62 85.74\n",
      "CB52 CB63 86.44\n",
      "CB52 CB72 100.43\n",
      "CB52 CB64 113.53\n",
      "CB52 CB74 119.81\n",
      "CB52 CB31 125.32\n",
      "CB52 CB32 108.23\n",
      "CB52 CB41C 60.93\n",
      "CB52 CB42C 33.62\n",
      "CB52 CB61 74.29\n",
      "CB52 CB73 124.00\n",
      "CB53 CB33C 108.99\n",
      "CB53 CB22 159.23\n",
      "CB53 CB43C 68.58\n",
      "CB53 CB44 37.86\n",
      "CB53 CB51 43.88\n",
      "CB53 CB52 9.11\n",
      "CB53 CB53 26.25\n",
      "CB53 CB54 31.63\n",
      "CB53 CB71 40.66\n",
      "CB53 CB62 60.36\n",
      "CB53 CB63 61.19\n",
      "CB53 CB72 74.85\n",
      "CB53 CB64 88.21\n",
      "CB53 CB74 94.33\n",
      "CB53 CB31 150.63\n",
      "CB53 CB32 133.74\n",
      "CB53 CB41C 86.46\n",
      "CB53 CB42C 59.04\n",
      "CB53 CB61 48.91\n",
      "CB53 CB73 98.44\n",
      "CB54 CB33C 120.93\n",
      "CB54 CB22 171.44\n",
      "CB54 CB43C 80.36\n",
      "CB54 CB44 49.69\n",
      "CB54 CB51 55.80\n",
      "CB54 CB52 11.67\n",
      "CB54 CB53 15.39\n",
      "CB54 CB54 20.22\n",
      "CB54 CB71 29.57\n",
      "CB54 CB62 48.33\n",
      "CB54 CB63 49.20\n",
      "CB54 CB72 62.80\n",
      "CB54 CB64 76.03\n",
      "CB54 CB74 82.87\n",
      "CB54 CB31 162.81\n",
      "CB54 CB32 145.87\n",
      "CB54 CB41C 98.18\n",
      "CB54 CB42C 70.71\n",
      "CB54 CB61 36.94\n",
      "CB54 CB73 86.44\n",
      "CB71 CB33C 137.14\n",
      "CB71 CB22 185.15\n",
      "CB71 CB43C 97.38\n",
      "CB71 CB44 66.86\n",
      "CB71 CB51 72.55\n",
      "CB71 CB52 29.42\n",
      "CB71 CB53 14.61\n",
      "CB71 CB54 14.08\n",
      "CB71 CB71 14.41\n",
      "CB71 CB62 38.76\n",
      "CB71 CB63 40.90\n",
      "CB71 CB72 49.94\n",
      "CB71 CB64 64.92\n",
      "CB71 CB74 66.54\n",
      "CB71 CB31 177.04\n",
      "CB71 CB32 160.84\n",
      "CB71 CB41C 115.30\n",
      "CB71 CB42C 88.14\n",
      "CB71 CB61 29.00\n",
      "CB71 CB73 72.84\n",
      "CB62 CB33C 155.50\n",
      "CB62 CB22 206.18\n",
      "CB62 CB43C 114.79\n",
      "CB62 CB44 84.26\n",
      "CB62 CB51 90.43\n",
      "CB62 CB52 44.99\n",
      "CB62 CB53 22.18\n",
      "CB62 CB54 17.33\n",
      "CB62 CB71 14.72\n",
      "CB62 CB62 16.21\n",
      "CB62 CB63 18.54\n",
      "CB62 CB72 28.75\n",
      "CB62 CB64 41.45\n",
      "CB62 CB74 50.79\n",
      "CB62 CB31 197.58\n",
      "CB62 CB32 180.63\n",
      "CB62 CB41C 132.50\n",
      "CB62 CB42C 105.04\n",
      "CB62 CB61 8.18\n",
      "CB62 CB73 52.20\n",
      "CB63 CB33C 163.69\n",
      "CB63 CB22 214.50\n",
      "CB63 CB43C 122.96\n",
      "CB63 CB44 92.48\n",
      "CB63 CB51 98.66\n",
      "CB63 CB52 53.27\n",
      "CB63 CB53 30.35\n",
      "CB63 CB54 25.31\n",
      "CB63 CB71 20.34\n",
      "CB63 CB62 10.35\n",
      "CB63 CB63 13.48\n",
      "CB63 CB72 21.43\n",
      "CB63 CB64 33.22\n",
      "CB63 CB74 44.20\n",
      "CB63 CB31 205.89\n",
      "CB63 CB32 188.90\n",
      "CB63 CB41C 140.62\n",
      "CB63 CB42C 113.18\n",
      "CB63 CB61 9.26\n",
      "CB63 CB73 44.30\n",
      "CB72 CB33C 164.76\n",
      "CB72 CB22 214.67\n",
      "CB72 CB43C 124.20\n",
      "CB72 CB44 93.53\n",
      "CB72 CB51 99.62\n",
      "CB72 CB52 54.04\n",
      "CB72 CB53 30.58\n",
      "CB72 CB54 25.10\n",
      "CB72 CB71 17.43\n",
      "CB72 CB62 12.35\n",
      "CB72 CB63 16.20\n",
      "CB72 CB72 20.13\n",
      "CB72 CB64 34.15\n",
      "CB72 CB74 40.65\n",
      "CB72 CB31 206.24\n",
      "CB72 CB32 189.52\n",
      "CB72 CB41C 142.00\n",
      "CB72 CB42C 114.53\n",
      "CB72 CB61 11.24\n",
      "CB72 CB73 43.24\n",
      "CB64 CB33C 182.51\n",
      "CB64 CB22 233.93\n",
      "CB64 CB43C 141.70\n",
      "CB64 CB44 111.48\n",
      "CB64 CB51 117.72\n",
      "CB64 CB52 72.65\n",
      "CB64 CB53 49.99\n",
      "CB64 CB54 44.97\n",
      "CB64 CB71 39.26\n",
      "CB64 CB62 16.49\n",
      "CB64 CB63 15.76\n",
      "CB64 CB72 13.10\n",
      "CB64 CB64 15.38\n",
      "CB64 CB74 35.31\n",
      "CB64 CB31 225.21\n",
      "CB64 CB32 208.06\n",
      "CB64 CB41C 159.21\n",
      "CB64 CB42C 131.86\n",
      "CB64 CB61 26.78\n",
      "CB64 CB73 29.08\n",
      "CB74 CB33C 211.16\n",
      "CB74 CB22 261.03\n",
      "CB74 CB43C 170.52\n",
      "CB74 CB44 139.91\n",
      "CB74 CB51 146.03\n",
      "CB74 CB52 100.43\n",
      "CB74 CB53 76.85\n",
      "CB74 CB54 71.32\n",
      "CB74 CB71 62.93\n",
      "CB74 CB62 43.02\n",
      "CB74 CB63 42.79\n",
      "CB74 CB72 28.02\n",
      "CB74 CB64 18.42\n",
      "CB74 CB74 21.64\n",
      "CB74 CB31 252.67\n",
      "CB74 CB32 235.99\n",
      "CB74 CB41C 188.27\n",
      "CB74 CB42C 160.80\n",
      "CB74 CB61 54.16\n",
      "CB74 CB73 12.71\n",
      "CB31 CB33C 44.23\n",
      "CB31 CB22 12.20\n",
      "CB31 CB43C 83.39\n",
      "CB31 CB44 112.38\n",
      "CB31 CB51 106.08\n",
      "CB31 CB52 151.00\n",
      "CB31 CB53 174.45\n",
      "CB31 CB54 179.92\n",
      "CB31 CB71 188.24\n",
      "CB31 CB62 209.10\n",
      "CB31 CB63 209.88\n",
      "CB31 CB72 223.52\n",
      "CB31 CB64 236.97\n",
      "CB31 CB74 241.10\n",
      "CB31 CB31 5.29\n",
      "CB31 CB32 17.05\n",
      "CB31 CB41C 67.71\n",
      "CB31 CB42C 93.39\n",
      "CB31 CB61 197.65\n",
      "CB31 CB73 246.87\n",
      "CB32 CB33C 33.48\n",
      "CB32 CB22 22.95\n",
      "CB32 CB43C 73.30\n",
      "CB32 CB44 102.78\n",
      "CB32 CB51 96.51\n",
      "CB32 CB52 141.76\n",
      "CB32 CB53 165.35\n",
      "CB32 CB54 170.86\n",
      "CB32 CB71 179.41\n",
      "CB32 CB62 199.88\n",
      "CB32 CB63 200.59\n",
      "CB32 CB72 214.45\n",
      "CB32 CB64 227.69\n",
      "CB32 CB74 232.53\n",
      "CB32 CB31 12.88\n",
      "CB32 CB32 7.60\n",
      "CB32 CB41C 57.13\n",
      "CB32 CB42C 83.32\n",
      "CB32 CB61 188.42\n",
      "CB32 CB73 237.87\n",
      "CB41C CB33C 10.21\n",
      "CB41C CB22 60.67\n",
      "CB41C CB43C 35.79\n",
      "CB41C CB44 66.04\n",
      "CB41C CB51 59.92\n",
      "CB41C CB52 105.52\n",
      "CB41C CB53 129.26\n",
      "CB41C CB54 134.86\n",
      "CB41C CB71 143.91\n",
      "CB41C CB62 163.37\n",
      "CB41C CB63 163.94\n",
      "CB41C CB72 178.23\n",
      "CB41C CB64 190.97\n",
      "CB41C CB74 197.42\n",
      "CB41C CB31 50.78\n",
      "CB41C CB32 32.46\n",
      "CB41C CB41C 19.47\n",
      "CB41C CB42C 45.75\n",
      "CB41C CB61 151.95\n",
      "CB41C CB73 201.75\n",
      "CB42C CB33C 25.50\n",
      "CB42C CB22 80.35\n",
      "CB42C CB43C 17.15\n",
      "CB42C CB44 46.90\n",
      "CB42C CB51 41.03\n",
      "CB42C CB52 86.39\n",
      "CB42C CB53 110.10\n",
      "CB42C CB54 115.74\n",
      "CB42C CB71 125.03\n",
      "CB42C CB62 143.91\n",
      "CB42C CB63 144.42\n",
      "CB42C CB72 158.89\n",
      "CB42C CB64 171.39\n",
      "CB42C CB74 178.61\n",
      "CB42C CB31 70.61\n",
      "CB42C CB32 52.38\n",
      "CB42C CB41C 9.09\n",
      "CB42C CB42C 26.55\n",
      "CB42C CB61 132.54\n",
      "CB42C CB73 182.43\n",
      "CB61 CB33C 144.33\n",
      "CB61 CB22 194.97\n",
      "CB61 CB43C 103.66\n",
      "CB61 CB44 73.08\n",
      "CB61 CB51 79.24\n",
      "CB61 CB52 33.80\n",
      "CB61 CB53 11.94\n",
      "CB61 CB54 8.89\n",
      "CB61 CB71 13.61\n",
      "CB61 CB62 26.06\n",
      "CB61 CB63 27.55\n",
      "CB61 CB72 39.53\n",
      "CB61 CB64 52.57\n",
      "CB61 CB74 60.81\n",
      "CB61 CB31 186.36\n",
      "CB61 CB32 169.41\n",
      "CB61 CB41C 121.40\n",
      "CB61 CB42C 93.93\n",
      "CB61 CB61 15.15\n",
      "CB61 CB73 63.18\n",
      "CB73 CB33C 196.55\n",
      "CB73 CB22 247.28\n",
      "CB73 CB43C 155.79\n",
      "CB73 CB44 125.34\n",
      "CB73 CB51 131.53\n",
      "CB73 CB52 86.10\n",
      "CB73 CB53 62.80\n",
      "CB73 CB54 57.45\n",
      "CB73 CB71 50.10\n",
      "CB73 CB62 28.17\n",
      "CB73 CB63 27.56\n",
      "CB73 CB72 14.90\n",
      "CB73 CB64 7.94\n",
      "CB73 CB74 26.14\n",
      "CB73 CB31 238.72\n",
      "CB73 CB32 221.78\n",
      "CB73 CB41C 173.42\n",
      "CB73 CB42C 146.00\n",
      "CB73 CB61 39.52\n",
      "CB73 CB73 18.68\n"
     ]
    }
   ],
   "source": [
    "ptcle_stat_distmat = pd.DataFrame(index=list(lowest_points.keys()), \n",
    "                                  columns=list(lowest_points.keys()))\n",
    "\n",
    "horiz_dist = lambda x: geopy.distance.distance((x[0], x[1]), (x[2], x[3])).km\n",
    "\n",
    "for s_stat in ptcle_stat_distmat.columns:\n",
    "    for p_stat in ptcle_stat_distmat.index:    \n",
    "        # these are the particle sets from the 'p' station\n",
    "        stat_p_ixs = lowest_points[p_stat][0]\n",
    "        \n",
    "        # replace the index with the dataframe full of particle locations\n",
    "        stat_p_ixs = [particle_chunks[s_p].loc[:, ['lat', 'lon']] for s_p in stat_p_ixs]\n",
    "        \n",
    "        # unpack all the points\n",
    "        stat_p_locs = [list(stat_p.loc[idx,['lat', 'lon']]) for stat_p in stat_p_ixs for idx in stat_p.index]\n",
    "\n",
    "        # this is the location of the station\n",
    "        stat_s_loc = list(stat_latlon.loc[s_stat, :])\n",
    "\n",
    "        arg_set = [(stat_s_loc[0], stat_s_loc[1], sx[0], sx[1]) for sx in stat_p_locs]\n",
    "        \n",
    "        dist_set = np.array(list(map(horiz_dist, arg_set)))\n",
    "        ptcle_stat_distmat.loc[p_stat, s_stat] = dist_set.mean()\n",
    "        print(\"{} {} {:.2f}\".format(s_stat, p_stat, dist_set.mean()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups of size 1: 20 [('CB33C',), ('CB22',), ('CB43C',)]\n",
      "Groups of size 2: 190 [('CB22', 'CB33C'), ('CB33C', 'CB43C'), ('CB33C', 'CB44')]\n",
      "Groups of size 3: 1140 [('CB22', 'CB33C', 'CB43C'), ('CB22', 'CB33C', 'CB44'), ('CB22', 'CB33C', 'CB51')]\n",
      "Groups of size 4: 4845 [('CB22', 'CB33C', 'CB43C', 'CB44'), ('CB22', 'CB33C', 'CB43C', 'CB51'), ('CB22', 'CB33C', 'CB43C', 'CB52')]\n",
      "6195\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "grps_by_size = {}\n",
    "all_grps = set()\n",
    "for grp_size in range(1,5):\n",
    "    gps_of_size_i = combinations(list(ptcle_stat_distmat.index), grp_size)\n",
    "    gps_of_srt_i = [tuple(sorted(list(j))) for j in gps_of_size_i]\n",
    "    assert len(gps_of_srt_i) == len(set(gps_of_srt_i))\n",
    "    print(\"Groups of size {}: {} {}\".format(grp_size, len(gps_of_srt_i), gps_of_srt_i[:3]))\n",
    "    grps_by_size[grp_size] = gps_of_srt_i\n",
    "    all_grps.update(gps_of_srt_i)\n",
    "\n",
    "print(len(all_grps))\n",
    "\n",
    "other_cols =  ['F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O']\n",
    "solver_sheet = pd.DataFrame(index=range(1, len(all_grps)+1), columns=['Q', 'A', 'B', 'C', 'D', 'E']+other_cols)\n",
    "\n",
    "counter = 0\n",
    "for gs_, gs_i in grps_by_size.items():\n",
    "    print(gs_)\n",
    "    for g_i in gs_i:\n",
    "        i = solver_sheet.index[counter]\n",
    "        solver_sheet.loc[i, 'Q'] = gs_\n",
    "        solver_sheet.loc[i, 'A'] = g_i\n",
    "        solver_sheet.loc[i, 'B'] = 0\n",
    "        inter_group_dist = 0\n",
    "        for p_stat in g_i:\n",
    "            for s_stat in g_i:\n",
    "                inter_group_dist += ptcle_stat_distmat.loc[p_stat, s_stat]\n",
    "        solver_sheet.loc[i, 'C'] = inter_group_dist\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 best_indexes\n",
      "190 1x\n",
      "153 1 2x\n",
      "153 3x\n",
      "120 2 4x\n",
      "120 3x\n",
      "91 3 4x\n",
      "91 3x\n",
      "66 4 4x\n",
      "66 3x\n",
      "45 5 4x\n",
      "45 3x\n",
      "28 6 4x\n",
      "28 3x\n",
      "15 7 4x\n",
      "15 3x\n",
      "6 8 4x\n",
      "6 3x\n",
      "1 9 4x\n",
      "1 3x\n",
      "0 10 4x\n",
      "0 10 5x\n",
      "10 added_groups out of 50\n",
      "180 remaining_groups\n",
      "180 1x\n",
      "145 1 2x\n",
      "145 3x\n",
      "114 2 4x\n",
      "114 3x\n",
      "86 3 4x\n",
      "86 3x\n",
      "62 4 4x\n",
      "62 3x\n",
      "43 5 4x\n",
      "43 3x\n",
      "27 6 4x\n",
      "27 3x\n",
      "15 7 4x\n",
      "15 3x\n",
      "6 8 4x\n",
      "6 3x\n",
      "1 9 4x\n",
      "1 3x\n",
      "0 10 4x\n",
      "0 10 5x\n",
      "20 added_groups out of 50\n",
      "170 remaining_groups\n",
      "170 1x\n",
      "137 1 2x\n",
      "137 3x\n",
      "108 2 4x\n",
      "108 3x\n",
      "80 3 4x\n",
      "80 3x\n",
      "59 4 4x\n",
      "59 3x\n",
      "40 5 4x\n",
      "40 3x\n",
      "25 6 4x\n",
      "25 3x\n",
      "15 7 4x\n",
      "15 3x\n",
      "6 8 4x\n",
      "6 3x\n",
      "1 9 4x\n",
      "1 3x\n",
      "0 10 4x\n",
      "0 10 5x\n",
      "30 added_groups out of 50\n",
      "160 remaining_groups\n",
      "160 1x\n",
      "129 1 2x\n",
      "129 3x\n",
      "101 2 4x\n",
      "101 3x\n",
      "74 3 4x\n",
      "74 3x\n",
      "55 4 4x\n",
      "55 3x\n",
      "36 5 4x\n",
      "36 3x\n",
      "22 6 4x\n",
      "22 3x\n",
      "10 7 4x\n",
      "10 3x\n",
      "5 8 4x\n",
      "5 3x\n",
      "0 9 4x\n",
      "0 10 5x\n",
      "39 added_groups out of 50\n",
      "151 remaining_groups\n",
      "151 1x\n",
      "122 1 2x\n",
      "122 3x\n",
      "95 2 4x\n",
      "95 3x\n",
      "70 3 4x\n",
      "70 3x\n",
      "47 4 4x\n",
      "47 3x\n",
      "33 5 4x\n",
      "33 3x\n",
      "21 6 4x\n",
      "21 3x\n",
      "12 7 4x\n",
      "12 3x\n",
      "5 8 4x\n",
      "5 3x\n",
      "0 9 4x\n",
      "0 10 5x\n",
      "48 added_groups out of 50\n",
      "142 remaining_groups\n",
      "142 1x\n",
      "115 1 2x\n",
      "115 3x\n",
      "89 2 4x\n",
      "89 3x\n",
      "63 3 4x\n",
      "63 3x\n",
      "48 4 4x\n",
      "48 3x\n",
      "34 5 4x\n",
      "34 3x\n",
      "22 6 4x\n",
      "22 3x\n",
      "12 7 4x\n",
      "12 3x\n",
      "5 8 4x\n",
      "5 3x\n",
      "0 9 4x\n",
      "0 10 5x\n",
      "57 added_groups out of 50\n",
      "133 remaining_groups\n",
      "1140 1x\n",
      "680 1 2x\n",
      "680 3x\n",
      "364 2 4x\n",
      "364 3x\n",
      "165 3 4x\n",
      "165 3x\n",
      "56 4 4x\n",
      "56 3x\n",
      "10 5 4x\n",
      "10 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "7 added_groups out of 60\n",
      "1134 remaining_groups\n",
      "1134 1x\n",
      "675 1 2x\n",
      "675 3x\n",
      "360 2 4x\n",
      "360 3x\n",
      "163 3 4x\n",
      "163 3x\n",
      "55 4 4x\n",
      "55 3x\n",
      "9 5 4x\n",
      "9 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "14 added_groups out of 60\n",
      "1128 remaining_groups\n",
      "1128 1x\n",
      "671 1 2x\n",
      "671 3x\n",
      "359 2 4x\n",
      "359 3x\n",
      "161 3 4x\n",
      "161 3x\n",
      "54 4 4x\n",
      "54 3x\n",
      "9 5 4x\n",
      "9 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "21 added_groups out of 60\n",
      "1122 remaining_groups\n",
      "1122 1x\n",
      "668 1 2x\n",
      "668 3x\n",
      "355 2 4x\n",
      "355 3x\n",
      "162 3 4x\n",
      "162 3x\n",
      "56 4 4x\n",
      "56 3x\n",
      "10 5 4x\n",
      "10 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "28 added_groups out of 60\n",
      "1116 remaining_groups\n",
      "1116 1x\n",
      "663 1 2x\n",
      "663 3x\n",
      "350 2 4x\n",
      "350 3x\n",
      "159 3 4x\n",
      "159 3x\n",
      "54 4 4x\n",
      "54 3x\n",
      "10 5 4x\n",
      "10 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "35 added_groups out of 60\n",
      "1110 remaining_groups\n",
      "1110 1x\n",
      "661 1 2x\n",
      "661 3x\n",
      "348 2 4x\n",
      "348 3x\n",
      "160 3 4x\n",
      "160 3x\n",
      "55 4 4x\n",
      "55 3x\n",
      "9 5 4x\n",
      "9 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "41 added_groups out of 60\n",
      "1104 remaining_groups\n",
      "1104 1x\n",
      "657 1 2x\n",
      "657 3x\n",
      "345 2 4x\n",
      "345 3x\n",
      "158 3 4x\n",
      "158 3x\n",
      "52 4 4x\n",
      "52 3x\n",
      "10 5 4x\n",
      "10 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "48 added_groups out of 60\n",
      "1098 remaining_groups\n",
      "1098 1x\n",
      "653 1 2x\n",
      "653 3x\n",
      "341 2 4x\n",
      "341 3x\n",
      "146 3 4x\n",
      "146 3x\n",
      "50 4 4x\n",
      "50 3x\n",
      "9 5 4x\n",
      "9 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "55 added_groups out of 60\n",
      "1092 remaining_groups\n",
      "1092 1x\n",
      "649 1 2x\n",
      "649 3x\n",
      "337 2 4x\n",
      "337 3x\n",
      "154 3 4x\n",
      "154 3x\n",
      "49 4 4x\n",
      "49 3x\n",
      "5 5 4x\n",
      "5 3x\n",
      "0 6 4x\n",
      "0 7 5x\n",
      "62 added_groups out of 60\n",
      "1086 remaining_groups\n",
      "4845 1x\n",
      "1820 1 2x\n",
      "1820 3x\n",
      "495 2 4x\n",
      "495 3x\n",
      "70 3 4x\n",
      "70 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "5 added_groups out of 60\n",
      "4840 remaining_groups\n",
      "4840 1x\n",
      "1817 1 2x\n",
      "1817 3x\n",
      "493 2 4x\n",
      "493 3x\n",
      "69 3 4x\n",
      "69 3x\n",
      "0 4 4x\n",
      "0 5 5x\n",
      "9 added_groups out of 60\n",
      "4836 remaining_groups\n",
      "4836 1x\n",
      "1815 1 2x\n",
      "1815 3x\n",
      "492 2 4x\n",
      "492 3x\n",
      "69 3 4x\n",
      "69 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "14 added_groups out of 60\n",
      "4831 remaining_groups\n",
      "4831 1x\n",
      "1812 1 2x\n",
      "1812 3x\n",
      "492 2 4x\n",
      "492 3x\n",
      "69 3 4x\n",
      "69 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "19 added_groups out of 60\n",
      "4826 remaining_groups\n",
      "4826 1x\n",
      "1809 1 2x\n",
      "1809 3x\n",
      "492 2 4x\n",
      "492 3x\n",
      "69 3 4x\n",
      "69 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "24 added_groups out of 60\n",
      "4821 remaining_groups\n",
      "4821 1x\n",
      "1806 1 2x\n",
      "1806 3x\n",
      "485 2 4x\n",
      "485 3x\n",
      "68 3 4x\n",
      "68 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "29 added_groups out of 60\n",
      "4816 remaining_groups\n",
      "4816 1x\n",
      "1803 1 2x\n",
      "1803 3x\n",
      "492 2 4x\n",
      "492 3x\n",
      "69 3 4x\n",
      "69 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "34 added_groups out of 60\n",
      "4811 remaining_groups\n",
      "4811 1x\n",
      "1800 1 2x\n",
      "1800 3x\n",
      "481 2 4x\n",
      "481 3x\n",
      "67 3 4x\n",
      "67 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "39 added_groups out of 60\n",
      "4806 remaining_groups\n",
      "4806 1x\n",
      "1797 1 2x\n",
      "1797 3x\n",
      "478 2 4x\n",
      "478 3x\n",
      "68 3 4x\n",
      "68 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "44 added_groups out of 60\n",
      "4801 remaining_groups\n",
      "4801 1x\n",
      "1795 1 2x\n",
      "1795 3x\n",
      "477 2 4x\n",
      "477 3x\n",
      "66 3 4x\n",
      "66 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "49 added_groups out of 60\n",
      "4796 remaining_groups\n",
      "4796 1x\n",
      "1792 1 2x\n",
      "1792 3x\n",
      "474 2 4x\n",
      "474 3x\n",
      "67 3 4x\n",
      "67 3x\n",
      "0 4 4x\n",
      "0 5 5x\n",
      "53 added_groups out of 60\n",
      "4792 remaining_groups\n",
      "4792 1x\n",
      "1790 1 2x\n",
      "1790 3x\n",
      "472 2 4x\n",
      "472 3x\n",
      "66 3 4x\n",
      "66 3x\n",
      "0 4 4x\n",
      "0 5 5x\n",
      "57 added_groups out of 60\n",
      "4788 remaining_groups\n",
      "4788 1x\n",
      "1789 1 2x\n",
      "1789 3x\n",
      "475 2 4x\n",
      "475 3x\n",
      "69 3 4x\n",
      "69 3x\n",
      "1 4 4x\n",
      "1 3x\n",
      "0 5 4x\n",
      "0 5 5x\n",
      "62 added_groups out of 60\n",
      "4783 remaining_groups\n"
     ]
    }
   ],
   "source": [
    "limits_by_size = {2:50, 3:60, 4:60}\n",
    "\n",
    "best_indexes = set()\n",
    "best_indexes.update(list(solver_sheet.loc[solver_sheet.Q == 1, 'A']))\n",
    "print(len(best_indexes), 'best_indexes')\n",
    "\n",
    "for grp_size, size_lim in limits_by_size.items():\n",
    "    subsolver = solver_sheet[solver_sheet.Q == grp_size]\n",
    "    remaining_groups = set(subsolver.A)\n",
    "    added_groups  = set()\n",
    "    while len(added_groups) < size_lim:\n",
    "        # see whats remaining among all \n",
    "        avail_bool = subsolver.A.isin(remaining_groups)\n",
    "        # make into a set \n",
    "        remaining_size_i = set(subsolver[avail_bool].A)\n",
    "        print(len(remaining_size_i), \"1x\")\n",
    "        # find the best single group\n",
    "        best_avail = subsolver[avail_bool].sort_values(by='C')\n",
    "        best_grp = best_avail.iloc[0, 1]\n",
    "        # set up a set of the remaining stations \n",
    "        coherency_ = set(ptcle_stat_distmat.index) - set(list(best_grp))\n",
    "        # set up a set of the stations pulled into this set group\n",
    "        coherent_group = set([best_grp])\n",
    "        # make the best group unavailable\n",
    "        remaining_size_i.remove(best_grp)\n",
    "        remaining_size_i = list(filter(lambda x: len(set(x) - coherency_) == 0, remaining_size_i))\n",
    "        print(len(remaining_size_i), len(coherent_group), \"2x\")\n",
    "        while len(remaining_size_i) > 0:\n",
    "            # remove anything with any stations from the best group(s)\n",
    "            print(len(remaining_size_i), \"3x\")\n",
    "            # resort and pull the best\n",
    "            avail_bool = subsolver.A.isin(remaining_size_i)\n",
    "            best_avail = subsolver[avail_bool].sort_values(by='C')\n",
    "            best_remaining = best_avail.iloc[0, 1]\n",
    "            # add it to the local set group\n",
    "            coherent_group.update([best_remaining])\n",
    "            # make the stations added unavailable\n",
    "            for br in best_remaining:\n",
    "                coherency_.remove(br)\n",
    "            remaining_size_i.remove(best_remaining)\n",
    "            remaining_size_i = list(filter(lambda x: len(set(x) - coherency_) == 0, remaining_size_i))\n",
    "            print(len(remaining_size_i), len(coherent_group), \"4x\")\n",
    "        \n",
    "        if len(coherency_) > 0:\n",
    "            coherent_group.update([tuple(sorted(coherency_))])\n",
    "        print(len(remaining_size_i), len(coherent_group), \"5x\")  \n",
    "        \n",
    "        added_groups.update(coherent_group)\n",
    "        print(len(added_groups), 'added_groups out of', size_lim)\n",
    "        for cg in coherent_group:\n",
    "            if cg in remaining_groups:\n",
    "                remaining_groups.remove(cg)\n",
    "        print(len(remaining_groups), 'remaining_groups')\n",
    "    \n",
    "    best_indexes.update(added_groups)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2, 3, 4]), array([20, 61, 54, 62]))\n",
      "set()\n",
      "197\n",
      "Index(['G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(np.unique([len(i) for i in best_indexes], return_counts=True))\n",
    "print(best_indexes - set(solver_sheet.A))\n",
    "solver_sheet2 = solver_sheet.copy().loc[solver_sheet.A.isin(best_indexes), :]\n",
    "print(len(solver_sheet2))\n",
    "solver_sheet2 = solver_sheet2.sort_values(by=['Q', 'A'])\n",
    "solver_sheet2 = solver_sheet2.rename(index={old_idx:ix+1 for ix, old_idx in enumerate(solver_sheet2.index)})\n",
    "\n",
    "solver_sheet2.drop('Q', axis=1, inplace=True)\n",
    "# the objective function is easy, its sumproduct(A, B) \n",
    "solver_sheet2.loc[1, 'D'] = 'Objective Function'\n",
    "solver_sheet2.loc[2, 'D'] = \"=SUMPRODUCT(B:B,C:C)\"\n",
    "solver_sheet2.loc[3, 'D'] = 'Group Creation Limit'\n",
    "solver_sheet2.loc[4, 'D'] = '=SUM(B:B)'\n",
    "solver_sheet2.loc[4, 'E'] = '==' \n",
    "solver_sheet2.loc[4, 'F'] = '6' \n",
    "solver_sheet2.loc[5, 'D'] = 'Group Constraints'\n",
    "\n",
    "for st_ix, stat in enumerate(sorted(ptcle_stat_distmat.index)):\n",
    "    solver_sheet2.loc[6+st_ix, 'D'] = stat\n",
    "    bcells = [str(idx) for idx in solver_sheet2.index if stat in solver_sheet2.loc[idx, 'A']]\n",
    "    solver_sheet2.loc[6+st_ix, 'E'] = \"=B\" + \"+B\".join(bcells)\n",
    "\n",
    "\n",
    "empty_cols = solver_sheet2.columns[solver_sheet2.isnull().sum() == len(solver_sheet2)]\n",
    "print(empty_cols)\n",
    "solver_sheet3 = solver_sheet2.drop(empty_cols, axis=1)\n",
    "solver_sheet3.to_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/mixing_data/station_group_opto.csv\", header=False, \n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    solver_sheet.loc[6+st_ix, 'D'] = stat\n",
    "    bcells = [str(s+1) for s in range(len(stat_grps)) if stat in stat_grps[s]]\n",
    "    per_cell = (len(bcells) // len(other_cols)) + 1\n",
    "    for sc in range(len(other_cols)):\n",
    "        sub_formula = \"=B\" + \"+B\".join(bcells[(sc*per_cell):((sc+1)*per_cell)])\n",
    "        solver_sheet.loc[6+st_ix, other_cols[sc]] = sub_formula\n",
    "    solver_sheet.loc[6+st_ix, 'E'] = \"=SUM(F{}:O{})\".format((6+st_ix), (6+st_ix))\n",
    "\n",
    "solver_sheet.to_csv(\"/Volumes/KeithSSD/CB_V4/otu_data/mixing_data/station_group_opto.csv\", header=False, \n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horiz_dist = lambda x: geopy.distance.distance((x[0], x[1]), (x[2], x[3])).km\n",
    "\n",
    "def summarize_chunk(particle_sub):\n",
    "    pos_cols = ['lat', 'lon', 'StatLat', 'StatLon']\n",
    "    particle_sub['horiz_dist'] = particle_sub[pos_cols].apply(horiz_dist, axis=1)\n",
    "    particle_sub['vert_dist'] = particle_sub['DepthName'] - particle_sub['depth']\n",
    "    sub_summary = {}\n",
    "    sub_summary['sum_horiz_dist'] = particle_sub['horiz_dist'].sum()\n",
    "    sub_summary['sum_vert_dist'] = abs(particle_sub['vert_dist']).sum()\n",
    "    sub_summary['sum_downwelling'] = particle_sub.loc[particle_sub.vert_dist < 0, 'vert_dist'].sum()\n",
    "    sub_summary['sum_upwelling'] = particle_sub.loc[particle_sub.vert_dist > 0, 'vert_dist'].sum()\n",
    "    sub_summary['north_lim'] = np.percentile(particle_sub.lat, 75) - particle_sub.StatLat.unique()[0]\n",
    "    sub_summary['south_lim'] = np.percentile(particle_sub.lat, 25) - particle_sub.StatLat.unique()[0]\n",
    "    sub_summary['east_lim'] = np.percentile(particle_sub.lon, 75) - particle_sub.StatLon.unique()[0]\n",
    "    sub_summary['west_lim'] = np.percentile(particle_sub.lon, 25) - particle_sub.StatLon.unique()[0]\n",
    "    sub_summary['up_lim'] = np.percentile(particle_sub.depth, 75) - particle_sub.DepthName.unique()[0]\n",
    "    sub_summary['down_lim'] = np.percentile(particle_sub.depth, 25) - particle_sub.DepthName.unique()[0]\n",
    "    return sub_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_particle_year(unformatted_df):\n",
    "    # the first six columns are date time depth station collection agency \n",
    "    sample_pts = unformatted_df.iloc[: , :6].dropna()\n",
    "    print(sample_pts.StationName.unique())\n",
    "    print(sample_pts.head())\n",
    "    # pull out this data and add the right column names \n",
    "    particle_data = unformatted_df.iloc[1:, 6:].dropna()\n",
    "    particle_data.columns = ['y_km', 'x_km', 'lat', 'lon', 'depth']\n",
    "    particle_data = particle_data.apply(pd.to_numeric, axis=1)\n",
    "    # longitude is wrong\n",
    "    particle_data['lon'] = particle_data['lon'] - 360\n",
    "    print(particle_data.head())\n",
    "    # the following 300 rows are the point locations \n",
    "    index_ranges = {i:list(range(i+1,i+301)) for i in sample_pts.index}\n",
    "    \n",
    "    assert set([j for i in index_ranges.values() for j in i]) == set(particle_data.index)\n",
    "\n",
    "    particle_chunks = {}\n",
    "    for s_p, ir_i in index_ranges.items():\n",
    "        particle_chunks[s_p] = prep_particle_chunk(s_p, sample_pts, stat_latlon, particle_data, ir_i)\n",
    "    \n",
    "    print(particle_chunks[s_p].head())\n",
    "    print(len(particle_chunks), sum([len(i) for i in particle_chunks.values()]))\n",
    "    \n",
    "    p_summary = {}\n",
    "    for sp_i, pcdf in particle_chunks.items():\n",
    "        p_summary[sp_i] = summarize_chunk(pcdf)\n",
    "\n",
    "    column_order = ['sum_horiz_dist', 'sum_vert_dist', 'sum_downwelling', 'sum_upwelling',\n",
    "                    'north_lim', 'south_lim', 'east_lim', 'west_lim', 'up_lim', 'down_lim']\n",
    "    psum_df = pd.DataFrame(p_summary).T.loc[:, column_order]\n",
    "    print(((psum_df.corr() > 0.6) & (psum_df.corr() < 1.0)).sum().sum() / 2)\n",
    "    \n",
    "    samples_summarized = pd.concat((sample_pts, psum_df), axis=1, sort=1, verify_integrity=1)\n",
    "    return samples_summarized.copy(), particle_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unformatted_df2_x = unformatted_df2.drop(15050, axis=0)\n",
    "unformatted_df2_x = unformatted_df2_x.drop(range(15050+1,15050+301), axis=0)\n",
    "\n",
    "print(unformatted_df2_x.shape)\n",
    "particles2016, particle_chunks16 = summarize_particle_year(unformatted_df2_x.copy())\n",
    "print(particles2016.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#particles2016.sort_values(by='DateMMDDYY').tail(50)\n",
    "particle_chunks16[8127].sort_values('depth').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles2015, particle_chunks15 = summarize_particle_year(unformatted_df1.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_index_june16 = [0, 301, 602, 903, 1204, 1505, 1806, 2107, 2408, 2709, 3010]\n",
    "depth_index_july16 = [3612, 3913, 4214, 4515, 4816, 5117, 5418, 5719, 6020, 6321]\n",
    "depth_index_aug16 = [12040, 12341, 12642, 12943, 13244, 13545, 13846, 14147]\n",
    "depths_to_add_back = [3010] + [6321] + [14147]\n",
    "depth_prof_idxs = depth_index_june16+depth_index_july16+depth_index_aug16\n",
    "\n",
    "transect_set = [i for i in depth_prof_idxs if not i in depths_to_add_back]\n",
    "particles_transect_2016 = particles2016.drop(transect_set, axis=0)\n",
    "depth_profiles_2016 = particles2016.loc[depth_prof_idxs, :]\n",
    "print(particles_transect_2016.shape)\n",
    "print(depth_profiles_2016.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_profiles_1516 = pd.concat((particles2015, depth_profiles_2016), axis=0, sort=1, ignore_index=1)\n",
    "particles1516srt =  depth_profiles_1516.sort_values(by=['DateMMDDYY', 'DepthName'])\n",
    "\n",
    "#particles2015srt.loc[particles2015srt.index[:20], ['DateMMDDYY', 'DepthName']]\n",
    "\n",
    "y1 = particles1516srt['sum_upwelling'].values\n",
    "y2 = particles1516srt['sum_downwelling'].values\n",
    "xlab  = particles1516srt[['DateMMDDYY', 'DepthName']].apply(lambda x: \"-\".join([str(int(x[0])), \n",
    "                                                                                str(int(x[1]))]), axis=1)\n",
    "\n",
    "x12 = list(range(1,len(y1)+1))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.bar(x12, y2*-1, 0.5, label='Downwelling', bottom=[0]*len(y2), color='r')\n",
    "ax.bar(x12, y1*-1, 0.5, label='Upwelling', bottom=[0]*len(y2), color='b')\n",
    "ax.set_ylabel('Sum Vertical Distance Traveled by Particles')\n",
    "ax.set_xticks(x12)\n",
    "ax.set_xticklabels(list(xlab), rotation = 90)\n",
    "#ax.set_yticks(np.arange(0, 81, 10))\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y1 = particles1516srt['up_lim'].values\n",
    "y2 = particles1516srt['down_lim'].values\n",
    "xlab  = particles1516srt[['DateMMDDYY', 'DepthName']].apply(lambda x: \"-\".join([str(int(x[0])), \n",
    "                                                                                str(int(x[1]))]), axis=1)\n",
    "\n",
    "x12 = list(range(1,len(y1)+1))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.bar(x12, y1, 0.5, label='Q1 of Vertical Travel', bottom=[0]*len(y1), color='r')\n",
    "ax.bar(x12, y2, 0.5, label='Q3 of Vertical Travel', bottom=[0]*len(y2), color='b')\n",
    "ax.set_ylabel('Sum Vertical Distance Traveled by Particles')\n",
    "ax.set_xticks(x12)\n",
    "ax.set_xticklabels(list(xlab), rotation = 90)\n",
    "#ax.set_yticks(np.arange(0, 81, 10))\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = particles1516srt['south_lim'].values\n",
    "y2 = particles1516srt['north_lim'].values\n",
    "y3 = particles1516srt['east_lim'].values\n",
    "y4 = particles1516srt['west_lim'].values\n",
    "\n",
    "print(particles1516srt[[i for i in particles1516srt.columns if 'lim' in i]].head())\n",
    "\n",
    "x12 = list(range(1,len(y1)+1))\n",
    "\n",
    "fig = plt.figure(figsize=(11, 6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.bar(x12, y1, 0.5, label='SouthRange', color='r')\n",
    "ax.bar(x12, y2, 0.5, label='NorthRange', color='b')\n",
    "ax.bar(x12, y3, 0.5, label='EastRange', bottom=[0.4]*len(y3), color='cyan')\n",
    "ax.bar(x12, y4, 0.5, label='WestRange', bottom=[0.4]*len(y4), color='magenta')\n",
    "ax.set_ylabel('Long/Lat Range for 95% of Particles')\n",
    "ax.set_xticks(x12)\n",
    "ax.set_xticklabels(list(xlab), rotation = 90)\n",
    "#ax.set_yticks(np.arange(0, 81, 10))\n",
    "ax.legend()\n",
    "plt.show()\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles2016srt =  particles_transect_2016.sort_values(by=['StationName', 'DateMMDDYY', 'DepthName'])\n",
    "quant_cols = ['sum_upwelling', 'sum_downwelling', 'up_lim', 'down_lim']\n",
    "\n",
    "updown_df = particles2016srt.loc[:, quant_cols + ['StationName']].groupby('StationName').agg([np.mean, np.std])\n",
    "#updown_df = updown_df.fillna(0)\n",
    "print(updown_df.head())\n",
    "\n",
    "y1 = updown_df[('sum_upwelling', 'mean')].values\n",
    "y2 = updown_df[('sum_downwelling', 'mean')].values\n",
    "y1s = updown_df[('sum_upwelling', 'std')].values\n",
    "y2s = updown_df[('sum_downwelling', 'std')].values\n",
    "\n",
    "y3 = updown_df[('up_lim', 'mean')].values\n",
    "y4 = updown_df[('down_lim', 'mean')].values\n",
    "y3s = updown_df[('up_lim', 'std')].values\n",
    "y4s = updown_df[('down_lim', 'std')].values\n",
    "\n",
    "xlab  = list(updown_df.index)\n",
    "x12 = np.array(list(range(1,len(y1)+1)))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6), dpi=300)\n",
    "ax1, ax2 = fig.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax1.bar(x12-0.2, y2*-1, 0.4, label='Downwelling', bottom=[0]*len(y2), color='white', edgecolor='grey',\n",
    "        yerr=y2s, ecolor='black', capsize=2)\n",
    "ax1.bar(x12+0.2, y1, 0.4, label='Upwelling', bottom=[0]*len(y2), color='white', edgecolor='grey', hatch='xxx',\n",
    "        yerr=y1s, ecolor='black', capsize=2)\n",
    "ax1.set_ylabel('Sum distance travelled')\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.set_xticks(x12)\n",
    "ax1.set_xticklabels(list(xlab), rotation = 45)\n",
    "#ax1.tick_params(bottom=False, top=False, left=True, right=False, \n",
    "#                labelbottom=False, labeltop=False, labelleft=True, labelright=False)\n",
    "ax1.legend()\n",
    "ax2.bar(x12-0.2, y3, 0.4, label='Q1', bottom=[0]*len(y3), color='white', edgecolor='grey',\n",
    "        yerr=y3s, ecolor='black', capsize=2)\n",
    "ax2.bar(x12+0.2, y4, 0.4, label='Q3', bottom=[0]*len(y4), color='white', edgecolor='grey', hatch='xxx',\n",
    "        yerr=y4s, ecolor='black', capsize=2)\n",
    "ax2.set_ylabel('Sinking Distance')\n",
    "ax2.set_xticks(x12)\n",
    "ax2.set_xticklabels(list(xlab), rotation = 45)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quant_cols = ['south_lim', 'north_lim', 'east_lim', 'west_lim']\n",
    "allround_df = particles2016srt.loc[:, quant_cols + ['StationName']].groupby('StationName').agg([np.mean, np.std])\n",
    "print(allround_df.head())\n",
    "\n",
    "y1 = allround_df[('south_lim', 'mean')].values\n",
    "y2 = allround_df[('north_lim', 'mean')].values\n",
    "y1s = allround_df[('south_lim', 'std')].values\n",
    "y2s = allround_df[('north_lim', 'std')].values\n",
    "\n",
    "y3 = allround_df[('east_lim', 'mean')].values\n",
    "y4 = allround_df[('west_lim', 'mean')].values\n",
    "y3s = allround_df[('east_lim', 'std')].values\n",
    "y4s = allround_df[('west_lim', 'std')].values\n",
    "\n",
    "xlab  = list(allround_df.index)\n",
    "x12 = np.array(list(range(1,len(y1)+1)))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6), dpi=300)\n",
    "ax1, ax2 = fig.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax1.bar(x12-0.2, y2, 0.4, label='North', bottom=[0]*len(y2), color='white', edgecolor='grey',\n",
    "        yerr=y2s, ecolor='black', capsize=2)\n",
    "ax1.bar(x12+0.2, y1, 0.4, label='South', bottom=[0]*len(y2), color='white', edgecolor='grey', hatch='xxx',\n",
    "        yerr=y1s, ecolor='black', capsize=2)\n",
    "ax1.set_ylabel(r'$Lon_{quartile} - Lon_{stat}$')\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.set_xticks(x12)\n",
    "ax1.set_xticklabels(list(xlab), rotation = 45)\n",
    "#ax1.tick_params(bottom=False, top=False, left=True, right=False, \n",
    "#                labelbottom=False, labeltop=False, labelleft=True, labelright=False)\n",
    "ax1.legend()\n",
    "ax2.bar(x12-0.2, y3, 0.4, label='East', bottom=[0]*len(y3), color='white', edgecolor='grey',\n",
    "        yerr=y3s, ecolor='black', capsize=2)\n",
    "ax2.bar(x12+0.2, y4, 0.4, label='West', bottom=[0]*len(y4), color='white', edgecolor='grey', hatch='xxx',\n",
    "        yerr=y4s, ecolor='black', capsize=2)\n",
    "ax2.set_ylabel(r'$Lat_{quartile} - Lat_{stat}$')\n",
    "ax2.set_xticks(x12)\n",
    "ax2.set_xticklabels(list(xlab), rotation = 45)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data['StatName'] = pd.Series()\n",
    "statg1 = ['CB22', 'CB31', 'CB32']\n",
    "statg2 = ['CB33C', 'CB41C']\n",
    "statg3 = ['CB42C', 'CB43C', 'CB44', 'CB51']\n",
    "statg4 = ['CB52', 'CB53', 'CB54', 'CB71']\n",
    "statg5 = ['CB61', 'CB62', 'CB63', 'CB64']\n",
    "statg6 = ['CB72', 'CB73', 'CB74']\n",
    "\n",
    "env_data.loc[env_data['StationName'].isin(statg1),'StatName'] = 1\n",
    "env_data.loc[env_data['StationName'].isin(statg2),'StatName'] = 2\n",
    "env_data.loc[env_data['StationName'].isin(statg3),'StatName'] = 3\n",
    "env_data.loc[env_data['StationName'].isin(statg4),'StatName'] = 4\n",
    "env_data.loc[env_data['StationName'].isin(statg5),'StatName'] = 5\n",
    "env_data.loc[env_data['StationName'].isin(statg6),'StatName'] = 6\n",
    "\n",
    "statdat = env_data[['StationName', 'StatName', 'Latitude', 'Longitude']].groupby('StationName').agg(['mean', 'count'])\n",
    "statdat.columns = statdat.columns.droplevel()\n",
    "\n",
    "statdat.columns = ['StationGroup', 'NumSamples', 'Lat', 'x', 'Lon', 'y']\n",
    "statdat.drop(['x', 'y'], axis=1, inplace=True)\n",
    "statdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_particles = pd.concat(particle_chunks16.values(), axis=0, sort=True, verify_integrity=True)\n",
    "#ap_m = all_particles[['lat', 'lon']].mean()\n",
    "#ap_sd = all_particles[['x_km', 'y_km']].std()\n",
    "\n",
    "\n",
    "part_data = {}\n",
    "for stat_group_i in statdat.StationGroup.unique():\n",
    "    stat_group_names = list(statdat[statdat.StationGroup == stat_group_i].index)\n",
    "    stat_group_bool = particles_transect_2016.StationName.isin(stat_group_names)\n",
    "    stat_group_ixs = list(particles_transect_2016[stat_group_bool].index)\n",
    "    print(\"Group\", int(stat_group_i), \":\", stat_group_names)\n",
    "    print(\"Row #s:\", stat_group_ixs)\n",
    "\n",
    "    stat_grp_particles = pd.concat([particle_chunks16[i] for i in stat_group_ixs], \n",
    "                                    axis=0, sort=True, verify_integrity=True)\n",
    "\n",
    "    print(\"N particles:\", len(stat_grp_particles))\n",
    "    x_pos = stat_grp_particles['lat']# - ap_m['lat']) #/ap_sd['x_km']\n",
    "    y_pos = stat_grp_particles['lon']# - ap_m['lon']) #/ap_sd['y_km']\n",
    "    xy_vec = np.sqrt(x_pos**2 + y_pos**2)\n",
    "    dist_lims = np.percentile(xy_vec, [5, 95])\n",
    "    x_pos = x_pos[(xy_vec > dist_lims[0]) & (xy_vec < dist_lims[1])]\n",
    "    y_pos = y_pos[(xy_vec > dist_lims[0]) & (xy_vec < dist_lims[1])]\n",
    "    part_data[int(stat_group_i)] = {'x': x_pos, 'y': y_pos, \n",
    "                               'sx': statdat.loc[stat_group_names, 'Lat'],\n",
    "                               'sy': statdat.loc[stat_group_names, 'Lon'],\n",
    "                               'l': \"Group{}\".format(int(stat_group_i))}\n",
    "\n",
    "import seaborn as sns\n",
    "np.random.seed(10)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "#sns.set(color_codes=True, font_scale=1)\n",
    "kde_f, kde_ax = plt.subplots(figsize=(6,8), dpi=150)\n",
    "#kde_ax.tick_params(axis='both', which='major', labelsize=22)\n",
    "ax1 = sns.kdeplot(part_data[1]['y'], part_data[1]['x'], cmap=\"Reds\", shade=True, shade_lowest=False, \n",
    "                  alpha=0.8, ax=kde_ax)\n",
    "kde_ax.scatter(x=part_data[1]['sy'], y=part_data[1]['sx'], c='red', edgecolors='k')\n",
    "\n",
    "ax2 = sns.kdeplot(part_data[2]['y'], part_data[2]['x'], cmap=\"Greens\", shade=True, shade_lowest=False, \n",
    "                  alpha=0.8, ax=kde_ax)\n",
    "kde_ax.scatter(x=part_data[2]['sy'], y=part_data[2]['sx'], c='green', edgecolors='k')\n",
    "\n",
    "ax3 = sns.kdeplot(part_data[3]['y'], part_data[3]['x'], cmap=\"Blues\", shade=True, shade_lowest=False, \n",
    "                  alpha=0.8, ax=kde_ax)\n",
    "kde_ax.scatter(x=part_data[3]['sy'], y=part_data[3]['sx'], c='blue', edgecolors='k')\n",
    "\n",
    "ax4 = sns.kdeplot(part_data[4]['y'], part_data[4]['x'], cmap=\"Purples\", shade=True, shade_lowest=False, \n",
    "                  alpha=0.8, ax=kde_ax)\n",
    "kde_ax.scatter(x=part_data[4]['sy'], y=part_data[4]['sx'], c='purple', edgecolors='k')\n",
    "\n",
    "ax5 = sns.kdeplot(part_data[5]['y'], part_data[5]['x'], cmap=\"Greys\", shade=True, shade_lowest=False, \n",
    "                  alpha=0.8, ax=kde_ax)\n",
    "kde_ax.scatter(x=part_data[5]['sy'], y=part_data[5]['sx'], c='k', edgecolors='k')\n",
    "\n",
    "ax5 = sns.kdeplot(part_data[5]['y'], part_data[5]['x'], cmap=\"Greys\", shade=True, shade_lowest=False, \n",
    "                  alpha=0.8, ax=kde_ax)\n",
    "kde_ax.scatter(x=part_data[5]['sy'], y=part_data[5]['sx'], c='k', edgecolors='k')\n",
    "\n",
    "ax6 = sns.kdeplot(part_data[6]['y'], part_data[6]['x'], cmap=\"Oranges\", shade=True, shade_lowest=False, \n",
    "                  alpha=0.8, ax=kde_ax)\n",
    "kde_ax.scatter(x=part_data[6]['sy'], y=part_data[6]['sx'], c='orange', edgecolors='k')\n",
    "\n",
    "#kde_ax.spines['right'].set_visible(False)\n",
    "#kde_ax.spines['top'].set_visible(False)\n",
    "kde_ax.set_xlim((-77, -75))\n",
    "kde_ax.set_ylim((36.75, 39.75))\n",
    "\n",
    "\n",
    "m2 = Basemap(llcrnrlat=36.75,urcrnrlat=39.75,llcrnrlon=-77,urcrnrlon=-75, resolution='f', ax=kde_ax)\n",
    "m2.drawcoastlines(linewidth=0.5)\n",
    "m2.fillcontinents(color='lightgrey',lake_color='white')\n",
    "m2.drawparallels(np.arange(36.5,40,0.5), labels=[True,True,False,False], dashes=[2,2])\n",
    "m2.drawmeridians(np.arange(-78.,-74,0.5), labels=[False,False,False,True], dashes=[2,2])\n",
    "m2.drawmapboundary(fill_color='white')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"~/Google Drive/SiYi_Xiaotong_Materials/bray_curtis_betadiversity.txt\"\n",
    "bd_df = pd.read_csv(f, sep=\"\\t\", index_col=0)\n",
    "bd_df.columns = [i.replace(\"_wu\", \"\") for i in bd_df.columns]\n",
    "\n",
    "sn_df = pd.DataFrame(index=list(range(1,7)),\n",
    "                     columns=list(range(1,7)))\n",
    "\n",
    "for i in sn_df.index:\n",
    "    for j in sn_df.index:\n",
    "        clust_ixs = env_data[env_data.StatName == i].index\n",
    "        clust_jxs = env_data[env_data.StatName == j].index\n",
    "        sn_df.loc[i, j] = bd_df.loc[clust_ixs, clust_jxs].mean().mean()\n",
    "\n",
    "score_c =  []\n",
    "for i in sn_df.index:\n",
    "    v = sn_df.iloc[i-1, i-1]\n",
    "    j = list(sn_df.iloc[i-1,:])\n",
    "    j.remove(v)\n",
    "    l = min(j)\n",
    "    print(i, round(v/l, 3))\n",
    "    score_c.append(v/l)\n",
    "    \n",
    "print(sum(score_c))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['CB72', 'CB73', 'CB74'] 5.739730923390547\n",
    "#['CB63', 'CB73', 'CB74'] : 5.811574898837427\n",
    "#73 and 74 alone : 5.713039267650133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "print(particles2016.DateMMDDYY.unique())\n",
    "date_range = ['71316', '71116', '71216']\n",
    "sub2016_1 = particles_transect_2016[particles_transect_2016.DateMMDDYY.isin(date_range)]\n",
    "station_set_3 = list(particles_transect_2016.StationName.unique())\n",
    "\n",
    "sub2016_jul = sub2016_1[sub2016_1.StationName.isin(station_set_3)]\n",
    "\n",
    "date_range_2 = ['81016', '81216', '80816', '80916']\n",
    "sub2016_2 = particles_transect_2016[particles_transect_2016.DateMMDDYY.isin(date_range_2)]\n",
    "sub2016_aug = sub2016_2[sub2016_2.StationName.isin(station_set_3)]\n",
    "mlp_colors = list(mpl.rcParams['axes.prop_cycle']) * 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 10), dpi=150)\n",
    "m = Basemap(llcrnrlat=36.75,urcrnrlat=39.75,llcrnrlon=-77,urcrnrlon=-75, resolution='h', ax=axes[0])\n",
    "m.drawcoastlines(linewidth=0.5)\n",
    "m.fillcontinents(color='tan',lake_color='lightblue')\n",
    "# draw parallels and meridians.\n",
    "m.drawparallels(np.arange(36.5,40,0.5),labels=[True,True,False,False], dashes=[2,2])\n",
    "m.drawmeridians(np.arange(-78.,-74,0.5), labels=[False,False,False,True], dashes=[2,2])\n",
    "m.drawmapboundary(fill_color='lightblue')\n",
    "m.drawcountries(linewidth=2, linestyle='solid', color='k' ) \n",
    "m.drawstates(linewidth=0.5, linestyle='solid', color='k')\n",
    "m.drawrivers(linewidth=0.5, linestyle='solid', color='blue')\n",
    "\n",
    "for idx_ in range(len(sub2016_jul)):\n",
    "    this_col = mlp_colors[idx_]['color']\n",
    "    idx_no = sub2016_jul.index[idx_]\n",
    "    statname = sub2016_jul.loc[idx_no, 'StationName']\n",
    "    m.scatter(x=particle_chunks16[idx_no]['lon'], \n",
    "              y=particle_chunks16[idx_no]['lat'], s=1, c=this_col, label=None)\n",
    "    m.scatter(x = stat_latlon.loc[statname, 'Longitude'], \n",
    "              y = stat_latlon.loc[statname, 'Latitude'], \n",
    "              s=35, marker='d', edgecolor='k', linewidths=0.7, c=this_col, label=statname)\n",
    "\n",
    "m2 = Basemap(llcrnrlat=36.75,urcrnrlat=39.75,llcrnrlon=-77,urcrnrlon=-75, resolution='h', ax=axes[1])\n",
    "m2.drawcoastlines(linewidth=0.5)\n",
    "m2.fillcontinents(color='tan',lake_color='lightblue')\n",
    "m2.drawparallels(np.arange(36.5,40,0.5), labels=[True,True,False,False], dashes=[2,2])\n",
    "m2.drawmeridians(np.arange(-78.,-74,0.5), labels=[False,False,False,True], dashes=[2,2])\n",
    "m2.drawmapboundary(fill_color='lightblue')\n",
    "m2.drawcountries(linewidth=2, linestyle='solid', color='k' ) \n",
    "m2.drawstates(linewidth=0.5, linestyle='solid', color='k')\n",
    "m2.drawrivers(linewidth=0.5, linestyle='solid', color='blue')\n",
    "for idx_ in range(len(sub2016_aug)):\n",
    "    this_col = mlp_colors[idx_]['color']\n",
    "    idx_no = sub2016_aug.index[idx_]\n",
    "    statname = sub2016_aug.loc[idx_no, 'StationName']\n",
    "    m2.scatter(x=particle_chunks16[idx_no]['lon'], \n",
    "               y=particle_chunks16[idx_no]['lat'], s=1, c=this_col, label=None)\n",
    "    m2.scatter(x = stat_latlon.loc[statname, 'Longitude'], \n",
    "              y = stat_latlon.loc[statname, 'Latitude'], \n",
    "              s=35, marker='d', edgecolor='k', linewidths=0.7, c=this_col, label=statname)\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp = '/Volumes/KeithSSD/CB_V4/otu_data/mixing_data'\n",
    "outf = \"2016_transect_mixing_data_products.txt\"\n",
    "particles_transect_2016.to_csv(os.path.join(outp, outf), sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "font = {'size': 10}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 8), dpi=300)\n",
    "m = Basemap(llcrnrlat=36.75,urcrnrlat=39.75,llcrnrlon=-77.5,urcrnrlon=-74.5, resolution='i', ax=axes,\n",
    "           projection='merc')\n",
    "m.drawcoastlines(linewidth=0.5)\n",
    "m.fillcontinents(color='lightgrey',lake_color='white')\n",
    "m.drawparallels(np.arange(36.5,40,0.5),labels=[True,False,False,False], dashes=[2,2])\n",
    "m.drawmeridians(np.arange(-78.,-74,0.5), labels=[False,False,False,True], dashes=[2,2])\n",
    "m.drawmapboundary(fill_color='white')\n",
    "#m.drawcountries(linewidth=1.5, linestyle='solid', color='k' ) \n",
    "#m.drawstates(linewidth=1.5, linestyle='solid', color='k')\n",
    "#m.drawrivers(linewidth=2, linestyle='solid', color='white')\n",
    "m.drawmapscale(lon=-74.75, lat=36.875, lon0=-75.25, lat0=36.875, length=50)\n",
    "\n",
    "marker_list = ['s', 'o', 'p', 'x', 'd', '*']\n",
    "for idx_ in statdat.StationGroup.unique():\n",
    "    this_col = mlp_colors[int(idx_)]['color']\n",
    "    substat = statdat[statdat.StationGroup == idx_].copy()\n",
    "    subx, suby = m(substat['Lon'].values, substat['Lat'].values)\n",
    "    m.scatter(x=subx, y=suby, s=20, c='k', label=\"Group\"+str(int(idx_)),\n",
    "              marker=marker_list[int(idx_)-1], zorder=10)\n",
    "\n",
    "bbox_args = dict(boxstyle=\"round\", fc=\"0.7\")\n",
    "for row in statdat.index:\n",
    "    rownew = row[:3] + '.' + row[3:]\n",
    "    realx, realy = statdat['Lon'][row], statdat['Lat'][row]\n",
    "    \n",
    "    if row == 'CB63':\n",
    "        subx, suby = m(realx-0.35, realy)\n",
    "    elif row in ['CB22', 'CB32', 'CB54', 'CB61']:\n",
    "        subx, suby = m(realx-0.35, realy+0.03)\n",
    "    elif row in ['CB44']:\n",
    "        subx, suby = m(realx-0.35, realy-0.01)\n",
    "    elif row in ['CB42C']:\n",
    "        subx, suby = m(realx-0.39, realy+0.03)\n",
    "    elif row in ['CB31']:\n",
    "        subx, suby = m(realx+0.1, realy-0.02)\n",
    "    elif row in ['CB32', 'CB43C', 'CB51']:\n",
    "        subx, suby = m(realx+0.1, realy-0.02)\n",
    "    elif row in ['CB71']:\n",
    "        subx, suby = m(realx+0.1, realy+0.01)\n",
    "    elif row in ['CB72']:\n",
    "        subx, suby = m(realx+0.09, realy-0.02)\n",
    "    elif row in ['CB62']:\n",
    "        subx, suby = m(realx+0.09, realy+0.03)\n",
    "    else:\n",
    "        subx, suby = m(realx+0.1, realy)\n",
    "    \n",
    "    allnotes = \"{}, n={}\".format(rownew, int(statdat['NumSamples'][row]))\n",
    "    axes.annotate(rownew, (subx, suby), bbox=bbox_args, size=10)\n",
    "    \n",
    "axes.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Arial']\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "\n",
    "mean, cov = [0, 2], [(2, 1), (.5, 1)]\n",
    "x1, y1 = np.random.multivariate_normal(mean, cov, size=50).T\n",
    "\n",
    "mean, cov = [5, 7], [(3, 2), (7, 1)]\n",
    "x2, y2 = np.random.multivariate_normal(mean, cov, size=50).T\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"foo\", fontsize=22)\n",
    "plt.ylabel(\"bar\", fontsize=22)\n",
    "plt.savefig(\"foo_vs_bar.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
